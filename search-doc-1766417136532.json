{"searchDocs":[{"title":"Chapter 1 - Introduction to Physical AI","type":0,"sectionRef":"#","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#learning-objectives","content":" Define Physical AI and distinguish it from traditional AIUnderstand the importance of embodiment in AI systemsRecognize the challenges of physical world interaction  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#introduction","content":" Physical AI represents a paradigm shift from traditional AI systems that operate purely in digital spaces to AI that interacts with and operates within the physical world. This field specifically emphasizes humanoid robots that can navigate, interact, and function in human-centered environments through the integrated Vision-Language-Action (VLA) pipeline.  Traditional AI systems excel at processing information, recognizing patterns, and making decisions based on data, but they operate in virtual environments. Physical AI, in contrast, must contend with the complexities and uncertainties of the real world: variable lighting conditions, friction, gravity, object dynamics, and the need for real-time responses to maintain stability and safety.  ","version":"Next","tagName":"h2"},{"title":"What is Physical AI?​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#what-is-physical-ai","content":" Physical AI is the science and engineering of artificial intelligence systems that exist and operate in the physical world. Unlike digital AI systems that process information on servers or in the cloud, Physical AI systems must:  Sense their environment through various sensors (cameras, lidar, IMUs, force/torque sensors)Make decisions under uncertainty and time constraintsExecute actions that affect the physical worldAdapt to changing environmental conditionsMaintain safety and stability in dynamic situations  ","version":"Next","tagName":"h2"},{"title":"Key Distinctions from Traditional AI​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#key-distinctions-from-traditional-ai","content":" Traditional AI\tPhysical AIOperates in digital environments\tOperates in physical environments Processes virtual data\tProcesses sensor data from the real world Can afford computational delays\tRequires real-time responses Perfect information (in digital games)\tImperfect information (sensor noise, occlusions) No consequences for errors\tPotentially dangerous consequences for errors Predictable environments\tUnpredictable environments  ","version":"Next","tagName":"h3"},{"title":"The Embodiment Principle​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#the-embodiment-principle","content":" The embodiment principle states that intelligence emerges from the interaction between an agent and its environment. This principle is fundamental to Physical AI and distinguishes it from traditional AI:  Morphological Computation: The physical form of the robot contributes to its computational capabilitiesEnvironmental Interaction: The environment provides information that the agent can use for decision-makingSensory-Motor Coordination: Perception and action are tightly coupled in real-time  ","version":"Next","tagName":"h2"},{"title":"Real-World Constraints​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#real-world-constraints","content":" Physical AI systems must operate under several constraints that digital AI systems do not face:  Physical Laws: Systems must comply with gravity, friction, momentum, and other physical lawsEnergy Limitations: Battery life and power consumption are critical constraintsSafety Requirements: Systems must operate safely around humans and propertyReal-Time Processing: Decisions must be made within strict time constraints to maintain stabilityHardware Limitations: Computation is constrained by embedded hardware capabilities  ","version":"Next","tagName":"h3"},{"title":"Vision-Language-Action Pipeline​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#vision-language-action-pipeline","content":" The VLA pipeline is the core architectural pattern for Physical AI systems:  Vision: Perceive the environment through cameras and other sensorsLanguage: Process commands and goals through natural language understandingAction: Execute physical actions to achieve goals  This pipeline enables humanoid robots to receive natural language commands and execute complex tasks in human environments.  ","version":"Next","tagName":"h2"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Object Manipulation​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#example-1-object-manipulation","content":" A humanoid robot receives the command &quot;Pick up the red cup from the table.&quot; The VLA pipeline executes as follows:  Vision: Identify the red cup in the scene, determine its position and orientationLanguage: Parse the command to understand the action and target objectAction: Plan and execute the grasping motion to pick up the cup  ","version":"Next","tagName":"h3"},{"title":"Example 2: Navigation​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#example-2-navigation","content":" A humanoid robot receives the command &quot;Go to the kitchen and bring me a water bottle.&quot; The VLA pipeline executes as follows:  Vision: Map the environment, identify navigable paths and obstaclesLanguage: Parse the destination and target objectAction: Plan and execute navigation to the kitchen, then locate and retrieve the water bottle  ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Conceptual Understanding​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#exercise-1-conceptual-understanding","content":" Explain in your own words how Physical AI differs from traditional AI. Provide at least 3 specific examples of constraints that Physical AI systems face but traditional AI systems do not.  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Real-World Application​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#exercise-2-real-world-application","content":" Consider a task that a humanoid robot might need to perform (e.g., opening a door, setting a table). Describe how the VLA pipeline would be used to accomplish this task, identifying the vision, language, and action components.  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#summary","content":" Physical AI represents a fundamental shift from digital AI systems to embodied systems that operate in the physical world. The VLA pipeline provides the architectural foundation for humanoid robots to interact with human environments using natural language commands. Understanding the constraints and challenges of the physical world is essential for developing effective Physical AI systems.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 1 - Introduction to Physical AI","url":"/docs/chapters/module-1-foundations/chapter-1-introduction-to-physical-ai#further-reading","content":" &quot;Embodied Cognition&quot; by Lawrence Shapiro&quot;The Robotics Primer&quot; by George Bekey&quot;Introduction to Autonomous Robots&quot; by Nikolaos Papanikolopoulos ","version":"Next","tagName":"h2"},{"title":"Chapter 2 - Embodied Intelligence & Real-World Constraints","type":0,"sectionRef":"#","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#learning-objectives","content":" Explain the concept of embodied intelligenceIdentify real-world constraints in roboticsAnalyze the relationship between embodiment and intelligence  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#introduction","content":" Embodied intelligence is the theory that intelligence emerges from the interaction between an agent and its environment. This concept is fundamental to Physical AI, where the physical form of a robot is not just an appendage to an intelligent system, but an integral part of the intelligence itself. In this chapter, we explore how embodiment shapes the capabilities and limitations of humanoid robots.  ","version":"Next","tagName":"h2"},{"title":"The Concept of Embodied Intelligence​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#the-concept-of-embodied-intelligence","content":" Embodied intelligence challenges the traditional view of intelligence as computation occurring in a disembodied mind. Instead, it posits that:  Intelligence is grounded in physical interaction: Cognitive processes are deeply influenced by the body's interactions with the environmentMorphological computation: The physical structure of the body contributes to computational processesSensory-motor coupling: Perception and action are tightly integrated in real-time  ","version":"Next","tagName":"h2"},{"title":"Historical Context​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#historical-context","content":" The concept of embodied intelligence emerged from research in cognitive science, robotics, and artificial life. It challenges the classical computational theory of mind by emphasizing the role of the body in cognition.  ","version":"Next","tagName":"h3"},{"title":"Key Principles​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#key-principles","content":" Subsumption Architecture: Intelligence emerges from simple behaviors that are combined hierarchicallyEmergence: Complex behaviors arise from the interaction of simple componentsSituatedness: Intelligence is always situated in a particular environmentDynamism: Intelligence is a property of the dynamic interaction between agent and environment  ","version":"Next","tagName":"h3"},{"title":"Real-World Constraints in Robotics​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#real-world-constraints-in-robotics","content":" Physical robots face numerous constraints that digital systems do not encounter:  ","version":"Next","tagName":"h2"},{"title":"Physical Laws​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#physical-laws","content":" Gravity: All robots must account for gravitational forces affecting movement and stabilityFriction: Contact forces between surfaces affect locomotion and manipulationMomentum: Moving objects have inertia that must be managedConservation of Energy: Systems must operate within energy constraints  ","version":"Next","tagName":"h3"},{"title":"Environmental Factors​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#environmental-factors","content":" Variable Lighting: Vision systems must function under different lighting conditionsWeather Conditions: Outdoor robots must handle rain, snow, wind, temperature variationsDynamic Environments: Environments change over time, requiring continuous adaptationUnstructured Spaces: Real-world environments are not designed for robots  ","version":"Next","tagName":"h3"},{"title":"Hardware Limitations​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#hardware-limitations","content":" Processing Power: Embedded systems have limited computational resourcesBattery Life: Energy constraints limit operational timeActuator Limits: Physical components have force, speed, and precision limitationsSensor Noise: Real sensors provide imperfect information  ","version":"Next","tagName":"h3"},{"title":"The Anthropomorphic Focus​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#the-anthropomorphic-focus","content":" Humanoid robots specifically face unique challenges due to their human-like form:  ","version":"Next","tagName":"h2"},{"title":"Bipedal Locomotion​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#bipedal-locomotion","content":" Balance: Maintaining stability on two legs requires continuous adjustmentTerrain Navigation: Walking on stairs, slopes, and uneven surfacesEnergy Efficiency: Human locomotion is energetically expensive to replicateDynamic Stability: Maintaining balance during movement transitions  ","version":"Next","tagName":"h3"},{"title":"Dexterous Manipulation​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#dexterous-manipulation","content":" Fine Motor Control: Precise finger movements for delicate tasksGrasp Planning: Determining how to grasp objects of various shapes and materialsForce Control: Applying appropriate forces to manipulate objects without damageTool Use: Using human-designed tools effectively  ","version":"Next","tagName":"h3"},{"title":"Human-Centered Environments​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#human-centered-environments","content":" Humanoid robots must operate in environments designed for humans:  Door handles, light switches, and furniture sized for human useNavigation through spaces with human traffic patternsSocial interaction following human norms and expectations  ","version":"Next","tagName":"h3"},{"title":"Morphological Computation​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#morphological-computation","content":" Morphological computation refers to the idea that the physical form of a robot contributes to its computational capabilities:  ","version":"Next","tagName":"h2"},{"title":"Passive Dynamics​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#passive-dynamics","content":" Pendulum Motion: Leg swing in walking can be partially driven by passive dynamicsSpring-Mass Systems: Compliant structures can store and release energy efficientlyMechanical Advantage: Joint configurations can provide natural force amplification  ","version":"Next","tagName":"h3"},{"title":"Material Properties​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#material-properties","content":" Compliance: Soft materials can provide safe interaction and shock absorptionFlexibility: Flexible structures can adapt to irregular surfacesInertia: Mass distribution affects stability and maneuverability  ","version":"Next","tagName":"h3"},{"title":"Sensory-Motor Coordination​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#sensory-motor-coordination","content":" Physical AI systems must maintain tight coordination between perception and action:  ","version":"Next","tagName":"h2"},{"title":"Real-Time Processing Requirements​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#real-time-processing-requirements","content":" Feedback Loops: Control systems must operate within strict timing constraintsLatency Management: Delays in perception or action can cause instabilitySynchronization: Multiple sensors and actuators must be coordinated precisely  ","version":"Next","tagName":"h3"},{"title":"Multimodal Integration​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#multimodal-integration","content":" Sensor Fusion: Combining information from multiple sensors (vision, touch, proprioception)Cross-Modal Learning: Information from one modality can enhance anotherPredictive Processing: Anticipating sensory input based on motor commands  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Balancing on Two Legs​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#example-1-balancing-on-two-legs","content":" A humanoid robot maintains balance through:  Vision: Detecting the environment and potential obstaclesProprioception: Sensing joint angles and body positionInertial Measurement: Detecting accelerations and angular velocitiesActuation: Adjusting joint torques to maintain stability  The physical form (two legs, center of mass) is integral to the balancing strategy.  ","version":"Next","tagName":"h3"},{"title":"Example 2: Grasping an Object​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#example-2-grasping-an-object","content":" A humanoid robot grasps an object through:  Vision: Identifying object shape, size, and positionTactile Feedback: Sensing contact and grip forceMotor Control: Coordinating finger movements and applying appropriate forcesAdaptive Control: Adjusting grip based on object properties  ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Morphological Computation Analysis​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#exercise-1-morphological-computation-analysis","content":" Identify three examples of morphological computation in humanoid robots. For each example, explain how the physical form contributes to the robot's computational capabilities.  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Constraint Prioritization​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#exercise-2-constraint-prioritization","content":" Rank the real-world constraints listed in this chapter by their impact on humanoid robot performance. Justify your ranking with specific examples.  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Embodied Design Challenge​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#exercise-3-embodied-design-challenge","content":" Design a simple task (e.g., walking up stairs) and explain how the humanoid form both enables and constrains the robot's ability to perform this task.  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#summary","content":" Embodied intelligence emphasizes the fundamental role of physical form in intelligence. For humanoid robots, embodiment provides both advantages (ability to operate in human environments) and constraints (complexity of bipedal locomotion and dexterous manipulation). Understanding these principles is essential for developing effective Physical AI systems that can operate in real-world environments.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 2 - Embodied Intelligence & Real-World Constraints","url":"/docs/chapters/module-1-foundations/chapter-2-embodied-intelligence#further-reading","content":" &quot;Understanding Intelligence&quot; by Rolf Pfeifer and Christian Scheier&quot;How the Body Shapes the Way We Think&quot; by Rolf Pfeifer and Josh Bongard&quot;The Embodied Mind&quot; by Francisco Varela, Evan Thompson, and Eleanor Rosch ","version":"Next","tagName":"h2"},{"title":"Chapter 4 - Sensors & Perception Systems","type":0,"sectionRef":"#","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#learning-objectives","content":" Identify various types of sensors used in roboticsUnderstand perception system architecturesAnalyze sensor fusion techniques for humanoid robots  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#introduction","content":" Sensors and perception systems form the foundation of Physical AI, enabling robots to understand and interact with the physical world. For humanoid robots operating in human-centered environments, sophisticated perception capabilities are essential for navigation, manipulation, and social interaction. This chapter explores the various sensors used in robotics and how they integrate into perception systems that support the Vision-Language-Action pipeline.  ","version":"Next","tagName":"h2"},{"title":"Types of Sensors in Robotics​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#types-of-sensors-in-robotics","content":" ","version":"Next","tagName":"h2"},{"title":"Vision Sensors​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#vision-sensors","content":" Vision sensors provide the primary means for environmental perception:  RGB Cameras​  Function: Capture color images of the environmentApplications: Object recognition, scene understanding, facial recognitionAdvantages: Rich visual information, low costLimitations: Performance varies with lighting conditions  Depth Sensors​  Function: Measure distance to objects in the sceneTypes: Stereo cameras: Use two cameras to calculate depthTime-of-flight: Measure light travel timeStructured light: Project patterns and analyze distortions Applications: 3D reconstruction, obstacle detection, grasp planning  Thermal Cameras​  Function: Detect heat signaturesApplications: Person detection, environmental monitoringAdvantages: Works in low-light conditionsLimitations: Lower resolution, specialized applications  ","version":"Next","tagName":"h3"},{"title":"Proprioceptive Sensors​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#proprioceptive-sensors","content":" Proprioceptive sensors provide information about the robot's own state:  Joint Encoders​  Function: Measure joint angles and velocitiesTypes: Absolute encoders, incremental encodersApplications: Motion control, kinematic calculationsAccuracy: Critical for precise control  Inertial Measurement Units (IMU)​  Function: Measure acceleration and angular velocityComponents: Accelerometers, gyroscopes (sometimes magnetometers)Applications: Balance control, orientation estimation, motion detectionCritical for: Bipedal locomotion stability  Force/Torque Sensors​  Function: Measure forces and torques at joints or end effectorsApplications: Grasp control, contact detection, compliant motionCritical for: Safe human-robot interaction  ","version":"Next","tagName":"h3"},{"title":"Tactile Sensors​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#tactile-sensors","content":" Tactile sensors provide information about physical contact:  Pressure Sensors​  Function: Detect contact and measure pressure distributionApplications: Grasp monitoring, surface explorationPlacement: Fingertips, palms, feet for balance  Temperature Sensors​  Function: Measure contact temperatureApplications: Object identification, safety monitoring  ","version":"Next","tagName":"h3"},{"title":"Auditory Sensors​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#auditory-sensors","content":" Microphones enable speech and sound processing:  Directional Microphones​  Function: Capture sound from specific directionsApplications: Sound source localization, speech recognitionArray Systems: Multiple microphones for enhanced processing  Noise Reduction​  Function: Filter environmental noiseApplications: Clear speech recognition in noisy environments  ","version":"Next","tagName":"h3"},{"title":"Perception System Architecture​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#perception-system-architecture","content":" A humanoid robot's perception system typically follows a hierarchical architecture:  ","version":"Next","tagName":"h2"},{"title":"Sensor Layer​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#sensor-layer","content":" Raw Data: Direct sensor readings (images, joint angles, IMU values)Preprocessing: Basic filtering, calibration, noise reductionSynchronization: Time-stamping and alignment of sensor data  ","version":"Next","tagName":"h3"},{"title":"Feature Extraction Layer​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#feature-extraction-layer","content":" Visual Features: Edges, corners, descriptors for object recognitionAudio Features: Spectral analysis, speech featuresKinematic Features: Joint position patterns, movement trajectories  ","version":"Next","tagName":"h3"},{"title":"Object Recognition Layer​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#object-recognition-layer","content":" Classification: Identify objects in the environmentLocalization: Determine object positions and orientationsTracking: Follow objects over time  ","version":"Next","tagName":"h3"},{"title":"Scene Understanding Layer​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#scene-understanding-layer","content":" Semantic Segmentation: Label image regions with semantic meaning3D Reconstruction: Build 3D models of the environmentScene Graphs: Represent relationships between objects  ","version":"Next","tagName":"h3"},{"title":"Cognitive Layer​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#cognitive-layer","content":" Intent Recognition: Understand human intentions from behaviorContext Awareness: Interpret situations based on environmentPlanning Integration: Feed perception results to action planning  ","version":"Next","tagName":"h3"},{"title":"Sensor Fusion Techniques​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#sensor-fusion-techniques","content":" Sensor fusion combines information from multiple sensors to improve perception accuracy:  ","version":"Next","tagName":"h2"},{"title":"Kalman Filtering​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#kalman-filtering","content":" Application: Combine noisy sensor readings over timeFunction: Estimate true state from uncertain measurementsUse Case: IMU and vision fusion for object tracking  ","version":"Next","tagName":"h3"},{"title":"Particle Filtering​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#particle-filtering","content":" Application: Non-linear, non-Gaussian estimation problemsFunction: Represent probability distributions with particlesUse Case: Robot localization in complex environments  ","version":"Next","tagName":"h3"},{"title":"Bayesian Networks​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#bayesian-networks","content":" Application: Reason with uncertain informationFunction: Combine prior knowledge with sensor evidenceUse Case: Multi-sensor object recognition  ","version":"Next","tagName":"h3"},{"title":"Deep Learning Fusion​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#deep-learning-fusion","content":" Application: Learn optimal fusion strategies from dataFunction: End-to-end learning of sensor integrationUse Case: VLA pipeline integration of vision and language  ","version":"Next","tagName":"h3"},{"title":"Vision Processing for Humanoid Robots​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#vision-processing-for-humanoid-robots","content":" Vision processing is particularly critical for humanoid robots:  ","version":"Next","tagName":"h2"},{"title":"Object Detection and Recognition​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#object-detection-and-recognition","content":" Real-time Processing: Essential for dynamic environmentsMulti-class Recognition: Identify various objects in human environmentsRobustness: Handle lighting, occlusion, and viewpoint changes  ","version":"Next","tagName":"h3"},{"title":"Human Pose Estimation​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#human-pose-estimation","content":" Function: Detect human body positions and movementsApplications: Social interaction, gesture recognitionReal-time Requirements: Critical for natural interaction  ","version":"Next","tagName":"h3"},{"title":"SLAM (Simultaneous Localization and Mapping)​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#slam-simultaneous-localization-and-mapping","content":" Function: Build maps while localizing within themVisual SLAM: Use cameras for mapping and localizationApplications: Navigation in unknown environments  ","version":"Next","tagName":"h3"},{"title":"Grasp Planning​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#grasp-planning","content":" Function: Determine how to grasp objectsInputs: Object shape, size, material propertiesOutputs: Optimal grasp positions and forces  ","version":"Next","tagName":"h3"},{"title":"Audio Processing and Language Understanding​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#audio-processing-and-language-understanding","content":" Audio processing enables the language component of the VLA pipeline:  ","version":"Next","tagName":"h2"},{"title":"Speech Recognition​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#speech-recognition","content":" Acoustic Models: Convert audio to phonetic representationsLanguage Models: Convert phonemes to words and sentencesReal-time Processing: Critical for natural interaction  ","version":"Next","tagName":"h3"},{"title":"Sound Source Localization​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#sound-source-localization","content":" Function: Determine direction of sound sourcesApplications: Identify speakers in multi-person conversationsTechniques: Time difference of arrival, beamforming  ","version":"Next","tagName":"h3"},{"title":"Audio Classification​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#audio-classification","content":" Function: Identify environmental soundsApplications: Detecting alarms, doors closing, footstepsContext Awareness: Understanding environmental state  ","version":"Next","tagName":"h3"},{"title":"Tactile Perception​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#tactile-perception","content":" Tactile sensing provides crucial feedback for manipulation:  ","version":"Next","tagName":"h2"},{"title":"Contact Detection​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#contact-detection","content":" Function: Detect when robot touches objectsApplications: Grasp confirmation, surface explorationSensitivity: Critical for safe interaction  ","version":"Next","tagName":"h3"},{"title":"Force Control​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#force-control","content":" Function: Control applied forces during manipulationApplications: Gentle grasping, assembly tasksSafety: Prevent damage to objects and humans  ","version":"Next","tagName":"h3"},{"title":"Texture Recognition​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#texture-recognition","content":" Function: Identify object surface propertiesApplications: Material identification, quality assessmentIntegration: Combine with vision for complete object understanding  ","version":"Next","tagName":"h3"},{"title":"Sensor Integration Challenges​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#sensor-integration-challenges","content":" ","version":"Next","tagName":"h2"},{"title":"Data Synchronization​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#data-synchronization","content":" Challenge: Align sensor data from different sourcesSolution: Precise time-stamping and interpolationCritical for: Real-time control systems  ","version":"Next","tagName":"h3"},{"title":"Computational Constraints​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#computational-constraints","content":" Challenge: Process sensor data in real-time on embedded hardwareSolution: Efficient algorithms optimized for target hardwareTarget: NVIDIA Jetson Orin Nano (8GB) platform  ","version":"Next","tagName":"h3"},{"title":"Calibration​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#calibration","content":" Challenge: Maintain accurate sensor models over timeSolution: Regular calibration procedures and self-calibrationTypes: Intrinsic (internal parameters), extrinsic (spatial relationships)  ","version":"Next","tagName":"h3"},{"title":"Noise and Uncertainty​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#noise-and-uncertainty","content":" Challenge: Handle sensor noise and uncertaintySolution: Robust algorithms and uncertainty quantificationImportance: Critical for safe robot operation  ","version":"Next","tagName":"h3"},{"title":"The VLA Pipeline Integration​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#the-vla-pipeline-integration","content":" Sensors and perception systems are integral to the Vision-Language-Action pipeline:  ","version":"Next","tagName":"h2"},{"title":"Vision Component​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#vision-component","content":" Input: Camera, depth sensor, IMU dataProcessing: Object recognition, scene understanding, human detectionOutput: Semantic scene representation for action planning  ","version":"Next","tagName":"h3"},{"title":"Language Component​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#language-component","content":" Input: Microphone arrays for speechProcessing: Speech recognition, natural language understandingOutput: Semantic command interpretation for action planning  ","version":"Next","tagName":"h3"},{"title":"Action Component​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#action-component","content":" Input: Proprioceptive sensors for state feedbackProcessing: Motion planning with perception constraintsOutput: Actuator commands for locomotion and manipulation  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Object Grasping​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#example-1-object-grasping","content":" A humanoid robot grasps a cup using:  Vision: Identify cup location, orientation, and shapeTactile: Confirm contact and adjust grasp forceProprioceptive: Monitor joint positions and forcesIntegration: Combine all sensors for successful grasp  ","version":"Next","tagName":"h3"},{"title":"Example 2: Human Interaction​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#example-2-human-interaction","content":" A humanoid robot responds to a human command using:  Audio: Recognize speech command &quot;Please bring me the book&quot;Vision: Locate the specified book in the environmentAction: Navigate to book and grasp it appropriatelyIntegration: Coordinate all systems for task completion  ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Sensor Selection​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#exercise-1-sensor-selection","content":" For a humanoid robot designed to serve drinks in a café environment, select the appropriate sensors for each task (navigation, object detection, human interaction) and justify your choices.  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Fusion Algorithm Design​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#exercise-2-fusion-algorithm-design","content":" Design a sensor fusion algorithm that combines IMU data and camera-based visual odometry for robot localization. Consider the strengths and limitations of each sensor type.  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: VLA Pipeline Enhancement​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#exercise-3-vla-pipeline-enhancement","content":" Explain how tactile sensors would enhance the VLA pipeline for a humanoid robot performing delicate assembly tasks.  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#summary","content":" Sensors and perception systems form the foundation of Physical AI, enabling humanoid robots to understand and interact with the physical world. The integration of multiple sensor types through sophisticated fusion techniques allows robots to operate effectively in human-centered environments. Understanding these systems is crucial for developing the Vision-Language-Action pipeline that enables natural human-robot interaction.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 4 - Sensors & Perception Systems","url":"/docs/chapters/module-1-foundations/chapter-4-sensors-perception-systems#further-reading","content":" &quot;Probabilistic Robotics&quot; by Sebastian Thrun, Wolfram Burgard, and Dieter Fox&quot;Computer Vision: Algorithms and Applications&quot; by Richard Szeliski&quot;Handbook of Robotics&quot; by Bruno Siciliano and Oussama Khatib (Sensor Systems chapter) ","version":"Next","tagName":"h2"},{"title":"Chapter 3 - Humanoid Robotics Overview","type":0,"sectionRef":"#","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#learning-objectives","content":" Identify the key components of humanoid robotsUnderstand the architecture of humanoid systemsAnalyze the challenges specific to humanoid robotics  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#introduction","content":" Humanoid robotics represents one of the most challenging domains in robotics, requiring the integration of multiple complex systems to create robots that can operate effectively in human-centered environments. Unlike wheeled or specialized robots, humanoid robots must navigate the same spaces as humans, use the same tools, and interact with the same infrastructure. This chapter provides an overview of humanoid robot architecture and key components.  ","version":"Next","tagName":"h2"},{"title":"What Makes Humanoid Robotics Challenging?​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#what-makes-humanoid-robotics-challenging","content":" Humanoid robotics is particularly challenging due to several factors:  ","version":"Next","tagName":"h2"},{"title":"Bipedal Locomotion​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#bipedal-locomotion","content":" Maintaining balance on two legs requires sophisticated control algorithmsWalking on uneven terrain, stairs, and slopes presents unique challengesEnergy efficiency is difficult to achieve compared to wheeled systems  ","version":"Next","tagName":"h3"},{"title":"Dexterous Manipulation​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#dexterous-manipulation","content":" Human-like hands require complex actuation for fine motor controlGrasping objects of various shapes, sizes, and materialsTool usage with human-designed implements  ","version":"Next","tagName":"h3"},{"title":"Human-Centered Environments​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#human-centered-environments","content":" Environments designed for human dimensions and capabilitiesNavigation through spaces with human traffic patternsSocial interaction following human norms and expectations  ","version":"Next","tagName":"h3"},{"title":"Humanoid Robot Architecture​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#humanoid-robot-architecture","content":" A typical humanoid robot consists of several key subsystems:  ","version":"Next","tagName":"h2"},{"title":"Mechanical Structure​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#mechanical-structure","content":" Skeleton: Lightweight, strong frame supporting all componentsJoints: Actuated degrees of freedom enabling movementEnd Effectors: Hands/feet designed for manipulation and locomotionCompliant Elements: Springs and dampers for safe interaction  ","version":"Next","tagName":"h3"},{"title":"Sensory Systems​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#sensory-systems","content":" Vision: Cameras for environmental perceptionProprioception: Joint encoders, IMUs for self-awarenessTactile Sensors: Force/torque sensors, touch sensorsAuditory: Microphones for sound processing and voice interaction  ","version":"Next","tagName":"h3"},{"title":"Actuation System​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#actuation-system","content":" Motors: Servo motors, brushless DC motors, or pneumatic/hydraulic actuatorsGearboxes: Reduction gears for increased torqueControllers: Motor controllers with position, velocity, and torque controlPower Distribution: Wiring and power management for all actuators  ","version":"Next","tagName":"h3"},{"title":"Computing Hardware​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#computing-hardware","content":" Main Computer: High-performance computing for AI and controlReal-time Controllers: Dedicated hardware for low-latency controlEdge Processing: Specialized chips for vision, audio processingCommunication: Network interfaces for distributed computing  ","version":"Next","tagName":"h3"},{"title":"Key Components in Detail​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#key-components-in-detail","content":" ","version":"Next","tagName":"h2"},{"title":"Actuators​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#actuators","content":" Actuators are the muscles of the robot, providing the force and motion for all movements:  Servo Motors: Precise position control with feedbackSeries Elastic Actuators (SEA): Compliant actuation for safe interactionPneumatic Muscles: Human-like compliance and power-to-weight ratioHydraulic Systems: High power output for heavy lifting  ","version":"Next","tagName":"h3"},{"title":"Sensors​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#sensors","content":" Sensors provide the robot's perception of the world:  Cameras: RGB, depth, thermal imaging for visionInertial Measurement Units (IMU): Acceleration and angular velocityJoint Encoders: Position, velocity, and torque feedbackForce/Torque Sensors: Contact force measurementTactile Sensors: Pressure and texture sensing on fingertips  ","version":"Next","tagName":"h3"},{"title":"Control Architecture​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#control-architecture","content":" The control system manages the robot's behavior:  Low-level Controllers: Joint position/velocity/torque controlBalance Controllers: Maintaining stability during locomotionMotion Planning: Trajectory generation for complex movementsBehavior Controllers: High-level task execution  ","version":"Next","tagName":"h3"},{"title":"Major Humanoid Robot Platforms​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#major-humanoid-robot-platforms","content":" ","version":"Next","tagName":"h2"},{"title":"Research Platforms​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#research-platforms","content":" Honda ASIMO: Pioneering humanoid with advanced bipedal walkingBoston Dynamics Atlas: High-performance humanoid for researchToyota HRP-4: Humanoid designed for human environmentsKawada HRP-2: Research platform for human-robot interaction  ","version":"Next","tagName":"h3"},{"title":"Commercial Platforms​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#commercial-platforms","content":" SoftBank Pepper: Humanoid for customer service applicationsSoftBank NAO: Small humanoid for education and researchUBTECH Alpha Series: Consumer humanoid robots  ","version":"Next","tagName":"h3"},{"title":"The VLA Pipeline in Humanoid Systems​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#the-vla-pipeline-in-humanoid-systems","content":" The Vision-Language-Action (VLA) pipeline is particularly important for humanoid robots:  ","version":"Next","tagName":"h2"},{"title":"Vision Component​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#vision-component","content":" Scene Understanding: Recognizing objects, people, and environmentsBody Tracking: Understanding human poses and gesturesSLAM: Simultaneous localization and mapping for navigationObject Recognition: Identifying targets for manipulation tasks  ","version":"Next","tagName":"h3"},{"title":"Language Component​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#language-component","content":" Speech Recognition: Converting human speech to textNatural Language Processing: Understanding commands and questionsCognitive Planning: Translating high-level goals to action sequencesSpeech Synthesis: Communicating with humans  ","version":"Next","tagName":"h3"},{"title":"Action Component​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#action-component","content":" Locomotion: Walking, climbing stairs, navigating obstaclesManipulation: Grasping, moving, and using objectsGestures: Expressive body language for communicationTask Execution: Completing complex multi-step tasks  ","version":"Next","tagName":"h3"},{"title":"Challenges and Limitations​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#challenges-and-limitations","content":" ","version":"Next","tagName":"h2"},{"title":"Technical Challenges​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#technical-challenges","content":" Energy Efficiency: Humanoid locomotion is energetically expensiveReal-time Processing: Complex AI algorithms must run in real-timeSafety: Ensuring safe interaction with humans and environmentRobustness: Operating reliably in unstructured environments  ","version":"Next","tagName":"h3"},{"title":"Economic Challenges​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#economic-challenges","content":" Cost: Complex systems require expensive componentsMaintenance: Sophisticated robots require specialized maintenanceReliability: Many failure points in complex mechanical systems  ","version":"Next","tagName":"h3"},{"title":"Social Challenges​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#social-challenges","content":" Acceptance: Public comfort with humanoid robotsEthics: Appropriate uses and behaviors for humanoid robotsIntegration: Incorporating robots into human workflows  ","version":"Next","tagName":"h3"},{"title":"Target Hardware: NVIDIA Jetson Orin Nano​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#target-hardware-nvidia-jetson-orin-nano","content":" Our humanoid robotics systems are designed to operate on the NVIDIA Jetson Orin Nano (8GB), which provides:  AI Performance: 40 TOPS for AI inferencePower Efficiency: Optimized for mobile robotics applicationsConnectivity: Multiple interfaces for sensors and actuatorsROS 2 Support: Full compatibility with Robot Operating System 2  This platform enables the deployment of complex VLA pipeline components while maintaining the power constraints necessary for mobile humanoid operation.  ","version":"Next","tagName":"h2"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Humanoid Navigation​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#example-1-humanoid-navigation","content":" A humanoid robot navigating through a crowded room must:  Vision: Detect humans, obstacles, and pathwaysLanguage: Process voice commands like &quot;Go to the kitchen&quot;Action: Plan bipedal walking trajectory while avoiding collisions  ","version":"Next","tagName":"h3"},{"title":"Example 2: Object Manipulation​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#example-2-object-manipulation","content":" A humanoid robot picking up a fragile object must:  Vision: Identify object location, shape, and fragilityLanguage: Understand command like &quot;Gently pick up the glass&quot;Action: Execute precise grasping with appropriate force control  ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Component Analysis​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#exercise-1-component-analysis","content":" For each component category (actuators, sensors, computing), identify the specific challenges that make humanoid implementations more complex than for wheeled robots.  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Architecture Design​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#exercise-2-architecture-design","content":" Design a high-level architecture for a humanoid robot that needs to operate in a home environment. Identify the key subsystems and their interconnections.  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: VLA Integration​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#exercise-3-vla-integration","content":" Explain how the VLA pipeline components would be distributed across the different subsystems of a humanoid robot, considering computational constraints.  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#summary","content":" Humanoid robotics represents one of the most challenging domains in robotics, requiring the integration of multiple complex systems. The anthropomorphic focus of our approach emphasizes the unique challenges of bipedal locomotion and dexterous manipulation in human-centered environments. Understanding the architecture and components of humanoid robots is essential for developing effective Physical AI systems that can operate in real-world environments.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 3 - Humanoid Robotics Overview","url":"/docs/chapters/module-1-foundations/chapter-3-humanoid-robotics-overview#further-reading","content":" &quot;Humanoid Robotics: A Reference&quot; by Ambarish Goswami and Prahlad Vadakkepat&quot;Humanoid Robots: Modeling and Control&quot; by Dragomir N. Nenchev, Atsushi Konno, and Teppei Tsujita&quot;The Development of Humanoid Robotics in Japan&quot; by Hirohisa Hirukawa ","version":"Next","tagName":"h2"},{"title":"Chapter 5 - ROS 2 Architecture & Core Concepts","type":0,"sectionRef":"#","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#learning-objectives","content":" Explain the core concepts of ROS 2 architectureIdentify the different communication patterns in ROS 2Understand QoS profiles for real-time control  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#introduction","content":" Robot Operating System 2 (ROS 2) serves as the nervous system for humanoid robots, enabling communication between different software components that control perception, decision-making, and action. For Physical AI systems that must operate in real-time with strict timing constraints, ROS 2 provides the architecture necessary to coordinate complex behaviors while meeting the real-time validation requirements mandated by our project constitution. This chapter explores the core concepts of ROS 2 architecture, with special emphasis on Quality of Service (QoS) profiles essential for humanoid stability and safety.  ","version":"Next","tagName":"h2"},{"title":"The ROS 2 Architecture​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#the-ros-2-architecture","content":" ROS 2 is a middleware framework that provides services designed for a heterogeneous computer cluster, including:  Hardware Abstraction: Interface with sensors and actuatorsDevice Drivers: Communication protocols for various hardwareLibraries: Reusable code for common robotics functionsVisualization Tools: Monitoring and debugging capabilitiesMessage-Passing: Communication between processesPackage Management: Organizing and distributing code  ","version":"Next","tagName":"h2"},{"title":"Key Design Principles​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#key-design-principles","content":" Distributed: Components can run on different machinesReal-time Capable: Supports real-time performance requirementsSecure: Built-in security features for safe operationReliable: Robust communication patternsLanguage Agnostic: Supports multiple programming languages  ","version":"Next","tagName":"h3"},{"title":"Core Components of ROS 2​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#core-components-of-ros-2","content":" ","version":"Next","tagName":"h2"},{"title":"Nodes​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#nodes","content":" Nodes are the fundamental building blocks of a ROS 2 system:  Definition: A process that performs computationFunction: Execute specific tasks like sensor processing or control algorithmsImplementation: Can be written in C++, Python, or other supported languagesLifecycle: Nodes can be started, stopped, and configured dynamically  graph TD A[Node A] --&gt; B[ROS 2 Middleware] C[Node B] --&gt; B D[Node C] --&gt; B B --&gt; E[DDS Implementation]   ","version":"Next","tagName":"h3"},{"title":"Communication Patterns​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#communication-patterns","content":" ROS 2 supports four main communication patterns:  Topics (Publish/Subscribe)​  Pattern: One-to-many communicationUse Case: Sensor data distribution, status updatesCharacteristics: Asynchronous, best-effort deliveryExample: Camera images, IMU data, joint states  Services (Request/Response)​  Pattern: One-to-one synchronous communicationUse Case: Action-on-demand, configuration requestsCharacteristics: Synchronous, guaranteed deliveryExample: Map saving, calibration requests  Actions (Goal/Result/Feedback)​  Pattern: Long-running tasks with progress updatesUse Case: Navigation, manipulation tasksCharacteristics: Synchronous goal, asynchronous feedbackExample: Move to pose, pick and place operations  Parameters​  Pattern: Configuration managementUse Case: System configuration, tuning valuesCharacteristics: Synchronous access, change notificationsExample: Control gains, sensor calibration values  ","version":"Next","tagName":"h3"},{"title":"Quality of Service (QoS) Profiles​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#quality-of-service-qos-profiles","content":" Quality of Service profiles are critical for real-time control in humanoid robots, directly addressing our constitution's requirement for &quot;ROS 2 QoS profiles configuration for bipedal balance feedback&quot;:  ","version":"Next","tagName":"h2"},{"title":"Reliability Policy​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#reliability-policy","content":" Reliable: All messages are delivered (with retries)Best Effort: Messages may be lost, no retries  ","version":"Next","tagName":"h3"},{"title":"Durability Policy​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#durability-policy","content":" Transient Local: Late-joining subscribers receive last known valueVolatile: No historical data retained  ","version":"Next","tagName":"h3"},{"title":"History Policy​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#history-policy","content":" Keep Last: Maintain N most recent messagesKeep All: Maintain all messages (memory intensive)  ","version":"Next","tagName":"h3"},{"title":"Lifespan Policy​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#lifespan-policy","content":" Duration: How long messages remain validDefault: Messages valid indefinitely  ","version":"Next","tagName":"h3"},{"title":"Specific QoS Settings for Humanoid Control​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#specific-qos-settings-for-humanoid-control","content":" # Critical for bipedal balance feedback balance_qos = QoSProfile( depth=1, # Only most recent value matters reliability=ReliabilityPolicy.RELIABLE, durability=DurabilityPolicy.VOLATILE, history=HistoryPolicy.KEEP_LAST ) # For sensor data with real-time requirements sensor_qos = QoSProfile( depth=5, reliability=ReliabilityPolicy.BEST_EFFORT, # Accept some loss for timeliness durability=DurabilityPolicy.VOLATILE, history=HistoryPolicy.KEEP_LAST )   ","version":"Next","tagName":"h3"},{"title":"The DDS Middleware​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#the-dds-middleware","content":" ROS 2 uses Data Distribution Service (DDS) as its underlying communication middleware:  Implementation: Various DDS vendors (Fast DDS, Cyclone DDS, RTI Connext)Function: Provides discovery, data delivery, and QoS enforcementAdvantages: Industry standard, real-time capable, configurable  ","version":"Next","tagName":"h2"},{"title":"Discovery Process​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#discovery-process","content":" Automatic: Nodes automatically discover each otherDynamic: New nodes can join/leave without system restartRobust: Handles network partitions and reconnections  ","version":"Next","tagName":"h3"},{"title":"Real-Time Considerations​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#real-time-considerations","content":" For humanoid robots requiring real-time performance:  ","version":"Next","tagName":"h2"},{"title":"Deterministic Behavior​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#deterministic-behavior","content":" Message Delivery: QoS profiles ensure predictable timingResource Management: Proper thread and process managementLatency Control: Minimize communication delays  ","version":"Next","tagName":"h3"},{"title":"Bipedal Balance Requirements​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#bipedal-balance-requirements","content":" Humanoid robots have strict timing requirements for balance control:  High Frequency: Balance updates often required at 100Hz or higherLow Latency: Feedback loops must close quickly to maintain stabilityReliability: Balance-critical messages must be delivered reliably  ","version":"Next","tagName":"h3"},{"title":"QoS Configuration for Balance​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#qos-configuration-for-balance","content":" # Configuration specifically for balance-critical data balance_control_qos = QoSProfile( depth=1, reliability=ReliabilityPolicy.RELIABLE, durability=DurabilityPolicy.VOLATILE, history=HistoryPolicy.KEEP_LAST, lifespan=Duration(seconds=0.01) # 10ms lifespan for balance data )   ","version":"Next","tagName":"h3"},{"title":"Package Structure​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#package-structure","content":" ROS 2 packages organize code and resources:  ","version":"Next","tagName":"h2"},{"title":"Package.xml​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#packagexml","content":" Metadata: Package name, version, description, dependenciesMaintainer: Contact information for package maintainersLicense: Legal licensing information  ","version":"Next","tagName":"h3"},{"title":"CMakeLists.txt (C++)​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#cmakeliststxt-c","content":" Build Configuration: Compiler settings, dependenciesExecutable Definitions: Which programs to buildInstallation Rules: Where to install files  ","version":"Next","tagName":"h3"},{"title":"setup.py (Python)​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#setuppy-python","content":" Python Package Configuration: Entry points, dependenciesInstallation Settings: Where to install Python modules  ","version":"Next","tagName":"h3"},{"title":"ROS 2 Tools and Utilities​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#ros-2-tools-and-utilities","content":" ","version":"Next","tagName":"h2"},{"title":"Command Line Tools​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#command-line-tools","content":" ros2 run: Execute nodesros2 topic: Monitor and publish to topicsros2 service: Call servicesros2 action: Send action goalsros2 param: Configure parametersros2 node: Monitor nodes  ","version":"Next","tagName":"h3"},{"title":"Visualization Tools​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#visualization-tools","content":" RViz2: 3D visualization of robot state and sensor datarqt: GUI framework for custom toolsros2 bag: Data recording and playback  ","version":"Next","tagName":"h3"},{"title":"Security Features​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#security-features","content":" ROS 2 includes built-in security capabilities:  Authentication: Verify node identityEncryption: Protect message contentsAccess Control: Restrict node interactionsSecure Discovery: Protected node discovery process  ","version":"Next","tagName":"h2"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Balance Control Node​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#example-1-balance-control-node","content":" A ROS 2 node for humanoid balance control:  import rclpy from rclpy.node import Node from sensor_msgs.msg import Imu from geometry_msgs.msg import Twist from rclpy.qos import QoSProfile, ReliabilityPolicy, DurabilityPolicy, HistoryPolicy class BalanceController(Node): def __init__(self): super().__init__('balance_controller') # Configure QoS for balance-critical IMU data balance_qos = QoSProfile( depth=1, reliability=ReliabilityPolicy.RELIABLE, durability=DurabilityPolicy.VOLATILE, history=HistoryPolicy.KEEP_LAST ) # Subscribe to IMU data with appropriate QoS self.imu_sub = self.create_subscription( Imu, '/imu/data', self.imu_callback, balance_qos ) # Publish balance corrections self.cmd_vel_pub = self.create_publisher( Twist, '/balance/cmd_vel', 1 ) # Balance control timer (100Hz for stability) self.timer = self.create_timer(0.01, self.balance_control_loop) def imu_callback(self, msg): # Process IMU data for balance feedback self.current_orientation = msg.orientation self.current_angular_velocity = msg.angular_velocity def balance_control_loop(self): # Implement balance control algorithm # This runs at 100Hz to maintain stability cmd = Twist() # ... balance control logic self.cmd_vel_pub.publish(cmd) def main(args=None): rclpy.init(args=args) balance_controller = BalanceController() rclpy.spin(balance_controller) balance_controller.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()   ","version":"Next","tagName":"h3"},{"title":"Example 2: Sensor Data Processing​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#example-2-sensor-data-processing","content":" A node that processes multiple sensor streams:  class SensorFusionNode(Node): def __init__(self): super().__init__('sensor_fusion') # Different QoS profiles for different sensor types # Camera: best effort, larger buffer for image processing camera_qos = QoSProfile( depth=10, reliability=ReliabilityPolicy.BEST_EFFORT, durability=DurabilityPolicy.VOLATILE, history=HistoryPolicy.KEEP_LAST ) # Joint states: reliable, small buffer for control joint_qos = QoSProfile( depth=1, reliability=ReliabilityPolicy.RELIABLE, durability=DurabilityPolicy.VOLATILE, history=HistoryPolicy.KEEP_LAST ) self.camera_sub = self.create_subscription( Image, '/camera/image_raw', self.camera_callback, camera_qos) self.joint_sub = self.create_subscription( JointState, '/joint_states', self.joint_callback, joint_qos)   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#constitution-alignment","content":" This chapter directly addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#real-time-validation-principle-iv","content":" QoS profiles enable low-latency control for bipedal balance feedbackConfigurable timing constraints support real-time requirements  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#target-hardware-optimization","content":" Examples optimized for Jetson Orin Nano computational constraintsEfficient communication patterns for embedded systems  ","version":"Next","tagName":"h3"},{"title":"VLA Pipeline Integration​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#vla-pipeline-integration","content":" ROS 2 architecture supports the integration of vision, language, and action systemsCommunication patterns enable coordination between VLA components  ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: QoS Profile Design​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#exercise-1-qos-profile-design","content":" Design appropriate QoS profiles for the following humanoid robot topics:  Joint position commands (critical for safety)Camera images (for vision processing)Battery status (for monitoring)Balance feedback (for stability control)  Justify your choices based on real-time requirements.  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Node Architecture​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#exercise-2-node-architecture","content":" Design a ROS 2 node architecture for a humanoid robot performing object manipulation. Identify the nodes, topics, services, and actions needed. Specify appropriate QoS profiles for each communication channel.  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Performance Analysis​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#exercise-3-performance-analysis","content":" Analyze the timing requirements for bipedal balance control in humanoid robots. Explain why QoS profiles are critical for maintaining stability and safety.  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#summary","content":" ROS 2 provides the architectural foundation for humanoid robot control systems, with Quality of Service profiles being particularly important for real-time applications like bipedal balance control. Understanding the core concepts of ROS 2 architecture, including nodes, topics, services, and actions, is essential for developing effective Physical AI systems that can operate in real-time with strict timing constraints.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 5 - ROS 2 Architecture & Core Concepts","url":"/docs/chapters/module-2-ros/chapter-5-ros2-architecture#further-reading","content":" &quot;Programming Robots with ROS&quot; by Morgan Quigley, Brian Gerkey, and William Smart&quot;Effective Robotics Programming with ROS&quot; by Anil Mahtani, Luis Sanchez Crespo, and Enrique Fernandez&quot;ROS 2 for Absolute Beginners&quot; by Anis Koubaa&quot;Real-Time Systems and Robotics&quot; by Chen and Liu ","version":"Next","tagName":"h2"},{"title":"Chapter 6 - Creating ROS 2 Packages (Python)","type":0,"sectionRef":"#","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#learning-objectives","content":" Create ROS 2 packages using PythonImplement nodes with proper build system configurationsConfigure QoS profiles for real-time applications  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#introduction","content":" Creating ROS 2 packages is fundamental to building modular, reusable robotics software. For Physical AI systems that must operate in real-time with strict timing constraints, proper package structure and configuration are essential. This chapter provides step-by-step instructions for creating ROS 2 packages in Python, with special attention to the production-ready requirements specified in our project constitution, including proper build system configurations (package.xml, setup.py) necessary for deployment on target hardware like the NVIDIA Jetson Orin Nano.  ","version":"Next","tagName":"h2"},{"title":"ROS 2 Package Structure​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#ros-2-package-structure","content":" A standard ROS 2 package follows this directory structure:  my_robot_package/ ├── CMakeLists.txt # Build configuration for C++ ├── package.xml # Package metadata and dependencies ├── setup.py # Python package configuration ├── setup.cfg # Installation configuration ├── resource/ # Resource files │ └── my_robot_package # Executable resource links ├── my_robot_package/ # Python source code │ ├── __init__.py │ ├── my_node.py │ └── my_module.py └── test/ # Test files └── test_my_node.py   ","version":"Next","tagName":"h2"},{"title":"Creating a Python Package with colcon​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#creating-a-python-package-with-colcon","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Create the Package Directory​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#step-1-create-the-package-directory","content":" mkdir -p ~/ros2_ws/src/my_robot_package cd ~/ros2_ws/src/my_robot_package   ","version":"Next","tagName":"h3"},{"title":"Step 2: Create package.xml​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#step-2-create-packagexml","content":" The package.xml file contains metadata about your package:  &lt;?xml version=&quot;1.0&quot;?&gt; &lt;?xml-model href=&quot;http://download.ros.org/schema/package_format3.xsd&quot; schematypens=&quot;http://www.w3.org/2001/XMLSchema&quot;?&gt; &lt;package format=&quot;3&quot;&gt; &lt;name&gt;my_robot_package&lt;/name&gt; &lt;version&gt;0.0.0&lt;/version&gt; &lt;description&gt;Example package for humanoid robot control&lt;/description&gt; &lt;maintainer email=&quot;user@example.com&quot;&gt;Your Name&lt;/maintainer&gt; &lt;license&gt;Apache-2.0&lt;/license&gt; &lt;exec_depend&gt;rclpy&lt;/exec_depend&gt; &lt;exec_depend&gt;std_msgs&lt;/exec_depend&gt; &lt;exec_depend&gt;sensor_msgs&lt;/exec_depend&gt; &lt;exec_depend&gt;geometry_msgs&lt;/exec_depend&gt; &lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt; &lt;test_depend&gt;ament_copyright&lt;/test_depend&gt; &lt;test_depend&gt;ament_flake8&lt;/test_depend&gt; &lt;test_depend&gt;ament_pep257&lt;/test_depend&gt; &lt;test_depend&gt;python3-pytest&lt;/test_depend&gt; &lt;export&gt; &lt;build_type&gt;ament_python&lt;/build_type&gt; &lt;/export&gt; &lt;/package&gt;   ","version":"Next","tagName":"h3"},{"title":"Step 3: Create setup.py​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#step-3-create-setuppy","content":" The setup.py file configures your Python package:  from setuptools import setup import os from glob import glob package_name = 'my_robot_package' setup( name=package_name, version='0.0.0', packages=[package_name], data_files=[ ('share/ament_index/resource_index/packages', ['resource/' + package_name]), ('share/' + package_name, ['package.xml']), # Include all launch files (os.path.join('share', package_name, 'launch'), glob('launch/*launch.[pxy][yma]*')), ], install_requires=['setuptools'], zip_safe=True, maintainer='Your Name', maintainer_email='user@example.com', description='Example package for humanoid robot control', license='Apache License 2.0', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'my_robot_node = my_robot_package.my_robot_node:main', 'balance_controller = my_robot_package.balance_controller:main', 'vision_processor = my_robot_package.vision_processor:main', ], }, )   ","version":"Next","tagName":"h3"},{"title":"Step 4: Create setup.cfg​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#step-4-create-setupcfg","content":" [develop] script-dir=$base/lib/my_robot_package [install] install-scripts=$base/lib/my_robot_package   ","version":"Next","tagName":"h3"},{"title":"Creating Your First ROS 2 Node​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#creating-your-first-ros-2-node","content":" ","version":"Next","tagName":"h2"},{"title":"Step 5: Create the Python Package Directory​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#step-5-create-the-python-package-directory","content":" mkdir my_robot_package touch my_robot_package/__init__.py   ","version":"Next","tagName":"h3"},{"title":"Step 6: Create a Basic Node​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#step-6-create-a-basic-node","content":" Create my_robot_package/my_robot_node.py:  import rclpy from rclpy.node import Node from std_msgs.msg import String from sensor_msgs.msg import JointState from geometry_msgs.msg import Twist from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy, DurabilityPolicy class MyRobotNode(Node): def __init__(self): super().__init__('my_robot_node') # Create a publisher for robot status self.status_publisher = self.create_publisher( String, 'robot_status', 10 ) # Create a subscriber for joint commands # Using QoS profile appropriate for control control_qos = QoSProfile( depth=1, reliability=ReliabilityPolicy.RELIABLE, durability=DurabilityPolicy.VOLATILE, history=HistoryPolicy.KEEP_LAST ) self.joint_subscriber = self.create_subscription( JointState, 'joint_commands', self.joint_callback, control_qos ) # Timer for periodic status updates self.timer = self.create_timer(1.0, self.timer_callback) self.i = 0 self.get_logger().info('MyRobotNode has been started') def joint_callback(self, msg): self.get_logger().info(f'Received joint positions: {msg.position}') # Process joint commands here def timer_callback(self): msg = String() msg.data = f'Robot status: Running for {self.i} seconds' self.status_publisher.publish(msg) self.i += 1 def main(args=None): rclpy.init(args=args) my_robot_node = MyRobotNode() try: rclpy.spin(my_robot_node) except KeyboardInterrupt: pass finally: my_robot_node.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()   ","version":"Next","tagName":"h3"},{"title":"Production-Ready Node with Real-Time Considerations​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#production-ready-node-with-real-time-considerations","content":" For humanoid robots requiring real-time performance, nodes must be designed with timing constraints in mind. Here's an example of a balance controller node:  Create my_robot_package/balance_controller.py:  import rclpy from rclpy.node import Node from sensor_msgs.msg import Imu from geometry_msgs.msg import Twist from std_msgs.msg import Float64MultiArray from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy, DurabilityPolicy from rclpy.duration import Duration import numpy as np from collections import deque class BalanceController(Node): def __init__(self): super().__init__('balance_controller') # Configuration parameters self.declare_parameter('control_frequency', 100) # 100Hz for balance self.declare_parameter('max_torque', 100.0) self.declare_parameter('balance_kp', 10.0) # Proportional gain self.declare_parameter('balance_kd', 1.0) # Derivative gain self.control_frequency = self.get_parameter('control_frequency').value self.max_torque = self.get_parameter('max_torque').value self.kp = self.get_parameter('balance_kp').value self.kd = self.get_parameter('balance_kd').value # Critical QoS for balance feedback balance_qos = QoSProfile( depth=1, # Only most recent value matters reliability=ReliabilityPolicy.RELIABLE, # Must not lose balance data durability=DurabilityPolicy.VOLATILE, history=HistoryPolicy.KEEP_LAST, lifespan=Duration(seconds=0.01) # 10ms lifespan for balance data ) # Publishers and subscribers self.imu_sub = self.create_subscription( Imu, '/imu/data', self.imu_callback, balance_qos ) self.joint_cmd_pub = self.create_publisher( Float64MultiArray, '/joint_group_position_controller/commands', 1 ) # Store previous values for derivative calculation self.prev_pitch = 0.0 self.prev_time = self.get_clock().now() # Initialize timing for control loop control_period = 1.0 / self.control_frequency self.control_timer = self.create_timer( control_period, self.balance_control_loop ) self.get_logger().info(f'Balance controller started at {self.control_frequency}Hz') def imu_callback(self, msg): &quot;&quot;&quot;Process IMU data for balance feedback&quot;&quot;&quot; # Extract pitch angle from IMU orientation # This is simplified - in practice, you'd use proper quaternion math orientation = msg.orientation pitch = self.quaternion_to_pitch(orientation) current_time = self.get_clock().now() dt = (current_time - self.prev_time).nanoseconds / 1e9 # Store values for derivative calculation self.current_pitch = pitch self.dt = dt self.prev_time = current_time self.prev_pitch = pitch def quaternion_to_pitch(self, q): &quot;&quot;&quot;Convert quaternion to pitch angle&quot;&quot;&quot; # Simplified conversion - use proper math in production sinr_cosp = 2 * (q.w * q.y - q.z * q.x) cosr_cosp = 1 - 2 * (q.y * q.y + q.x * q.x) pitch = np.arctan2(sinr_cosp, cosr_cosp) return pitch def balance_control_loop(self): &quot;&quot;&quot;Main balance control loop running at configured frequency&quot;&quot;&quot; try: # Calculate pitch error (assuming desired pitch is 0) pitch_error = -self.current_pitch # Negative to correct tilt # Calculate derivative for PD control if hasattr(self, 'dt') and self.dt &gt; 0: pitch_rate = (self.current_pitch - self.prev_pitch) / self.dt else: pitch_rate = 0.0 # PD control law control_output = self.kp * pitch_error - self.kd * pitch_rate # Apply torque limits control_output = max(min(control_output, self.max_torque), -self.max_torque) # Create joint command message cmd_msg = Float64MultiArray() cmd_msg.data = [control_output] # Simplified - real system would have multiple joints # Publish control command self.joint_cmd_pub.publish(cmd_msg) # Log control performance (only occasionally to avoid spam) if hasattr(self, '_log_counter'): self._log_counter += 1 else: self._log_counter = 0 if self._log_counter % 100 == 0: # Log every 100 iterations self.get_logger().info( f'Balance: pitch={self.current_pitch:.3f}, ' f'control={control_output:.3f}, ' f'freq={1.0/self.dt:.1f}Hz' if self.dt &gt; 0 else 'freq=N/A' ) except AttributeError: # Handle case where IMU data hasn't arrived yet pass except Exception as e: self.get_logger().error(f'Balance control error: {e}') def main(args=None): rclpy.init(args=args) balance_controller = BalanceController() try: rclpy.spin(balance_controller) except KeyboardInterrupt: balance_controller.get_logger().info('Balance controller shutting down...') finally: balance_controller.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()   ","version":"Next","tagName":"h2"},{"title":"Launch Files for Node Management​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#launch-files-for-node-management","content":" Create a launch directory and launch file:  mkdir launch   Create launch/my_robot_launch.py:  from launch import LaunchDescription from launch_ros.actions import Node from ament_index_python.packages import get_package_share_directory import os def generate_launch_description(): return LaunchDescription([ Node( package='my_robot_package', executable='my_robot_node', name='my_robot_node', output='screen', parameters=[ # Add parameters here if needed ] ), Node( package='my_robot_package', executable='balance_controller', name='balance_controller', output='screen', parameters=[ {'control_frequency': 100}, # 100Hz for balance {'max_torque': 50.0}, {'balance_kp': 15.0}, {'balance_kd': 2.0} ] ) ])   ","version":"Next","tagName":"h2"},{"title":"Building and Running Your Package​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#building-and-running-your-package","content":" ","version":"Next","tagName":"h2"},{"title":"Step 7: Build the Package​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#step-7-build-the-package","content":" cd ~/ros2_ws colcon build --packages-select my_robot_package source install/setup.bash   ","version":"Next","tagName":"h3"},{"title":"Step 8: Run the Package​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#step-8-run-the-package","content":" # Run a single node ros2 run my_robot_package my_robot_node # Or run with launch file ros2 launch my_robot_package my_robot_launch.py   ","version":"Next","tagName":"h3"},{"title":"Testing Your Package​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#testing-your-package","content":" Create test/test_my_robot_node.py:  import unittest import rclpy from rclpy.executors import SingleThreadedExecutor from my_robot_package.my_robot_node import MyRobotNode class TestMyRobotNode(unittest.TestCase): @classmethod def setUpClass(cls): rclpy.init() @classmethod def tearDownClass(cls): rclpy.shutdown() def setUp(self): self.node = MyRobotNode() self.executor = SingleThreadedExecutor() self.executor.add_node(self.node) def tearDown(self): self.node.destroy_node() def test_node_creation(self): &quot;&quot;&quot;Test that the node was created successfully&quot;&quot;&quot; self.assertIsNotNone(self.node) self.assertEqual(self.node.get_name(), 'my_robot_node') if __name__ == '__main__': unittest.main()   ","version":"Next","tagName":"h2"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Production-Ready Code (Key Standard III)​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#production-ready-code-key-standard-iii","content":" Proper build system configurations (package.xml, setup.py)Error handling and loggingParameter configuration for different deployments  ","version":"Next","tagName":"h3"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#real-time-validation-principle-iv","content":" QoS profile configuration for low-latency controlHigh-frequency control loops for bipedal balance feedbackTiming considerations for real-time applications  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#target-hardware-optimization","content":" Code optimized for Jetson Orin Nano computational constraintsEfficient memory usage and processing patterns  ","version":"Next","tagName":"h3"},{"title":"Best Practices for Humanoid Robot Packages​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#best-practices-for-humanoid-robot-packages","content":" ","version":"Next","tagName":"h2"},{"title":"1. Real-Time Safety​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#1-real-time-safety","content":" Use appropriate QoS profiles for safety-critical communicationsImplement proper error handling and recoveryDesign control loops with known timing characteristics  ","version":"Next","tagName":"h3"},{"title":"2. Modularity​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#2-modularity","content":" Separate concerns into different nodesUse parameters for configurationImplement proper interfaces between components  ","version":"Next","tagName":"h3"},{"title":"3. Resource Management​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#3-resource-management","content":" Monitor CPU and memory usageImplement efficient algorithms for embedded systemsUse appropriate data structures for real-time performance  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Vision Processing Node​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#example-1-vision-processing-node","content":" A ROS 2 node for processing camera images:  import rclpy from rclpy.node import Node from sensor_msgs.msg import Image from cv_bridge import CvBridge from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy import cv2 import numpy as np class VisionProcessor(Node): def __init__(self): super().__init__('vision_processor') # Use appropriate QoS for camera data camera_qos = QoSProfile( depth=1, # Only most recent image matters reliability=ReliabilityPolicy.BEST_EFFORT, # Can drop frames if needed durability=DurabilityPolicy.VOLATILE, history=HistoryPolicy.KEEP_LAST ) self.image_sub = self.create_subscription( Image, '/camera/image_raw', self.image_callback, camera_qos ) self.bridge = CvBridge() self.get_logger().info('Vision processor started') def image_callback(self, msg): try: # Convert ROS image to OpenCV cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8') # Process image (example: edge detection) gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY) edges = cv2.Canny(gray, 50, 150) # Log processing time for performance monitoring self.get_logger().debug('Image processed successfully') except Exception as e: self.get_logger().error(f'Image processing error: {e}') def main(args=None): rclpy.init(args=args) vision_processor = VisionProcessor() try: rclpy.spin(vision_processor) except KeyboardInterrupt: pass finally: vision_processor.destroy_node() rclpy.shutdown()   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Package Creation​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#exercise-1-package-creation","content":" Create a ROS 2 package for a humanoid robot's walking controller. Include proper package.xml, setup.py, and a node that implements a simple walking gait with QoS profiles appropriate for locomotion control.  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Parameter Configuration​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#exercise-2-parameter-configuration","content":" Modify the balance controller node to accept additional parameters for different walking speeds and turning rates. Implement parameter validation to ensure safe operation.  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Multi-Node System​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#exercise-3-multi-node-system","content":" Design a ROS 2 system with multiple nodes (balance control, vision processing, and high-level planning) that work together for humanoid navigation. Specify the communication patterns and QoS profiles between nodes.  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#summary","content":" Creating ROS 2 packages in Python requires attention to proper build system configurations, QoS profiles for real-time applications, and production-ready code practices. For humanoid robots with strict timing constraints, these considerations are critical for safety and stability. Understanding the package structure and configuration files enables the development of modular, reusable robotics software that can be deployed on target hardware like the NVIDIA Jetson Orin Nano.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 6 - Creating ROS 2 Packages (Python)","url":"/docs/chapters/module-2-ros/chapter-6-ros2-packages#further-reading","content":" &quot;ROS 2 Documentation&quot; - Official ROS 2 tutorials and API documentation&quot;Programming Robots with ROS&quot; by Quigley et al.&quot;Effective Robotics Programming with ROS&quot; by Mahtani et al.&quot;Real-Time Systems and Robotics&quot; for timing-critical applications ","version":"Next","tagName":"h2"},{"title":"Chapter 7 - URDF & XACRO for Humanoid Robots","type":0,"sectionRef":"#","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#learning-objectives","content":" Create URDF models for humanoid robotsUse XACRO for complex robot descriptionsImplement anthropomorphic design principles  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#introduction","content":" Unified Robot Description Format (URDF) and its XML macro extension (XACRO) are fundamental tools for describing robot kinematics, dynamics, and visual properties in ROS 2. For humanoid robots operating in human-centered environments, proper URDF modeling is essential to capture the unique challenges of bipedal locomotion and dexterous manipulation as mandated by our project constitution's Anthropomorphic Focus principle. This chapter covers the creation of sophisticated URDF models for humanoid robots, with special attention to the anthropomorphic design requirements and the complex kinematic structures necessary for human-like movement.  ","version":"Next","tagName":"h2"},{"title":"Understanding URDF​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#understanding-urdf","content":" URDF (Unified Robot Description Format) is an XML format for representing a robot model:  Kinematics: Joint and link relationships defining robot structureDynamics: Mass, inertia, and friction propertiesVisual: Appearance for simulation and visualizationCollision: Collision geometry for physics simulation  ","version":"Next","tagName":"h2"},{"title":"Basic URDF Structure​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#basic-urdf-structure","content":" &lt;?xml version=&quot;1.0&quot;?&gt; &lt;robot name=&quot;my_humanoid_robot&quot;&gt; &lt;!-- Links define rigid bodies --&gt; &lt;link name=&quot;base_link&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;box size=&quot;0.5 0.5 0.2&quot;/&gt; &lt;/geometry&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;box size=&quot;0.5 0.5 0.2&quot;/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;1.0&quot;/&gt; &lt;inertia ixx=&quot;0.1&quot; ixy=&quot;0&quot; ixz=&quot;0&quot; iyy=&quot;0.1&quot; iyz=&quot;0&quot; izz=&quot;0.1&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;!-- Joints connect links --&gt; &lt;joint name=&quot;base_to_head&quot; type=&quot;fixed&quot;&gt; &lt;parent link=&quot;base_link&quot;/&gt; &lt;child link=&quot;head&quot;/&gt; &lt;origin xyz=&quot;0 0 0.7&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;head&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;sphere radius=&quot;0.1&quot;/&gt; &lt;/geometry&gt; &lt;/visual&gt; &lt;/link&gt; &lt;/robot&gt;   ","version":"Next","tagName":"h3"},{"title":"Anthropomorphic Design Principles​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#anthropomorphic-design-principles","content":" Humanoid robots must address the unique challenges of human-like form:  ","version":"Next","tagName":"h2"},{"title":"Bipedal Locomotion Requirements​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#bipedal-locomotion-requirements","content":" Center of Mass: Position for stable walkingFoot Design: Ground contact for balanceLeg Structure: Joints for walking, running, climbing stairs  ","version":"Next","tagName":"h3"},{"title":"Dexterous Manipulation Requirements​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#dexterous-manipulation-requirements","content":" Hand Design: Multiple degrees of freedom for graspingArm Structure: Reach and dexterity for human tasksWrist Configuration: Orientation for tool use  ","version":"Next","tagName":"h3"},{"title":"Human-Centered Environment Adaptation​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#human-centered-environment-adaptation","content":" Height and Reach: Compatible with human furniture and toolsShoulder Configuration: Human-like range of motionHip Design: Bipedal stability with human-like movement  ","version":"Next","tagName":"h3"},{"title":"Advanced URDF Concepts​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#advanced-urdf-concepts","content":" ","version":"Next","tagName":"h2"},{"title":"Joint Types for Humanoid Robots​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#joint-types-for-humanoid-robots","content":" &lt;!-- Revolute joint (rotational with limits) --&gt; &lt;joint name=&quot;hip_pitch&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;torso&quot;/&gt; &lt;child link=&quot;thigh&quot;/&gt; &lt;origin xyz=&quot;0 0 -0.1&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;1 0 0&quot;/&gt; &lt;limit lower=&quot;-1.57&quot; upper=&quot;1.57&quot; effort=&quot;100&quot; velocity=&quot;3.0&quot;/&gt; &lt;dynamics damping=&quot;1.0&quot; friction=&quot;0.1&quot;/&gt; &lt;/joint&gt; &lt;!-- Continuous joint (unlimited rotation) --&gt; &lt;joint name=&quot;head_pan&quot; type=&quot;continuous&quot;&gt; &lt;parent link=&quot;neck&quot;/&gt; &lt;child link=&quot;head&quot;/&gt; &lt;origin xyz=&quot;0 0 0.05&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;0 0 1&quot;/&gt; &lt;dynamics damping=&quot;0.5&quot;/&gt; &lt;/joint&gt; &lt;!-- Fixed joint (no movement) --&gt; &lt;joint name=&quot;sensor_mount&quot; type=&quot;fixed&quot;&gt; &lt;parent link=&quot;head&quot;/&gt; &lt;child link=&quot;camera_link&quot;/&gt; &lt;origin xyz=&quot;0.05 0 0&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/joint&gt;   ","version":"Next","tagName":"h3"},{"title":"Inertial Properties​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#inertial-properties","content":" Proper inertial properties are crucial for realistic simulation:  &lt;inertial&gt; &lt;!-- Mass in kilograms --&gt; &lt;mass value=&quot;2.5&quot;/&gt; &lt;!-- Inertia tensor - important for dynamic simulation --&gt; &lt;inertia ixx=&quot;0.01&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.01&quot; iyz=&quot;0.0&quot; izz=&quot;0.01&quot;/&gt; &lt;/inertial&gt;   ","version":"Next","tagName":"h3"},{"title":"Visual and Collision Properties​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#visual-and-collision-properties","content":" &lt;visual&gt; &lt;!-- Visual appearance for RViz and simulation --&gt; &lt;origin xyz=&quot;0 0 0&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;geometry&gt; &lt;mesh filename=&quot;package://my_robot_description/meshes/upper_arm.dae&quot;/&gt; &lt;/geometry&gt; &lt;material name=&quot;gray&quot;&gt; &lt;color rgba=&quot;0.5 0.5 0.5 1.0&quot;/&gt; &lt;/material&gt; &lt;/visual&gt; &lt;collision&gt; &lt;!-- Collision geometry for physics simulation --&gt; &lt;!-- Often simplified compared to visual geometry for performance --&gt; &lt;origin xyz=&quot;0 0 0&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.3&quot; radius=&quot;0.05&quot;/&gt; &lt;/geometry&gt; &lt;/collision&gt;   ","version":"Next","tagName":"h3"},{"title":"XACRO: XML Macros for Robot Description​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#xacro-xml-macros-for-robot-description","content":" XACRO extends URDF with macros, properties, and mathematical expressions, making complex humanoid models more manageable:  ","version":"Next","tagName":"h2"},{"title":"Basic XACRO Structure​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#basic-xacro-structure","content":" &lt;?xml version=&quot;1.0&quot;?&gt; &lt;robot xmlns:xacro=&quot;http://www.ros.org/wiki/xacro&quot; name=&quot;humanoid_robot&quot;&gt; &lt;!-- Properties for easy parameterization --&gt; &lt;xacro:property name=&quot;M_PI&quot; value=&quot;3.1415926535897931&quot; /&gt; &lt;xacro:property name=&quot;robot_height&quot; value=&quot;1.6&quot; /&gt; &lt;xacro:property name=&quot;link_radius&quot; value=&quot;0.05&quot; /&gt; &lt;xacro:property name=&quot;link_length&quot; value=&quot;0.3&quot; /&gt; &lt;!-- Macros for repeated structures --&gt; &lt;xacro:macro name=&quot;simple_link&quot; params=&quot;name xyz_length&quot;&gt; &lt;link name=&quot;${name}&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;cylinder length=&quot;${xyz_length}&quot; radius=&quot;${link_radius}&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 ${xyz_length/2}&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;cylinder length=&quot;${xyz_length}&quot; radius=&quot;${link_radius}&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 ${xyz_length/2}&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;1.0&quot;/&gt; &lt;inertia ixx=&quot;0.01&quot; ixy=&quot;0&quot; ixz=&quot;0&quot; iyy=&quot;0.01&quot; iyz=&quot;0&quot; izz=&quot;0.01&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;/xacro:macro&gt; &lt;/robot&gt;   ","version":"Next","tagName":"h3"},{"title":"Complete Humanoid Robot URDF Example​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#complete-humanoid-robot-urdf-example","content":" Here's a more complete example of a humanoid robot using XACRO:  &lt;?xml version=&quot;1.0&quot;?&gt; &lt;robot xmlns:xacro=&quot;http://www.ros.org/wiki/xacro&quot; name=&quot;simple_humanoid&quot;&gt; &lt;!-- Properties --&gt; &lt;xacro:property name=&quot;M_PI&quot; value=&quot;3.1415926535897931&quot; /&gt; &lt;xacro:property name=&quot;robot_height&quot; value=&quot;1.6&quot; /&gt; &lt;xacro:property name=&quot;torso_height&quot; value=&quot;0.6&quot; /&gt; &lt;xacro:property name=&quot;upper_leg_length&quot; value=&quot;0.4&quot; /&gt; &lt;xacro:property name=&quot;lower_leg_length&quot; value=&quot;0.4&quot; /&gt; &lt;xacro:property name=&quot;upper_arm_length&quot; value=&quot;0.3&quot; /&gt; &lt;xacro:property name=&quot;lower_arm_length&quot; value=&quot;0.3&quot; /&gt; &lt;!-- Materials --&gt; &lt;material name=&quot;black&quot;&gt; &lt;color rgba=&quot;0.0 0.0 0.0 1.0&quot;/&gt; &lt;/material&gt; &lt;material name=&quot;blue&quot;&gt; &lt;color rgba=&quot;0.0 0.0 0.8 1.0&quot;/&gt; &lt;/material&gt; &lt;material name=&quot;green&quot;&gt; &lt;color rgba=&quot;0.0 0.8 0.0 1.0&quot;/&gt; &lt;/material&gt; &lt;material name=&quot;grey&quot;&gt; &lt;color rgba=&quot;0.5 0.5 0.5 1.0&quot;/&gt; &lt;/material&gt; &lt;material name=&quot;orange&quot;&gt; &lt;color rgba=&quot;1.0 0.423529411765 0.0392156862745 1.0&quot;/&gt; &lt;/material&gt; &lt;material name=&quot;brown&quot;&gt; &lt;color rgba=&quot;0.870588235294 0.811764705882 0.764705882353 1.0&quot;/&gt; &lt;/material&gt; &lt;material name=&quot;red&quot;&gt; &lt;color rgba=&quot;0.8 0.0 0.0 1.0&quot;/&gt; &lt;/material&gt; &lt;material name=&quot;white&quot;&gt; &lt;color rgba=&quot;1.0 1.0 1.0 1.0&quot;/&gt; &lt;/material&gt; &lt;!-- Base link --&gt; &lt;link name=&quot;base_link&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;box size=&quot;0.1 0.1 0.1&quot;/&gt; &lt;/geometry&gt; &lt;material name=&quot;white&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;box size=&quot;0.1 0.1 0.1&quot;/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;1.0&quot;/&gt; &lt;inertia ixx=&quot;0.01&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.01&quot; iyz=&quot;0.0&quot; izz=&quot;0.01&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;!-- Torso --&gt; &lt;joint name=&quot;torso_joint&quot; type=&quot;fixed&quot;&gt; &lt;parent link=&quot;base_link&quot;/&gt; &lt;child link=&quot;torso&quot;/&gt; &lt;origin xyz=&quot;0 0 0.05&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;torso&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;box size=&quot;0.3 0.2 0.5&quot;/&gt; &lt;/geometry&gt; &lt;material name=&quot;orange&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;box size=&quot;0.3 0.2 0.5&quot;/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;10.0&quot;/&gt; &lt;inertia ixx=&quot;0.2&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.3&quot; iyz=&quot;0.0&quot; izz=&quot;0.2&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;!-- Head --&gt; &lt;joint name=&quot;neck_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;torso&quot;/&gt; &lt;child link=&quot;head&quot;/&gt; &lt;origin xyz=&quot;0 0 0.4&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;0 1 0&quot;/&gt; &lt;limit lower=&quot;${-M_PI/4}&quot; upper=&quot;${M_PI/4}&quot; effort=&quot;10.0&quot; velocity=&quot;3.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;head&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;sphere radius=&quot;0.1&quot;/&gt; &lt;/geometry&gt; &lt;material name=&quot;white&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;sphere radius=&quot;0.1&quot;/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;2.0&quot;/&gt; &lt;inertia ixx=&quot;0.01&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.01&quot; iyz=&quot;0.0&quot; izz=&quot;0.01&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;!-- Left Arm --&gt; &lt;joint name=&quot;left_shoulder_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;torso&quot;/&gt; &lt;child link=&quot;left_upper_arm&quot;/&gt; &lt;origin xyz=&quot;0.2 0.1 0.2&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;0 1 0&quot;/&gt; &lt;limit lower=&quot;${-M_PI/2}&quot; upper=&quot;${M_PI/2}&quot; effort=&quot;20.0&quot; velocity=&quot;3.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;left_upper_arm&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.3&quot; radius=&quot;0.05&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 0.15&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;material name=&quot;blue&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.3&quot; radius=&quot;0.05&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 0.15&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;2.0&quot;/&gt; &lt;inertia ixx=&quot;0.01&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.01&quot; iyz=&quot;0.0&quot; izz=&quot;0.01&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;joint name=&quot;left_elbow_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;left_upper_arm&quot;/&gt; &lt;child link=&quot;left_lower_arm&quot;/&gt; &lt;origin xyz=&quot;0 0 0.3&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;0 1 0&quot;/&gt; &lt;limit lower=&quot;${-M_PI/2}&quot; upper=&quot;${M_PI/2}&quot; effort=&quot;15.0&quot; velocity=&quot;3.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;left_lower_arm&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.3&quot; radius=&quot;0.04&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 0.15&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;material name=&quot;blue&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.3&quot; radius=&quot;0.04&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 0.15&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;1.5&quot;/&gt; &lt;inertia ixx=&quot;0.01&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.01&quot; iyz=&quot;0.0&quot; izz=&quot;0.01&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;!-- Left Leg --&gt; &lt;joint name=&quot;left_hip_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;torso&quot;/&gt; &lt;child link=&quot;left_upper_leg&quot;/&gt; &lt;origin xyz=&quot;0.05 -0.08 -0.25&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;1 0 0&quot;/&gt; &lt;limit lower=&quot;${-M_PI/2}&quot; upper=&quot;${M_PI/2}&quot; effort=&quot;50.0&quot; velocity=&quot;3.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;left_upper_leg&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.4&quot; radius=&quot;0.06&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 -0.2&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;material name=&quot;green&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.4&quot; radius=&quot;0.06&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 -0.2&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;5.0&quot;/&gt; &lt;inertia ixx=&quot;0.1&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.1&quot; iyz=&quot;0.0&quot; izz=&quot;0.05&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;joint name=&quot;left_knee_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;left_upper_leg&quot;/&gt; &lt;child link=&quot;left_lower_leg&quot;/&gt; &lt;origin xyz=&quot;0 0 -0.4&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;1 0 0&quot;/&gt; &lt;limit lower=&quot;0&quot; upper=&quot;${M_PI/2}&quot; effort=&quot;40.0&quot; velocity=&quot;3.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;left_lower_leg&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.4&quot; radius=&quot;0.05&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 -0.2&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;material name=&quot;green&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.4&quot; radius=&quot;0.05&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 -0.2&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;4.0&quot;/&gt; &lt;inertia ixx=&quot;0.08&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.08&quot; iyz=&quot;0.0&quot; izz=&quot;0.04&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;joint name=&quot;left_ankle_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;left_lower_leg&quot;/&gt; &lt;child link=&quot;left_foot&quot;/&gt; &lt;origin xyz=&quot;0 0 -0.4&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;1 0 0&quot;/&gt; &lt;limit lower=&quot;${-M_PI/6}&quot; upper=&quot;${M_PI/6}&quot; effort=&quot;20.0&quot; velocity=&quot;3.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;left_foot&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;box size=&quot;0.2 0.1 0.05&quot;/&gt; &lt;/geometry&gt; &lt;material name=&quot;black&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;box size=&quot;0.2 0.1 0.05&quot;/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;1.0&quot;/&gt; &lt;inertia ixx=&quot;0.001&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.001&quot; iyz=&quot;0.0&quot; izz=&quot;0.001&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;!-- Right side (similar to left, mirrored) --&gt; &lt;joint name=&quot;right_hip_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;torso&quot;/&gt; &lt;child link=&quot;right_upper_leg&quot;/&gt; &lt;origin xyz=&quot;0.05 0.08 -0.25&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;1 0 0&quot;/&gt; &lt;limit lower=&quot;${-M_PI/2}&quot; upper=&quot;${M_PI/2}&quot; effort=&quot;50.0&quot; velocity=&quot;3.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;right_upper_leg&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.4&quot; radius=&quot;0.06&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 -0.2&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;material name=&quot;green&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.4&quot; radius=&quot;0.06&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 -0.2&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;5.0&quot;/&gt; &lt;inertia ixx=&quot;0.1&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.1&quot; iyz=&quot;0.0&quot; izz=&quot;0.05&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;joint name=&quot;right_knee_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;right_upper_leg&quot;/&gt; &lt;child link=&quot;right_lower_leg&quot;/&gt; &lt;origin xyz=&quot;0 0 -0.4&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;1 0 0&quot;/&gt; &lt;limit lower=&quot;0&quot; upper=&quot;${M_PI/2}&quot; effort=&quot;40.0&quot; velocity=&quot;3.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;right_lower_leg&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.4&quot; radius=&quot;0.05&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 -0.2&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;material name=&quot;green&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.4&quot; radius=&quot;0.05&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 -0.2&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;4.0&quot;/&gt; &lt;inertia ixx=&quot;0.08&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.08&quot; iyz=&quot;0.0&quot; izz=&quot;0.04&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;joint name=&quot;right_ankle_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;right_lower_leg&quot;/&gt; &lt;child link=&quot;right_foot&quot;/&gt; &lt;origin xyz=&quot;0 0 -0.4&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;1 0 0&quot;/&gt; &lt;limit lower=&quot;${-M_PI/6}&quot; upper=&quot;${M_PI/6}&quot; effort=&quot;20.0&quot; velocity=&quot;3.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;right_foot&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;box size=&quot;0.2 0.1 0.05&quot;/&gt; &lt;/geometry&gt; &lt;material name=&quot;black&quot;/&gt; &lt;/visual&gt; &lt;collision&gt; &lt;geometry&gt; &lt;box size=&quot;0.2 0.1 0.05&quot;/&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;inertial&gt; &lt;mass value=&quot;1.0&quot;/&gt; &lt;inertia ixx=&quot;0.001&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.001&quot; iyz=&quot;0.0&quot; izz=&quot;0.001&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;!-- Transmission for ROS Control --&gt; &lt;transmission name=&quot;left_hip_trans&quot;&gt; &lt;type&gt;transmission_interface/SimpleTransmission&lt;/type&gt; &lt;joint name=&quot;left_hip_joint&quot;&gt; &lt;hardwareInterface&gt;hardware_interface/EffortJointInterface&lt;/hardwareInterface&gt; &lt;/joint&gt; &lt;actuator name=&quot;left_hip_motor&quot;&gt; &lt;hardwareInterface&gt;hardware_interface/EffortJointInterface&lt;/hardwareInterface&gt; &lt;mechanicalReduction&gt;1&lt;/mechanicalReduction&gt; &lt;/actuator&gt; &lt;/transmission&gt; &lt;!-- Gazebo-specific elements --&gt; &lt;gazebo reference=&quot;base_link&quot;&gt; &lt;material&gt;Gazebo/Orange&lt;/material&gt; &lt;/gazebo&gt; &lt;/robot&gt;   ","version":"Next","tagName":"h2"},{"title":"Advanced XACRO Features for Humanoid Robots​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#advanced-xacro-features-for-humanoid-robots","content":" ","version":"Next","tagName":"h2"},{"title":"Mathematical Expressions​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#mathematical-expressions","content":" XACRO supports mathematical expressions for parameter calculations:  &lt;xacro:property name=&quot;hip_offset_x&quot; value=&quot;0.05&quot; /&gt; &lt;xacro:property name=&quot;hip_offset_y&quot; value=&quot;0.08&quot; /&gt; &lt;xacro:property name=&quot;leg_length&quot; value=&quot;0.8&quot; /&gt; &lt;!-- Calculate derived values --&gt; &lt;xacro:property name=&quot;foot_z&quot; value=&quot;${-leg_length/2}&quot; /&gt;   ","version":"Next","tagName":"h3"},{"title":"Conditional Statements​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#conditional-statements","content":" &lt;xacro:macro name=&quot;optional_sensor&quot; params=&quot;install_sensor:=false&quot;&gt; &lt;xacro:if value=&quot;${install_sensor}&quot;&gt; &lt;joint name=&quot;sensor_joint&quot; type=&quot;fixed&quot;&gt; &lt;parent link=&quot;head&quot;/&gt; &lt;child link=&quot;camera_link&quot;/&gt; &lt;origin xyz=&quot;0.05 0 0&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;camera_link&quot;/&gt; &lt;/xacro:if&gt; &lt;/xacro:macro&gt;   ","version":"Next","tagName":"h3"},{"title":"Include Other Files​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#include-other-files","content":" &lt;!-- Include common definitions --&gt; &lt;xacro:include filename=&quot;$(find my_robot_description)/urdf/materials.xacro&quot; /&gt; &lt;xacro:include filename=&quot;$(find my_robot_description)/urdf/common_properties.xacro&quot; /&gt; &lt;xacro:include filename=&quot;$(find my_robot_description)/urdf/hand.xacro&quot; /&gt;   ","version":"Next","tagName":"h3"},{"title":"Vision-Language-Action Integration​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#vision-language-action-integration","content":" URDF models support the VLA pipeline by providing:  ","version":"Next","tagName":"h2"},{"title":"Kinematic Information​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#kinematic-information","content":" Forward Kinematics: Calculate end-effector positions from joint anglesInverse Kinematics: Calculate joint angles for desired end-effector positionsCollision Checking: Prevent self-collisions during manipulation  ","version":"Next","tagName":"h3"},{"title":"Simulation Integration​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#simulation-integration","content":" Physics Properties: Mass, inertia for realistic simulationSensor Mounting: Proper placement of cameras, IMUs, and other sensorsActuator Models: Realistic joint dynamics for control development  ","version":"Next","tagName":"h3"},{"title":"Testing and Validation​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#testing-and-validation","content":" ","version":"Next","tagName":"h2"},{"title":"URDF Validation​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#urdf-validation","content":" # Check URDF syntax check_urdf /path/to/robot.urdf # Parse XACRO to URDF xacro input.xacro &gt; output.urdf   ","version":"Next","tagName":"h3"},{"title":"Visualization in RViz​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#visualization-in-rviz","content":" &lt;!-- Add TF publishing for visualization --&gt; &lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;robot_state_publisher&quot; /&gt;   ","version":"Next","tagName":"h3"},{"title":"Simulation in Gazebo​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#simulation-in-gazebo","content":" &lt;!-- Gazebo-specific elements --&gt; &lt;gazebo&gt; &lt;plugin name=&quot;gazebo_ros_control&quot; filename=&quot;libgazebo_ros_control.so&quot;&gt; &lt;robotNamespace&gt;/my_robot&lt;/robotNamespace&gt; &lt;/plugin&gt; &lt;/gazebo&gt;   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Anthropomorphic Focus (Principle II)​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#anthropomorphic-focus-principle-ii","content":" Emphasis on bipedal locomotion kinematicsDexterous manipulation structuresHuman-centered environment compatibility  ","version":"Next","tagName":"h3"},{"title":"Sim-to-Real Rigor (Principle III)​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#sim-to-real-rigor-principle-iii","content":" Accurate physical properties for simulationProper sensor integration for perceptionRealistic dynamics for control development  ","version":"Next","tagName":"h3"},{"title":"Visualization Requirements (Key Standard II)​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#visualization-requirements-key-standard-ii","content":" Use of proper geometric representationsAppropriate materials and colorsClear kinematic structure visualization  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Hand Model with XACRO​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#example-1-hand-model-with-xacro","content":" &lt;xacro:macro name=&quot;robotic_hand&quot; params=&quot;side parent_link position&quot;&gt; &lt;!-- Thumb --&gt; &lt;joint name=&quot;${side}_thumb_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;${parent_link}&quot;/&gt; &lt;child link=&quot;${side}_thumb_knuckle&quot;/&gt; &lt;origin xyz=&quot;${position}&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;0 1 0&quot;/&gt; &lt;limit lower=&quot;0&quot; upper=&quot;1.57&quot; effort=&quot;5.0&quot; velocity=&quot;2.0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;${side}_thumb_knuckle&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;cylinder length=&quot;0.04&quot; radius=&quot;0.015&quot;/&gt; &lt;/geometry&gt; &lt;origin xyz=&quot;0 0 0.02&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/visual&gt; &lt;inertial&gt; &lt;mass value=&quot;0.05&quot;/&gt; &lt;inertia ixx=&quot;0.0001&quot; ixy=&quot;0&quot; ixz=&quot;0&quot; iyy=&quot;0.0001&quot; iyz=&quot;0&quot; izz=&quot;0.0001&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;!-- Additional finger joints would continue similarly --&gt; &lt;/xacro:macro&gt;   ","version":"Next","tagName":"h3"},{"title":"Example 2: Complete Robot with Sensor Integration​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#example-2-complete-robot-with-sensor-integration","content":" &lt;?xml version=&quot;1.0&quot;?&gt; &lt;robot xmlns:xacro=&quot;http://www.ros.org/wiki/xacro&quot; name=&quot;humanoid_with_sensors&quot;&gt; &lt;!-- Include basic structure --&gt; &lt;xacro:include filename=&quot;humanoid_base.xacro&quot;/&gt; &lt;!-- RGB-D Camera --&gt; &lt;joint name=&quot;camera_joint&quot; type=&quot;fixed&quot;&gt; &lt;parent link=&quot;head&quot;/&gt; &lt;child link=&quot;camera_link&quot;/&gt; &lt;origin xyz=&quot;0.05 0 0.05&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;camera_link&quot;&gt; &lt;visual&gt; &lt;geometry&gt; &lt;box size=&quot;0.05 0.05 0.03&quot;/&gt; &lt;/geometry&gt; &lt;/visual&gt; &lt;/link&gt; &lt;!-- IMU for balance feedback --&gt; &lt;joint name=&quot;imu_joint&quot; type=&quot;fixed&quot;&gt; &lt;parent link=&quot;torso&quot;/&gt; &lt;child link=&quot;imu_link&quot;/&gt; &lt;origin xyz=&quot;0 0 0.1&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;/joint&gt; &lt;link name=&quot;imu_link&quot;/&gt; &lt;!-- Gazebo plugins for sensors --&gt; &lt;gazebo reference=&quot;camera_link&quot;&gt; &lt;sensor type=&quot;depth&quot; name=&quot;camera_sensor&quot;&gt; &lt;always_on&gt;true&lt;/always_on&gt; &lt;update_rate&gt;30&lt;/update_rate&gt; &lt;camera name=&quot;head_camera&quot;&gt; &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt; &lt;image&gt; &lt;format&gt;R8G8B8&lt;/format&gt; &lt;width&gt;640&lt;/width&gt; &lt;height&gt;480&lt;/height&gt; &lt;/image&gt; &lt;clip&gt; &lt;near&gt;0.1&lt;/near&gt; &lt;far&gt;10&lt;/far&gt; &lt;/clip&gt; &lt;/camera&gt; &lt;plugin name=&quot;camera_controller&quot; filename=&quot;libgazebo_ros_camera.so&quot;&gt; &lt;frame_name&gt;camera_link&lt;/frame_name&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt; &lt;/robot&gt;   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Custom Humanoid Model​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#exercise-1-custom-humanoid-model","content":" Create a URDF/XACRO model for a humanoid robot that includes:  Complete kinematic chain for bipedal locomotionAnthropomorphic arms with 7 DOF eachProper inertial properties for dynamic simulationMounting points for RGB-D camera and IMU  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Modular Design​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#exercise-2-modular-design","content":" Design a modular URDF system using XACRO includes for:  Reusable leg macroReusable arm macroReusable hand/foot macroMain robot assembly that combines these modules  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Sensor Integration​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#exercise-3-sensor-integration","content":" Extend your robot model to include proper sensor integration for the VLA pipeline:  Vision sensors (cameras) for perceptionIMU for balance feedbackForce/torque sensors in jointsProper Gazebo plugins for simulation  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#summary","content":" URDF and XACRO are essential tools for modeling humanoid robots, enabling the representation of complex kinematic structures necessary for bipedal locomotion and dexterous manipulation. The anthropomorphic focus of our approach is supported through proper modeling of human-like kinematics and dynamics. Understanding these tools is crucial for developing effective Physical AI systems that can operate in human-centered environments, with accurate models for both simulation and real-world deployment.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 7 - URDF & XACRO for Humanoid Robots","url":"/docs/chapters/module-2-ros/chapter-7-urdf-xacro#further-reading","content":" &quot;Programming Robots with ROS&quot; by Quigley et al. (URDF chapter)&quot;Mastering ROS for Robotics Programming&quot; by Jayanam&quot;Robotics, Vision and Control&quot; by Peter Corke&quot;URDF for Dummies&quot; - ROS Wiki tutorial&quot;XACRO tutorial&quot; - ROS Wiki ","version":"Next","tagName":"h2"},{"title":"Chapter 10 - Physics & Sensor Simulation in Gazebo","type":0,"sectionRef":"#","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#learning-objectives","content":" Understand physics simulation parameters for humanoid robotsConfigure realistic sensor simulation for humanoid applicationsOptimize simulation for Vision-Language-Action pipeline  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#introduction","content":" Physics and sensor simulation in Gazebo are critical for creating realistic digital twins of humanoid robots that can be used for training and validation before deployment to physical hardware. The accuracy of physics simulation directly impacts the sim-to-real transfer capability of humanoid robots, which is essential for the Vision-Language-Action pipeline. This chapter delves into the advanced configuration of physics engines and sensor simulation parameters, with special attention to the requirements for bipedal locomotion, dexterous manipulation, and real-time perception that characterize humanoid robotics systems.  ","version":"Next","tagName":"h2"},{"title":"Physics Simulation Fundamentals​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#physics-simulation-fundamentals","content":" ","version":"Next","tagName":"h2"},{"title":"Physics Engine Selection​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#physics-engine-selection","content":" Gazebo supports multiple physics engines, each with different characteristics:  Open Dynamics Engine (ODE)​  Pros: Fast, stable, well-testedCons: Less accurate for complex contactsBest for: General-purpose simulation, real-time applications  Bullet Physics​  Pros: Better contact handling, more accurateCons: Slower than ODEBest for: Complex contact scenarios, high-accuracy requirements  DART (Dynamic Animation and Robotics Toolkit)​  Pros: Advanced contact handling, stableCons: More complex setupBest for: Complex multi-body systems  ","version":"Next","tagName":"h3"},{"title":"Physics Configuration Parameters​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#physics-configuration-parameters","content":" Time Step Configuration​  The physics time step is crucial for humanoid stability:  &lt;physics type=&quot;ode&quot;&gt; &lt;!-- Critical for humanoid balance - smaller steps for stability --&gt; &lt;max_step_size&gt;0.001&lt;/max_step_size&gt; &lt;!-- 1ms steps --&gt; &lt;real_time_factor&gt;1.0&lt;/real_time_factor&gt; &lt;real_time_update_rate&gt;1000&lt;/real_time_update_rate&gt; &lt;gravity&gt;0 0 -9.8&lt;/gravity&gt; &lt;ode&gt; &lt;solver&gt; &lt;type&gt;quick&lt;/type&gt; &lt;iters&gt;100&lt;/iters&gt; &lt;!-- More iterations for stability --&gt; &lt;sor&gt;1.3&lt;/sor&gt; &lt;/solver&gt; &lt;constraints&gt; &lt;cfm&gt;0.000001&lt;/cfm&gt; &lt;!-- Constraint Force Mixing --&gt; &lt;erp&gt;0.2&lt;/erp&gt; &lt;!-- Error Reduction Parameter --&gt; &lt;contact_max_correcting_vel&gt;100&lt;/contact_max_correcting_vel&gt; &lt;contact_surface_layer&gt;0.001&lt;/contact_surface_layer&gt; &lt;/constraints&gt; &lt;/ode&gt; &lt;/physics&gt;   Solver Configuration​  For humanoid robots requiring high stability:  Iterations: Higher values (50-200) for more stable contact resolutionSOR (Successive Over-Relaxation): Values around 1.2-1.3 for stabilityCFM (Constraint Force Mixing): Low values (1e-6) for stiff constraintsERP (Error Reduction Parameter): 0.1-0.8 for error correction  ","version":"Next","tagName":"h3"},{"title":"Material Properties and Friction​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#material-properties-and-friction","content":" Realistic material properties are essential for humanoid locomotion:  &lt;!-- Ground surface properties --&gt; &lt;model name=&quot;ground_plane&quot;&gt; &lt;static&gt;true&lt;/static&gt; &lt;link name=&quot;link&quot;&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;surface&gt; &lt;friction&gt; &lt;ode&gt; &lt;mu&gt;1.0&lt;/mu&gt; &lt;!-- Static friction coefficient --&gt; &lt;mu2&gt;1.0&lt;/mu2&gt; &lt;!-- Dynamic friction coefficient --&gt; &lt;slip1&gt;0.0&lt;/slip1&gt; &lt;!-- Primary slip coefficient --&gt; &lt;slip2&gt;0.0&lt;/slip2&gt; &lt;!-- Secondary slip coefficient --&gt; &lt;/ode&gt; &lt;torsional&gt; &lt;coefficient&gt;1.0&lt;/coefficient&gt; &lt;use_patch_radius&gt;false&lt;/use_patch_radius&gt; &lt;surface_radius&gt;0.01&lt;/surface_radius&gt; &lt;/torsional&gt; &lt;/friction&gt; &lt;contact&gt; &lt;ode&gt; &lt;soft_cfm&gt;0&lt;/soft_cfm&gt; &lt;soft_erp&gt;0.2&lt;/soft_erp&gt; &lt;kp&gt;1e5&lt;/kp&gt; &lt;!-- Contact stiffness --&gt; &lt;kd&gt;1e3&lt;/kd&gt; &lt;!-- Contact damping --&gt; &lt;max_vel&gt;100&lt;/max_vel&gt; &lt;min_depth&gt;0.001&lt;/min_depth&gt; &lt;/ode&gt; &lt;/contact&gt; &lt;/surface&gt; &lt;/collision&gt; &lt;/link&gt; &lt;/model&gt;   ","version":"Next","tagName":"h3"},{"title":"Humanoid-Specific Physics Configuration​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#humanoid-specific-physics-configuration","content":" ","version":"Next","tagName":"h2"},{"title":"Bipedal Locomotion Physics​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#bipedal-locomotion-physics","content":" For stable bipedal walking, special attention must be paid to:  Center of Mass Configuration​  &lt;!-- Example torso link with proper inertial properties --&gt; &lt;link name=&quot;torso&quot;&gt; &lt;inertial&gt; &lt;mass value=&quot;10.0&quot;/&gt; &lt;!-- Proper inertial tensor for human-like torso --&gt; &lt;inertia ixx=&quot;0.2&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.3&quot; iyz=&quot;0.0&quot; izz=&quot;0.2&quot;/&gt; &lt;/inertial&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;geometry&gt; &lt;box size=&quot;0.3 0.2 0.5&quot;/&gt; &lt;/geometry&gt; &lt;surface&gt; &lt;contact&gt; &lt;ode&gt; &lt;max_vel&gt;100&lt;/max_vel&gt; &lt;min_depth&gt;0.001&lt;/min_depth&gt; &lt;/ode&gt; &lt;/contact&gt; &lt;friction&gt; &lt;ode&gt; &lt;mu&gt;0.5&lt;/mu&gt; &lt;mu2&gt;0.5&lt;/mu2&gt; &lt;/ode&gt; &lt;/friction&gt; &lt;/surface&gt; &lt;/collision&gt; &lt;/link&gt;   Foot Contact Configuration​  Critical for humanoid balance:  &lt;link name=&quot;left_foot&quot;&gt; &lt;inertial&gt; &lt;mass value=&quot;1.0&quot;/&gt; &lt;inertia ixx=&quot;0.001&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.001&quot; iyz=&quot;0.0&quot; izz=&quot;0.001&quot;/&gt; &lt;/inertial&gt; &lt;collision name=&quot;sole_collision&quot;&gt; &lt;geometry&gt; &lt;box size=&quot;0.2 0.1 0.02&quot;/&gt; &lt;!-- Flat sole for good contact --&gt; &lt;/geometry&gt; &lt;surface&gt; &lt;contact&gt; &lt;ode&gt; &lt;max_vel&gt;100&lt;/max_vel&gt; &lt;min_depth&gt;0.002&lt;/min_depth&gt; &lt;!-- Slightly deeper for stability --&gt; &lt;kp&gt;1e6&lt;/kp&gt; &lt;!-- High stiffness for foot contact --&gt; &lt;kd&gt;1e4&lt;/kd&gt; &lt;!-- Appropriate damping --&gt; &lt;/ode&gt; &lt;/contact&gt; &lt;friction&gt; &lt;ode&gt; &lt;mu&gt;1.0&lt;/mu&gt; &lt;!-- High friction for walking --&gt; &lt;mu2&gt;1.0&lt;/mu2&gt; &lt;/ode&gt; &lt;/friction&gt; &lt;/surface&gt; &lt;/collision&gt; &lt;!-- Slightly raised heel and toe for natural walking --&gt; &lt;collision name=&quot;heel_collision&quot;&gt; &lt;pose&gt;0.08 0 -0.01 0 0 0&lt;/pose&gt; &lt;geometry&gt; &lt;box size=&quot;0.04 0.08 0.02&quot;/&gt; &lt;/geometry&gt; &lt;surface&gt; &lt;contact&gt; &lt;ode&gt; &lt;min_depth&gt;0.001&lt;/min_depth&gt; &lt;kp&gt;1e5&lt;/kp&gt; &lt;/ode&gt; &lt;/contact&gt; &lt;/surface&gt; &lt;/collision&gt; &lt;/link&gt;   ","version":"Next","tagName":"h3"},{"title":"Joint Configuration for Humanoid Dynamics​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#joint-configuration-for-humanoid-dynamics","content":" &lt;!-- Hip joint with appropriate dynamics for walking --&gt; &lt;joint name=&quot;left_hip_joint&quot; type=&quot;revolute&quot;&gt; &lt;parent link=&quot;torso&quot;/&gt; &lt;child link=&quot;left_thigh&quot;/&gt; &lt;origin xyz=&quot;0.05 -0.08 -0.25&quot; rpy=&quot;0 0 0&quot;/&gt; &lt;axis xyz=&quot;1 0 0&quot;/&gt; &lt;limit lower=&quot;-1.57&quot; upper=&quot;1.57&quot; effort=&quot;100&quot; velocity=&quot;5.0&quot;/&gt; &lt;dynamics damping=&quot;5.0&quot; friction=&quot;1.0&quot;/&gt; &lt;!-- Appropriate damping for walking --&gt; &lt;/joint&gt;   ","version":"Next","tagName":"h3"},{"title":"Advanced Sensor Simulation​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#advanced-sensor-simulation","content":" ","version":"Next","tagName":"h2"},{"title":"Camera Simulation for Vision Systems​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#camera-simulation-for-vision-systems","content":" For the Vision component of the VLA pipeline:  &lt;gazebo reference=&quot;camera_link&quot;&gt; &lt;sensor type=&quot;camera&quot; name=&quot;rgb_camera&quot;&gt; &lt;always_on&gt;true&lt;/always_on&gt; &lt;update_rate&gt;30&lt;/update_rate&gt; &lt;camera name=&quot;head_camera&quot;&gt; &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt; &lt;!-- 60 degrees --&gt; &lt;image&gt; &lt;format&gt;R8G8B8&lt;/format&gt; &lt;width&gt;640&lt;/width&gt; &lt;height&gt;480&lt;/height&gt; &lt;/image&gt; &lt;clip&gt; &lt;near&gt;0.1&lt;/near&gt; &lt;far&gt;10.0&lt;/far&gt; &lt;/clip&gt; &lt;noise&gt; &lt;type&gt;gaussian&lt;/type&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;0.007&lt;/stddev&gt; &lt;!-- Realistic noise for RGB camera --&gt; &lt;/noise&gt; &lt;/camera&gt; &lt;plugin name=&quot;camera_controller&quot; filename=&quot;libgazebo_ros_camera.so&quot;&gt; &lt;frame_name&gt;camera_link&lt;/frame_name&gt; &lt;min_depth&gt;0.1&lt;/min_depth&gt; &lt;max_depth&gt;10.0&lt;/max_depth&gt; &lt;update_rate&gt;30.0&lt;/update_rate&gt; &lt;!-- Camera calibration parameters --&gt; &lt;hack_baseline&gt;0.07&lt;/hack_baseline&gt; &lt;distortion_k1&gt;0.0&lt;/distortion_k1&gt; &lt;distortion_k2&gt;0.0&lt;/distortion_k2&gt; &lt;distortion_k3&gt;0.0&lt;/distortion_k3&gt; &lt;distortion_t1&gt;0.0&lt;/distortion_t1&gt; &lt;distortion_t2&gt;0.0&lt;/distortion_t2&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt;   ","version":"Next","tagName":"h3"},{"title":"Depth Camera and Point Cloud Simulation​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#depth-camera-and-point-cloud-simulation","content":" For 3D perception in the VLA pipeline:  &lt;gazebo reference=&quot;camera_link&quot;&gt; &lt;sensor type=&quot;depth&quot; name=&quot;depth_camera&quot;&gt; &lt;always_on&gt;true&lt;/always_on&gt; &lt;update_rate&gt;30&lt;/update_rate&gt; &lt;camera name=&quot;depth_camera&quot;&gt; &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt; &lt;image&gt; &lt;format&gt;R8G8B8&lt;/format&gt; &lt;width&gt;640&lt;/width&gt; &lt;height&gt;480&lt;/height&gt; &lt;/image&gt; &lt;clip&gt; &lt;near&gt;0.1&lt;/near&gt; &lt;far&gt;8.0&lt;/far&gt; &lt;/clip&gt; &lt;/camera&gt; &lt;plugin name=&quot;depth_camera_controller&quot; filename=&quot;libgazebo_ros_openni_kinect.so&quot;&gt; &lt;baseline&gt;0.2&lt;/baseline&gt; &lt;alwaysOn&gt;true&lt;/alwaysOn&gt; &lt;updateRate&gt;30.0&lt;/updateRate&gt; &lt;cameraName&gt;depth_camera&lt;/cameraName&gt; &lt;imageTopicName&gt;/camera/image_raw&lt;/imageTopicName&gt; &lt;depthImageTopicName&gt;/camera/depth/image_raw&lt;/depthImageTopicName&gt; &lt;pointCloudTopicName&gt;/camera/depth/points&lt;/pointCloudTopicName&gt; &lt;cameraInfoTopicName&gt;/camera/camera_info&lt;/cameraInfoTopicName&gt; &lt;frameName&gt;camera_link&lt;/frameName&gt; &lt;pointCloudCutoff&gt;0.1&lt;/pointCloudCutoff&gt; &lt;pointCloudCutoffMax&gt;8.0&lt;/pointCloudCutoffMax&gt; &lt;distortion_k1&gt;0.0&lt;/distortion_k1&gt; &lt;distortion_k2&gt;0.0&lt;/distortion_k2&gt; &lt;distortion_k3&gt;0.0&lt;/distortion_k3&gt; &lt;distortion_t1&gt;0.0&lt;/distortion_t1&gt; &lt;distortion_t2&gt;0.0&lt;/distortion_t2&gt; &lt;CxPrime&gt;0.0&lt;/CxPrime&gt; &lt;Cx&gt;320.5&lt;/Cx&gt; &lt;Cy&gt;240.5&lt;/Cy&gt; &lt;focalLength&gt;320.0&lt;/focalLength&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt;   ","version":"Next","tagName":"h3"},{"title":"IMU Simulation for Balance Feedback​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#imu-simulation-for-balance-feedback","content":" Critical for real-time validation (Principle IV):  &lt;gazebo reference=&quot;imu_link&quot;&gt; &lt;sensor name=&quot;imu_sensor&quot; type=&quot;imu&quot;&gt; &lt;always_on&gt;true&lt;/always_on&gt; &lt;update_rate&gt;1000&lt;/update_rate&gt; &lt;!-- High rate for balance control --&gt; &lt;imu&gt; &lt;angular_velocity&gt; &lt;x&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;2e-4&lt;/stddev&gt; &lt;!-- Low noise for accurate measurement --&gt; &lt;bias_mean&gt;0.002&lt;/bias_mean&gt; &lt;bias_stddev&gt;0.0003&lt;/bias_stddev&gt; &lt;/noise&gt; &lt;/x&gt; &lt;y&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;2e-4&lt;/stddev&gt; &lt;bias_mean&gt;0.002&lt;/bias_mean&gt; &lt;bias_stddev&gt;0.0003&lt;/bias_stddev&gt; &lt;/noise&gt; &lt;/y&gt; &lt;z&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;2e-4&lt;/stddev&gt; &lt;bias_mean&gt;0.002&lt;/bias_mean&gt; &lt;bias_stddev&gt;0.0003&lt;/bias_stddev&gt; &lt;/noise&gt; &lt;/z&gt; &lt;/angular_velocity&gt; &lt;linear_acceleration&gt; &lt;x&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;1.7e-2&lt;/stddev&gt; &lt;bias_mean&gt;0.01&lt;/bias_mean&gt; &lt;bias_stddev&gt;0.001&lt;/bias_stddev&gt; &lt;/noise&gt; &lt;/x&gt; &lt;y&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;1.7e-2&lt;/stddev&gt; &lt;bias_mean&gt;0.01&lt;/bias_mean&gt; &lt;bias_stddev&gt;0.001&lt;/bias_stddev&gt; &lt;/noise&gt; &lt;/y&gt; &lt;z&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;1.7e-2&lt;/stddev&gt; &lt;bias_mean&gt;0.01&lt;/bias_mean&gt; &lt;bias_stddev&gt;0.001&lt;/bias_stddev&gt; &lt;/noise&gt; &lt;/z&gt; &lt;/linear_acceleration&gt; &lt;/imu&gt; &lt;plugin filename=&quot;libgazebo_ros_imu_sensor.so&quot; name=&quot;imu_plugin&quot;&gt; &lt;topicName&gt;imu/data&lt;/topicName&gt; &lt;bodyName&gt;imu_link&lt;/bodyName&gt; &lt;updateRateHZ&gt;1000.0&lt;/updateRateHZ&gt; &lt;gaussianNoise&gt;0.0&lt;/gaussianNoise&gt; &lt;frameName&gt;imu_link&lt;/frameName&gt; &lt;initialOrientationAsReference&gt;false&lt;/initialOrientationAsReference&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt;   ","version":"Next","tagName":"h3"},{"title":"Force/Torque Sensor Simulation​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#forcetorque-sensor-simulation","content":" For manipulation and contact detection:  &lt;gazebo reference=&quot;left_hand&quot;&gt; &lt;sensor name=&quot;left_hand_ft_sensor&quot; type=&quot;force_torque&quot;&gt; &lt;always_on&gt;true&lt;/always_on&gt; &lt;update_rate&gt;100&lt;/update_rate&gt; &lt;force_torque&gt; &lt;frame&gt;child&lt;/frame&gt; &lt;measure_direction&gt;child_to_parent&lt;/measure_direction&gt; &lt;/force_torque&gt; &lt;plugin name=&quot;left_hand_ft_plugin&quot; filename=&quot;libgazebo_ros_ft_sensor.so&quot;&gt; &lt;topicName&gt;left_hand/force_torque&lt;/topicName&gt; &lt;frameName&gt;left_hand&lt;/frameName&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt;   ","version":"Next","tagName":"h3"},{"title":"Performance Optimization for Real-Time Simulation​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#performance-optimization-for-real-time-simulation","content":" ","version":"Next","tagName":"h2"},{"title":"Physics Optimization Strategies​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#physics-optimization-strategies","content":" Contact Parameter Tuning​  &lt;!-- Optimize contact parameters for humanoid feet --&gt; &lt;collision name=&quot;foot_collision&quot;&gt; &lt;geometry&gt; &lt;box size=&quot;0.2 0.1 0.02&quot;/&gt; &lt;/geometry&gt; &lt;surface&gt; &lt;contact&gt; &lt;ode&gt; &lt;!-- Balance between stability and performance --&gt; &lt;min_depth&gt;0.002&lt;/min_depth&gt; &lt;!-- Not too small to avoid jitter --&gt; &lt;max_vel&gt;100&lt;/max_vel&gt; &lt;kp&gt;1e6&lt;/kp&gt; &lt;!-- High stiffness for good contact --&gt; &lt;kd&gt;1e4&lt;/kd&gt; &lt;!-- Appropriate damping to prevent oscillation --&gt; &lt;/ode&gt; &lt;/contact&gt; &lt;/surface&gt; &lt;/collision&gt;   Mass Distribution Optimization​  &lt;!-- Optimize link masses for simulation performance --&gt; &lt;link name=&quot;upper_arm&quot;&gt; &lt;inertial&gt; &lt;mass value=&quot;2.0&quot;/&gt; &lt;!-- Realistic but not too heavy --&gt; &lt;!-- Use simplified inertia tensor for performance --&gt; &lt;inertia ixx=&quot;0.01&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.01&quot; iyz=&quot;0.0&quot; izz=&quot;0.01&quot;/&gt; &lt;/inertial&gt; &lt;/link&gt;   ","version":"Next","tagName":"h3"},{"title":"Sensor Performance Optimization​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#sensor-performance-optimization","content":" Camera Performance Settings​  &lt;sensor type=&quot;camera&quot; name=&quot;optimized_camera&quot;&gt; &lt;update_rate&gt;30&lt;/update_rate&gt; &lt;!-- Balance quality and performance --&gt; &lt;camera name=&quot;head_camera&quot;&gt; &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt; &lt;image&gt; &lt;format&gt;R8G8B8&lt;/format&gt; &lt;width&gt;640&lt;/width&gt; &lt;!-- Not too high resolution for performance --&gt; &lt;height&gt;480&lt;/height&gt; &lt;/image&gt; &lt;clip&gt; &lt;near&gt;0.1&lt;/near&gt; &lt;far&gt;10.0&lt;/far&gt; &lt;/clip&gt; &lt;/camera&gt; &lt;!-- Use compressed transport for better performance --&gt; &lt;plugin name=&quot;camera_controller&quot; filename=&quot;libgazebo_ros_camera.so&quot;&gt; &lt;frame_name&gt;camera_link&lt;/frame_name&gt; &lt;update_rate&gt;30.0&lt;/update_rate&gt; &lt;/plugin&gt; &lt;/sensor&gt;   ","version":"Next","tagName":"h3"},{"title":"Vision-Language-Action Pipeline Integration​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#vision-language-action-pipeline-integration","content":" ","version":"Next","tagName":"h2"},{"title":"Physics for Vision Processing​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#physics-for-vision-processing","content":" The physics simulation must support accurate vision processing:  &lt;!-- Textured surfaces for visual feature detection --&gt; &lt;visual name=&quot;table_visual&quot;&gt; &lt;geometry&gt; &lt;box size=&quot;1 0.8 0.8&quot;/&gt; &lt;/geometry&gt; &lt;material&gt; &lt;script&gt; &lt;uri&gt;file://media/materials/scripts/gazebo.material&lt;/uri&gt; &lt;name&gt;Gazebo/Wood&lt;/name&gt; &lt;/script&gt; &lt;/material&gt; &lt;/visual&gt; &lt;!-- Distinctive objects for vision training --&gt; &lt;model name=&quot;training_object&quot;&gt; &lt;link name=&quot;link&quot;&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;cylinder radius=&quot;0.05&quot; length=&quot;0.1&quot;/&gt; &lt;/geometry&gt; &lt;material&gt; &lt;ambient&gt;1.0 0.0 0.0 1.0&lt;/ambient&gt; &lt;!-- Red for easy detection --&gt; &lt;diffuse&gt;1.0 0.0 0.0 1.0&lt;/diffuse&gt; &lt;/material&gt; &lt;/visual&gt; &lt;/link&gt; &lt;/model&gt;   ","version":"Next","tagName":"h3"},{"title":"Simulation Parameters for Learning​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#simulation-parameters-for-learning","content":" For effective training of VLA systems:  Physics Accuracy: High enough for sim-to-real transferSensor Noise: Realistic to improve robustnessEnvironmental Variation: Diverse scenarios for generalizationPerformance: Fast enough for efficient training  ","version":"Next","tagName":"h3"},{"title":"Advanced Configuration Examples​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#advanced-configuration-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Complete Humanoid Physics Configuration​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#complete-humanoid-physics-configuration","content":" &lt;?xml version=&quot;1.0&quot;?&gt; &lt;sdf version=&quot;1.7&quot;&gt; &lt;world name=&quot;humanoid_advanced&quot;&gt; &lt;!-- Advanced physics configuration --&gt; &lt;physics type=&quot;ode&quot;&gt; &lt;max_step_size&gt;0.001&lt;/max_step_size&gt; &lt;real_time_factor&gt;1.0&lt;/real_time_factor&gt; &lt;real_time_update_rate&gt;1000&lt;/real_time_update_rate&gt; &lt;gravity&gt;0 0 -9.8&lt;/gravity&gt; &lt;ode&gt; &lt;solver&gt; &lt;type&gt;quick&lt;/type&gt; &lt;iters&gt;200&lt;/iters&gt; &lt;!-- Higher iterations for humanoid stability --&gt; &lt;sor&gt;1.3&lt;/sor&gt; &lt;/solver&gt; &lt;constraints&gt; &lt;cfm&gt;1e-5&lt;/cfm&gt; &lt;erp&gt;0.2&lt;/erp&gt; &lt;contact_max_correcting_vel&gt;100&lt;/contact_max_correcting_vel&gt; &lt;contact_surface_layer&gt;0.001&lt;/contact_surface_layer&gt; &lt;/constraints&gt; &lt;/ode&gt; &lt;/physics&gt; &lt;!-- Lighting for vision systems --&gt; &lt;light name=&quot;sun&quot; type=&quot;directional&quot;&gt; &lt;cast_shadows&gt;true&lt;/cast_shadows&gt; &lt;pose&gt;0 0 10 0 0 0&lt;/pose&gt; &lt;diffuse&gt;0.8 0.8 0.8 1&lt;/diffuse&gt; &lt;specular&gt;0.2 0.2 0.2 1&lt;/specular&gt; &lt;attenuation&gt; &lt;range&gt;1000&lt;/range&gt; &lt;/attenuation&gt; &lt;direction&gt;-0.1 -0.1 -0.9&lt;/direction&gt; &lt;/light&gt; &lt;!-- Ground plane with realistic properties --&gt; &lt;model name=&quot;ground_plane&quot;&gt; &lt;static&gt;true&lt;/static&gt; &lt;link name=&quot;link&quot;&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;surface&gt; &lt;friction&gt; &lt;ode&gt; &lt;mu&gt;1.0&lt;/mu&gt; &lt;mu2&gt;1.0&lt;/mu2&gt; &lt;/ode&gt; &lt;/friction&gt; &lt;contact&gt; &lt;ode&gt; &lt;kp&gt;1e6&lt;/kp&gt; &lt;kd&gt;1e4&lt;/kd&gt; &lt;min_depth&gt;0.001&lt;/min_depth&gt; &lt;max_vel&gt;100&lt;/max_vel&gt; &lt;/ode&gt; &lt;/contact&gt; &lt;/surface&gt; &lt;/collision&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;plane&gt; &lt;normal&gt;0 0 1&lt;/normal&gt; &lt;size&gt;100 100&lt;/size&gt; &lt;/plane&gt; &lt;/geometry&gt; &lt;material&gt; &lt;script&gt; &lt;uri&gt;file://media/materials/scripts/gazebo.material&lt;/uri&gt; &lt;name&gt;Gazebo/Grey&lt;/name&gt; &lt;/script&gt; &lt;/material&gt; &lt;/visual&gt; &lt;/link&gt; &lt;/model&gt; &lt;!-- Environment objects --&gt; &lt;model name=&quot;table&quot;&gt; &lt;pose&gt;2 0 0.5 0 0 0&lt;/pose&gt; &lt;link name=&quot;base&quot;&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;1 0.8 0.8&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;surface&gt; &lt;friction&gt; &lt;ode&gt; &lt;mu&gt;0.5&lt;/mu&gt; &lt;/ode&gt; &lt;/friction&gt; &lt;/surface&gt; &lt;/collision&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;1 0.8 0.8&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;material&gt; &lt;ambient&gt;0.8 0.6 0.2 1&lt;/ambient&gt; &lt;diffuse&gt;0.8 0.6 0.2 1&lt;/diffuse&gt; &lt;/material&gt; &lt;/visual&gt; &lt;inertial&gt; &lt;mass&gt;20.0&lt;/mass&gt; &lt;inertia&gt; &lt;ixx&gt;1.0&lt;/ixx&gt; &lt;ixy&gt;0&lt;/ixy&gt; &lt;ixz&gt;0&lt;/ixz&gt; &lt;iyy&gt;1.0&lt;/iyy&gt; &lt;iyz&gt;0&lt;/iyz&gt; &lt;izz&gt;1.0&lt;/izz&gt; &lt;/inertia&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;/model&gt; &lt;!-- Objects for manipulation training --&gt; &lt;model name=&quot;training_objects&quot;&gt; &lt;!-- Red cylinder --&gt; &lt;model name=&quot;red_cylinder&quot;&gt; &lt;pose&gt;2.2 0.1 0.9 0 0 0&lt;/pose&gt; &lt;link name=&quot;link&quot;&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;geometry&gt; &lt;cylinder&gt; &lt;radius&gt;0.05&lt;/radius&gt; &lt;length&gt;0.1&lt;/length&gt; &lt;/cylinder&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;cylinder&gt; &lt;radius&gt;0.05&lt;/radius&gt; &lt;length&gt;0.1&lt;/length&gt; &lt;/cylinder&gt; &lt;/geometry&gt; &lt;material&gt; &lt;ambient&gt;0.8 0.1 0.1 1&lt;/ambient&gt; &lt;diffuse&gt;0.8 0.1 0.1 1&lt;/diffuse&gt; &lt;/material&gt; &lt;/visual&gt; &lt;inertial&gt; &lt;mass&gt;0.1&lt;/mass&gt; &lt;inertia&gt; &lt;ixx&gt;0.0001&lt;/ixx&gt; &lt;ixy&gt;0&lt;/ixy&gt; &lt;ixz&gt;0&lt;/ixz&gt; &lt;iyy&gt;0.0001&lt;/iyy&gt; &lt;iyz&gt;0&lt;/iyz&gt; &lt;izz&gt;0.0002&lt;/izz&gt; &lt;/inertia&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;/model&gt; &lt;!-- Blue cube --&gt; &lt;model name=&quot;blue_cube&quot;&gt; &lt;pose&gt;2.3 0.1 0.9 0 0 0&lt;/pose&gt; &lt;link name=&quot;link&quot;&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;0.08 0.08 0.08&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;0.08 0.08 0.08&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;material&gt; &lt;ambient&gt;0.1 0.1 0.8 1&lt;/ambient&gt; &lt;diffuse&gt;0.1 0.1 0.8 1&lt;/diffuse&gt; &lt;/material&gt; &lt;/visual&gt; &lt;inertial&gt; &lt;mass&gt;0.08&lt;/mass&gt; &lt;inertia&gt; &lt;ixx&gt;0.0001&lt;/ixx&gt; &lt;ixy&gt;0&lt;/ixy&gt; &lt;ixz&gt;0&lt;/ixz&gt; &lt;iyy&gt;0.0001&lt;/iyy&gt; &lt;iyz&gt;0&lt;/iyz&gt; &lt;izz&gt;0.0001&lt;/izz&gt; &lt;/inertia&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;/model&gt; &lt;/model&gt; &lt;/world&gt; &lt;/sdf&gt;   ","version":"Next","tagName":"h3"},{"title":"Real-time Performance Monitoring​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#real-time-performance-monitoring","content":" Create a monitoring node to track simulation performance:  import rclpy from rclpy.node import Node from std_msgs.msg import Float32 from gazebo_msgs.msg import PerformanceMetrics import time class SimulationMonitor(Node): def __init__(self): super().__init__('simulation_monitor') # Subscriptions self.metrics_sub = self.create_subscription( PerformanceMetrics, '/gazebo/performance_metrics', self.metrics_callback, 10 ) # Publishers for performance data self.real_time_factor_pub = self.create_publisher( Float32, '/simulation/real_time_factor', 10 ) self.simulation_timer = self.create_timer(1.0, self.performance_check) # Performance tracking self.last_sim_time = 0 self.last_real_time = time.time() self.get_logger().info('Simulation monitor initialized') def metrics_callback(self, msg): &quot;&quot;&quot;Process performance metrics from Gazebo&quot;&quot;&quot; if msg.real_time_factor &gt; 0: rtf_msg = Float32() rtf_msg.data = msg.real_time_factor self.real_time_factor_pub.publish(rtf_msg) self.get_logger().debug( f'Sim Time: {msg.sim_time.sec}.{msg.sim_time.nanosec}, ' f'Real Time Factor: {msg.real_time_factor:.2f}, ' f'Pending Commands: {msg.pending_commands}' ) def performance_check(self): &quot;&quot;&quot;Check overall simulation performance&quot;&quot;&quot; # This would include more sophisticated performance analysis self.get_logger().info('Performance check completed') def main(args=None): rclpy.init(args=args) monitor = SimulationMonitor() try: rclpy.spin(monitor) except KeyboardInterrupt: monitor.get_logger().info('Shutting down simulation monitor') finally: monitor.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Sim-to-Real Rigor (Principle III)​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#sim-to-real-rigor-principle-iii","content":" High-fidelity physics simulation parametersRealistic sensor noise models matching hardwareProper contact dynamics for accurate simulation  ","version":"Next","tagName":"h3"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#real-time-validation-principle-iv","content":" High-frequency IMU simulation (1000Hz) for balance feedbackPerformance optimization for real-time operationProper QoS profiles for time-critical sensor data  ","version":"Next","tagName":"h3"},{"title":"Visualization Requirements (Key Standard II)​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#visualization-requirements-key-standard-ii","content":" Mermaid diagrams for physics parameter relationshipsDetailed configuration examples with proper formatting  ","version":"Next","tagName":"h3"},{"title":"Best Practices for Humanoid Simulation​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#best-practices-for-humanoid-simulation","content":" ","version":"Next","tagName":"h2"},{"title":"1. Physics Stability​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#1-physics-stability","content":" Use appropriate time steps (0.001s) for humanoid stabilityConfigure sufficient solver iterations (100+) for contact stabilitySet proper constraint parameters (CFM, ERP) for realistic behavior  ","version":"Next","tagName":"h3"},{"title":"2. Sensor Accuracy​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#2-sensor-accuracy","content":" Match sensor noise characteristics to real hardwareUse appropriate update rates for different sensor typesConfigure realistic sensor ranges and fields of view  ","version":"Next","tagName":"h3"},{"title":"3. Performance Optimization​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#3-performance-optimization","content":" Balance accuracy with simulation speedUse simplified collision geometry where appropriateOptimize mesh complexity for visual rendering  ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Physics Parameter Tuning​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#exercise-1-physics-parameter-tuning","content":" Create a humanoid model with optimized physics parameters for:  Stable bipedal walkingRealistic joint dynamicsProper contact behavior for feet and handsPerformance optimization for real-time simulation  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Sensor Configuration​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#exercise-2-sensor-configuration","content":" Configure a complete sensor suite for a humanoid robot that includes:  RGB-D camera with realistic noise and distortionHigh-frequency IMU for balance feedbackForce/torque sensors for manipulationProper Gazebo plugins for each sensor type  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Performance Optimization​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#exercise-3-performance-optimization","content":" Implement and test performance optimization techniques for:  Physics simulation with multiple humanoid robotsHigh-resolution sensor simulationReal-time visualization and controlSim-to-real transfer validation  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#summary","content":" Physics and sensor simulation in Gazebo are fundamental to creating realistic digital twins of humanoid robots. Proper configuration of physics parameters, contact dynamics, and sensor models is essential for effective sim-to-real transfer and real-time validation. The Vision-Language-Action pipeline relies on accurate simulation of both physical interactions and sensor data to enable effective training and validation of humanoid robot capabilities.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 10 - Physics & Sensor Simulation in Gazebo","url":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation#further-reading","content":" &quot;Gazebo Physics Documentation&quot; - Official Gazebo physics guide&quot;Robotics, Vision and Control&quot; by Peter Corke (Simulation chapter)&quot;Programming Robots with ROS&quot; by Quigley et al. (Simulation section)&quot;Sim-to-Real Transfer in Robotics&quot; - Research papers on domain randomization&quot;Physics-Based Animation&quot; by Kenny Erleben et al. ","version":"Next","tagName":"h2"},{"title":"Chapter 11 - Unity for Human–Robot Interaction","type":0,"sectionRef":"#","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#learning-objectives","content":" Set up Unity for human-robot interaction simulationCreate interactive environments for HRI researchImplement VLA pipeline integration with Unity  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#introduction","content":" Unity provides a powerful platform for creating realistic human-robot interaction (HRI) scenarios that complement traditional robotics simulation tools. While Gazebo excels at physics simulation, Unity offers superior visual fidelity and interactive capabilities for HRI research. This chapter explores the integration of Unity with ROS 2 for creating immersive human-robot interaction environments, supporting the Vision-Language-Action pipeline through realistic visual simulation and interactive scenarios. Unity's capabilities for creating human-centered environments align with our Anthropomorphic Focus principle, enabling the development of humanoid robots that can operate effectively in spaces designed for human use.  ","version":"Next","tagName":"h2"},{"title":"Unity in the Robotics Ecosystem​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#unity-in-the-robotics-ecosystem","content":" ","version":"Next","tagName":"h2"},{"title":"Unity vs. Traditional Robotics Simulation​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#unity-vs-traditional-robotics-simulation","content":" Aspect\tGazebo\tUnityPhysics Simulation\tHigh-fidelity, real-time\tGood, with Unity Physics Visual Fidelity\tModerate\tVery High Human-Centered Environments\tBasic\tExcellent Interactivity\tLimited\tExtensive Realism\tFunctional\tPhotorealistic VR/AR Support\tLimited\tExcellent  ","version":"Next","tagName":"h3"},{"title":"Unity Robotics Hub​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#unity-robotics-hub","content":" Unity provides the Robotics Hub package that facilitates integration with ROS 2:  ROS-TCP-Connector: Communication bridge between Unity and ROS 2URDF-Importer: Import URDF models directly into UnityRobotics-Object-Pools: Optimized object management for robotics simulationsVisualizations: Tools for sensor data visualization  ","version":"Next","tagName":"h3"},{"title":"Setting Up Unity for Robotics​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#setting-up-unity-for-robotics","content":" ","version":"Next","tagName":"h2"},{"title":"Prerequisites and Installation​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#prerequisites-and-installation","content":" Unity Hub: Download from unity3d.comUnity Editor: Install version 2021.3 LTS or laterROS 2: Ensure ROS 2 (Humble Hawksbill) is installedVisual Studio: For C# scripting (Windows) or appropriate IDE  ","version":"Next","tagName":"h3"},{"title":"Installing Unity Robotics Packages​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#installing-unity-robotics-packages","content":" Open Unity Hub and create a new 3D projectIn the Package Manager (Window &gt; Package Manager): Install &quot;ROS TCP Connector&quot; from Package ManagerInstall &quot;URDF Importer&quot; from Package ManagerInstall &quot;Cinemachine&quot; for camera control  ","version":"Next","tagName":"h3"},{"title":"Basic Project Structure​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#basic-project-structure","content":" UnityHRIProject/ ├── Assets/ │ ├── Scripts/ # C# scripts for robotics integration │ ├── URDFs/ # Imported robot models │ ├── Scenes/ # Unity scenes for different environments │ ├── Materials/ # Visual materials for realistic rendering │ ├── Prefabs/ # Reusable robot and environment objects │ └── Plugins/ # ROS communication libraries ├── Packages/ └── ProjectSettings/   ","version":"Next","tagName":"h3"},{"title":"ROS-TCP-Connector Integration​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#ros-tcp-connector-integration","content":" ","version":"Next","tagName":"h2"},{"title":"Setting up ROS Communication​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#setting-up-ros-communication","content":" First, install the ROS-TCP-Connector in Unity:  // RobotCommunicator.cs - Basic ROS communication setup using UnityEngine; using Unity.Robotics.ROSTCPConnector; using RosMessageTypes.Std; public class RobotCommunicator : MonoBehaviour { ROSConnection ros; public string rosIPAddress = &quot;127.0.0.1&quot;; public int rosPort = 10000; void Start() { // Get the ROS connection static instance ros = ROSConnection.instance; ros.RegisteredGID += OnRegisteredGID; // Set the IP address and port for the ROS connection ros.Initialize(rosIPAddress, rosPort); Debug.Log($&quot;ROS Connection initialized to {rosIPAddress}:{rosPort}&quot;); } void OnRegisteredGID(ulong GID) { Debug.Log($&quot;Connected to ROS with GID: {GID}&quot;); } void OnDestroy() { if (ros != null) { ros.RegisteredGID -= OnRegisteredGID; } } }   ","version":"Next","tagName":"h3"},{"title":"Publishing and Subscribing to ROS Topics​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#publishing-and-subscribing-to-ros-topics","content":" using UnityEngine; using Unity.Robotics.ROSTCPConnector; using RosMessageTypes.Sensor; using RosMessageTypes.Geometry; public class HumanoidController : MonoBehaviour { ROSConnection ros; // Topics public string jointStatesTopic = &quot;/joint_states&quot;; public string cameraTopic = &quot;/camera/image_raw&quot;; // Robot joint transforms public Transform leftHip; public Transform leftKnee; public Transform leftAnkle; public Transform rightHip; public Transform rightKnee; public Transform rightAnkle; void Start() { ros = ROSConnection.instance; // Subscribe to joint states ros.Subscribe&lt;JointStateMsg&gt;(jointStatesTopic, JointStateCallback); } void JointStateCallback(JointStateMsg jointState) { // Update robot joint positions based on ROS messages for (int i = 0; i &lt; jointState.name.Count; i++) { string jointName = jointState.name[i]; double jointPosition = jointState.position[i]; Transform jointTransform = GetJointTransformByName(jointName); if (jointTransform != null) { // Apply rotation based on joint position jointTransform.localRotation = Quaternion.Euler( (float)jointPosition * Mathf.Rad2Deg, 0, 0); } } } Transform GetJointTransformByName(string name) { switch (name) { case &quot;left_hip_joint&quot;: return leftHip; case &quot;left_knee_joint&quot;: return leftKnee; case &quot;left_ankle_joint&quot;: return leftAnkle; case &quot;right_hip_joint&quot;: return rightHip; case &quot;right_knee_joint&quot;: return rightKnee; case &quot;right_ankle_joint&quot;: return rightAnkle; default: return null; } } void Update() { // Send periodic status updates SendRobotStatus(); } void SendRobotStatus() { // Create and publish robot status message // Implementation depends on specific requirements } }   ","version":"Next","tagName":"h3"},{"title":"Creating Human-Centered Environments​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#creating-human-centered-environments","content":" ","version":"Next","tagName":"h2"},{"title":"Environment Design Principles​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#environment-design-principles","content":" For effective HRI simulation, environments should reflect human-centered design:  Proportional Accuracy: Furniture and spaces at human scaleNavigation Paths: Clear pathways for both humans and robotsInteraction Points: Counters, tables, and surfaces for object exchangeAccessibility Features: Doors, ramps, and controls usable by humanoid robots  ","version":"Next","tagName":"h3"},{"title":"Sample Kitchen Environment​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#sample-kitchen-environment","content":" using UnityEngine; public class KitchenEnvironment : MonoBehaviour { [Header(&quot;Furniture References&quot;)] public Transform counterTop; public Transform fridge; public Transform table; public Transform cabinet; [Header(&quot;Interaction Points&quot;)] public Transform[] pickupPoints; public Transform[] placePoints; public Transform[] navigationWaypoints; void Start() { SetupEnvironment(); } void SetupEnvironment() { // Configure counter height for humanoid interaction if (counterTop != null) { counterTop.position = new Vector3(0, 0.9f, 0); // Standard counter height } // Setup pickup and place points SetupInteractionPoints(); // Create navigation mesh for pathfinding CreateNavigationMesh(); } void SetupInteractionPoints() { // Define points where robot can interact with objects // These should be at appropriate heights for humanoid manipulation for (int i = 0; i &lt; pickupPoints.Length; i++) { // Ensure points are reachable by humanoid arm if (pickupPoints[i].position.y &lt; 0.6f || pickupPoints[i].position.y &gt; 1.2f) { Debug.LogWarning($&quot;Pickup point {i} may be unreachable for humanoid&quot;); } } } void CreateNavigationMesh() { // In practice, you'd use Unity's NavMesh system // This is a placeholder for navigation setup Debug.Log(&quot;Navigation mesh setup completed&quot;); } }   ","version":"Next","tagName":"h3"},{"title":"Lighting and Realism​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#lighting-and-realism","content":" For photorealistic HRI scenarios:  using UnityEngine; using UnityEngine.Rendering; public class EnvironmentLighting : MonoBehaviour { [Header(&quot;Lighting Setup&quot;)] public Light mainLight; public Light[] fillLights; public bool useHDRP = false; void Start() { ConfigureLighting(); } void ConfigureLighting() { if (useHDRP) { ConfigureHDRPLighting(); } else { ConfigureBuiltInLighting(); } } void ConfigureHDRPLighting() { // Configure High Definition Render Pipeline RenderPipelineManager.beginCameraRendering += OnBeginCameraRendering; } void ConfigureBuiltInLighting() { // Configure built-in render pipeline if (mainLight != null) { mainLight.type = LightType.Directional; mainLight.intensity = 1.2f; mainLight.color = Color.white; mainLight.shadows = LightShadows.Soft; } // Add ambient lighting RenderSettings.ambientLight = new Color(0.4f, 0.4f, 0.4f, 1); RenderSettings.ambientMode = UnityEngine.Rendering.AmbientMode.Trilight; } void OnBeginCameraRendering(ScriptableRenderContext context, Camera camera) { // HDRP-specific rendering setup } }   ","version":"Next","tagName":"h3"},{"title":"Implementing Human-Robot Interaction Scenarios​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#implementing-human-robot-interaction-scenarios","content":" ","version":"Next","tagName":"h2"},{"title":"Basic HRI Framework​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#basic-hri-framework","content":" using UnityEngine; using System.Collections.Generic; public class HRIManager : MonoBehaviour { [Header(&quot;Human Interaction&quot;)] public GameObject humanCharacter; public float interactionDistance = 2.0f; [Header(&quot;Robot Configuration&quot;)] public GameObject humanoidRobot; public Transform[] interactionZones; [Header(&quot;Communication&quot;)] public string speechTopic = &quot;/speech_recognition&quot;; public string gestureTopic = &quot;/gesture_detection&quot;; private List&lt;InteractionEvent&gt; activeInteractions = new List&lt;InteractionEvent&gt;(); void Update() { CheckForInteractions(); UpdateInteractionStatus(); } void CheckForInteractions() { if (humanCharacter == null || humanoidRobot == null) return; float distance = Vector3.Distance( humanCharacter.transform.position, humanoidRobot.transform.position ); if (distance &lt;= interactionDistance) { // Trigger interaction logic HandleProximityInteraction(); } } void HandleProximityInteraction() { // Implement interaction logic based on distance and context Debug.Log(&quot;Human-Robot interaction detected!&quot;); // Possible interactions: // - Greeting sequence // - Task assignment // - Object handover // - Navigation assistance } void UpdateInteractionStatus() { // Update UI elements, send ROS messages, etc. } } [System.Serializable] public class InteractionEvent { public string eventType; public float timestamp; public Vector3 humanPosition; public Vector3 robotPosition; public string context; }   ","version":"Next","tagName":"h3"},{"title":"Gesture Recognition and Response​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#gesture-recognition-and-response","content":" using UnityEngine; public class GestureRecognition : MonoBehaviour { [Header(&quot;Gesture Configuration&quot;)] public Transform humanHandLeft; public Transform humanHandRight; public float gestureThreshold = 0.1f; [Header(&quot;Gesture Responses&quot;)] public GameObject robotHead; public float headTurnSpeed = 2.0f; private Vector3 lastLeftHandPos; private Vector3 lastRightHandPos; private bool gestureDetected = false; void Start() { if (humanHandLeft != null) lastLeftHandPos = humanHandLeft.position; if (humanHandRight != null) lastRightHandPos = humanHandRight.position; } void Update() { DetectGestures(); RespondToGestures(); } void DetectGestures() { if (humanHandLeft == null || humanHandRight == null) return; Vector3 currentLeftPos = humanHandLeft.position; Vector3 currentRightPos = humanHandRight.position; // Calculate movement vectors Vector3 leftMovement = currentLeftPos - lastLeftHandPos; Vector3 rightMovement = currentRightPos - lastRightHandPos; // Detect specific gestures if (leftMovement.magnitude &gt; gestureThreshold) { DetectLeftHandGesture(leftMovement); } if (rightMovement.magnitude &gt; gestureThreshold) { DetectRightHandGesture(rightMovement); } // Update positions for next frame lastLeftHandPos = currentLeftPos; lastRightHandPos = currentRightPos; } void DetectLeftHandGesture(Vector3 movement) { // Example: Wave gesture (horizontal movement) if (Mathf.Abs(movement.x) &gt; Mathf.Abs(movement.y) &amp;&amp; Mathf.Abs(movement.x) &gt; Mathf.Abs(movement.z)) { if (movement.x &gt; 0) { Debug.Log(&quot;Left hand wave right detected&quot;); RespondToWave(); } else { Debug.Log(&quot;Left hand wave left detected&quot;); } } } void DetectRightHandGesture(Vector3 movement) { // Example: Point gesture (forward movement) if (movement.z &gt; gestureThreshold * 2) { Debug.Log(&quot;Pointing gesture detected&quot;); LookAtPoint(); } } void RespondToWave() { // Robot responds to wave with head movement or speech StartCoroutine(RobotWaveResponse()); } void LookAtPoint() { // Robot looks in the direction pointed by human if (robotHead != null) { Vector3 lookDirection = (humanHandRight.position - transform.position).normalized; Quaternion targetRotation = Quaternion.LookRotation(lookDirection, Vector3.up); robotHead.rotation = Quaternion.Slerp( robotHead.rotation, targetRotation, headTurnSpeed * Time.deltaTime ); } } System.Collections.IEnumerator RobotWaveResponse() { // Simple head nod response if (robotHead != null) { Vector3 originalRotation = robotHead.localEulerAngles; for (float t = 0; t &lt; 1; t += Time.deltaTime * 2) { robotHead.localRotation = Quaternion.Euler( originalRotation.x + Mathf.Sin(t * Mathf.PI * 4) * 10, originalRotation.y, originalRotation.z ); yield return null; } robotHead.localEulerAngles = originalRotation; } } }   ","version":"Next","tagName":"h3"},{"title":"Vision-Language-Action Pipeline Integration​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#vision-language-action-pipeline-integration","content":" ","version":"Next","tagName":"h2"},{"title":"Unity Camera Integration​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#unity-camera-integration","content":" using UnityEngine; using Unity.Robotics.ROSTCPConnector; using RosMessageTypes.Sensor; using System.Collections; public class UnityCameraPublisher : MonoBehaviour { ROSConnection ros; public string cameraTopic = &quot;/unity_camera/image_raw&quot;; public Camera unityCamera; public int imageWidth = 640; int imageHeight = 480; int publishRate = 30; // Hz RenderTexture renderTexture; Texture2D texture2D; void Start() { ros = ROSConnection.instance; // Create render texture for camera renderTexture = new RenderTexture(imageWidth, imageHeight, 24); if (unityCamera != null) { unityCamera.targetTexture = renderTexture; } // Start coroutine to publish images at desired rate StartCoroutine(PublishCameraImages()); } IEnumerator PublishCameraImages() { float frameTime = 1.0f / publishRate; while (true) { yield return new WaitForSeconds(frameTime); PublishImage(); } } void PublishImage() { if (unityCamera == null) return; // Create temporary render texture to read pixels RenderTexture currentRT = RenderTexture.active; RenderTexture.active = renderTexture; if (texture2D == null) { texture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false); } texture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0); texture2D.Apply(); // Convert to ROS image format ImageMsg rosImage = CreateROSImage(texture2D); // Publish to ROS ros.Publish(cameraTopic, rosImage); RenderTexture.active = currentRT; } ImageMsg CreateROSImage(Texture2D texture) { // Convert Unity texture to ROS Image message ImageMsg img = new ImageMsg(); img.header = new std_msgs.Header(); img.header.stamp = new builtin_interfaces.Time(); img.header.frame_id = &quot;unity_camera_optical_frame&quot;; img.height = (uint)texture.height; img.width = (uint)texture.width; img.encoding = &quot;rgb8&quot;; img.is_bigendian = false; img.step = (uint)(texture.width * 3); // 3 bytes per pixel for RGB // Convert texture to byte array Color32[] colors = texture.GetPixels32(); byte[] imageData = new byte[colors.Length * 3]; for (int i = 0; i &lt; colors.Length; i++) { imageData[i * 3] = colors[i].r; imageData[i * 3 + 1] = colors[i].g; imageData[i * 3 + 2] = colors[i].b; } img.data = imageData; return img; } }   ","version":"Next","tagName":"h3"},{"title":"Speech and Language Integration​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#speech-and-language-integration","content":" using UnityEngine; using Unity.Robotics.ROSTCPConnector; using RosMessageTypes.Std; using RosMessageTypes.Diagnostic; public class SpeechIntegration : MonoBehaviour { ROSConnection ros; public string speechCommandTopic = &quot;/speech_commands&quot;; public string textToSpeechTopic = &quot;/tts_input&quot;; [Header(&quot;Speech Configuration&quot;)] public float confidenceThreshold = 0.7f; public string[] validCommands = { &quot;hello&quot;, &quot;stop&quot;, &quot;go&quot;, &quot;pick up&quot;, &quot;put down&quot;, &quot;follow me&quot; }; void Start() { ros = ROSConnection.instance; // Subscribe to speech recognition results ros.Subscribe&lt;StringMsg&gt;(&quot;/speech_recognition&quot;, SpeechCallback); } void SpeechCallback(StringMsg speechMsg) { string recognizedText = speechMsg.data; Debug.Log($&quot;Speech recognized: {recognizedText}&quot;); ProcessSpeechCommand(recognizedText); } void ProcessSpeechCommand(string command) { // Check if command is in our valid command list foreach (string validCmd in validCommands) { if (command.ToLower().Contains(validCmd.ToLower())) { ExecuteCommand(validCmd); return; } } // If no valid command found, publish to TTS StringMsg ttsMsg = new StringMsg(); ttsMsg.data = $&quot;I don't understand the command: {command}&quot;; ros.Publish(textToSpeechTopic, ttsMsg); } void ExecuteCommand(string command) { Debug.Log($&quot;Executing command: {command}&quot;); // Publish command-specific messages to ROS StringMsg cmdMsg = new StringMsg(); cmdMsg.data = command; ros.Publish(speechCommandTopic, cmdMsg); // Provide feedback StringMsg feedbackMsg = new StringMsg(); feedbackMsg.data = $&quot;Okay, I will {command}&quot;; ros.Publish(textToSpeechTopic, feedbackMsg); } // Method to simulate speech input (for testing) public void SimulateSpeechInput(string text) { StringMsg speechMsg = new StringMsg(); speechMsg.data = text; SpeechCallback(speechMsg); } }   ","version":"Next","tagName":"h3"},{"title":"Performance Optimization for Real-time HRI​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#performance-optimization-for-real-time-hri","content":" ","version":"Next","tagName":"h2"},{"title":"Object Pooling for Dynamic Objects​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#object-pooling-for-dynamic-objects","content":" using UnityEngine; using System.Collections.Generic; public class ObjectPooler : MonoBehaviour { [System.Serializable] public class Pool { public string tag; public GameObject prefab; public int size; } public List&lt;Pool&gt; pools; public Dictionary&lt;string, Queue&lt;GameObject&gt;&gt; poolDictionary; void Start() { poolDictionary = new Dictionary&lt;string, Queue&lt;GameObject&gt;&gt;(); foreach (Pool pool in pools) { Queue&lt;GameObject&gt; objectPool = new Queue&lt;GameObject&gt;(); for (int i = 0; i &lt; pool.size; i++) { GameObject obj = Instantiate(pool.prefab); obj.SetActive(false); objectPool.Enqueue(obj); } poolDictionary.Add(pool.tag, objectPool); } } public GameObject SpawnFromPool(string tag, Vector3 position, Quaternion rotation) { if (!poolDictionary.ContainsKey(tag)) { Debug.LogWarning($&quot;Pool with tag {tag} doesn't exist!&quot;); return null; } GameObject objectToSpawn = poolDictionary[tag].Dequeue(); objectToSpawn.SetActive(true); objectToSpawn.transform.position = position; objectToSpawn.transform.rotation = rotation; // Add object back to pool when deactivated PoolObject poolObj = objectToSpawn.GetComponent&lt;PoolObject&gt;(); if (poolObj == null) { poolObj = objectToSpawn.AddComponent&lt;PoolObject&gt;(); } poolObj.pool = this; poolObj.tag = tag; poolDictionary[tag].Enqueue(objectToSpawn); return objectToSpawn; } } public class PoolObject : MonoBehaviour { public ObjectPooler pool; public string tag; void OnDisable() { if (pool != null) { pool.SpawnFromPool(tag, transform.position, transform.rotation); } } }   ","version":"Next","tagName":"h3"},{"title":"Level of Detail (LOD) for Complex Environments​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#level-of-detail-lod-for-complex-environments","content":" using UnityEngine; [RequireComponent(typeof(MeshRenderer))] public class HRILODSystem : MonoBehaviour { [Header(&quot;LOD Configuration&quot;)] public float[] lodDistances = { 10f, 30f, 50f }; public Mesh[] lodMeshes; [Header(&quot;Performance Settings&quot;)] public bool enableLOD = true; public float updateInterval = 0.1f; private MeshRenderer meshRenderer; private Transform cameraTransform; private float lastUpdateTime; private int currentLOD = 0; void Start() { meshRenderer = GetComponent&lt;MeshRenderer&gt;(); cameraTransform = Camera.main.transform; if (lodMeshes.Length == 0) { // If no LOD meshes provided, disable LOD enableLOD = false; return; } } void Update() { if (!enableLOD || cameraTransform == null) return; if (Time.time - lastUpdateTime &gt; updateInterval) { UpdateLOD(); lastUpdateTime = Time.time; } } void UpdateLOD() { if (lodMeshes.Length == 0) return; float distance = Vector3.Distance(transform.position, cameraTransform.position); int newLOD = 0; for (int i = 0; i &lt; lodDistances.Length; i++) { if (distance &gt; lodDistances[i]) { newLOD = i + 1; } else { break; } } // Clamp to available LOD levels newLOD = Mathf.Min(newLOD, lodMeshes.Length - 1); if (newLOD != currentLOD) { UpdateMesh(newLOD); currentLOD = newLOD; } } void UpdateMesh(int lodLevel) { if (lodLevel &lt; lodMeshes.Length) { MeshFilter meshFilter = GetComponent&lt;MeshFilter&gt;(); if (meshFilter != null) { meshFilter.mesh = lodMeshes[lodLevel]; } } } }   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Anthropomorphic Focus (Principle II)​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#anthropomorphic-focus-principle-ii","content":" Human-centered environment design for humanoid robot interactionProportional accuracy for human-sized furniture and spacesInteraction scenarios designed for human-robot collaboration  ","version":"Next","tagName":"h3"},{"title":"Sim-to-Real Rigor (Principle III)​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#sim-to-real-rigor-principle-iii","content":" Realistic visual simulation for vision system trainingProper sensor simulation integration with Unity camerasEnvironment complexity matching real-world scenarios  ","version":"Next","tagName":"h3"},{"title":"Visualization Requirements (Key Standard II)​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#visualization-requirements-key-standard-ii","content":" High-fidelity visual rendering for realistic HRI scenariosProper material and lighting setup for photorealistic environmentsClear examples with proper code formatting  ","version":"Next","tagName":"h3"},{"title":"VLA Convergence Mandate (Principle I)​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#vla-convergence-mandate-principle-i","content":" Integration of vision, language, and action systems in UnityCamera integration for vision processingSpeech and command processing for language interface  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Humanoid Assistant Scenario​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#example-1-humanoid-assistant-scenario","content":" using UnityEngine; using Unity.Robotics.ROSTCPConnector; using RosMessageTypes.Actionlib; public class HumanoidAssistant : MonoBehaviour { ROSConnection ros; public Transform[] servicePoints; public float serviceRadius = 2.0f; [Header(&quot;Service Actions&quot;)] public string navigationTopic = &quot;/move_base_simple/goal&quot;; public string manipulationTopic = &quot;/manipulation_controller/command&quot;; void Start() { ros = ROSConnection.instance; Debug.Log(&quot;Humanoid assistant initialized&quot;); } void Update() { CheckForServiceRequests(); } void CheckForServiceRequests() { // Check if human is within service radius of any service point foreach (Transform servicePoint in servicePoints) { Collider[] hitColliders = Physics.OverlapSphere(servicePoint.position, serviceRadius); foreach (Collider collider in hitColliders) { if (collider.CompareTag(&quot;Human&quot;)) { HandleServiceRequest(servicePoint, collider.gameObject); break; } } } } void HandleServiceRequest(Transform servicePoint, GameObject human) { Debug.Log($&quot;Service request detected at {servicePoint.name}&quot;); // Move to service point MoveToLocation(servicePoint.position); // Wait for service completion StartCoroutine(ProvideService(servicePoint, human)); } void MoveToLocation(Vector3 targetPosition) { // Publish navigation goal to ROS // Implementation would involve sending a PoseStamped message Debug.Log($&quot;Moving to {targetPosition}&quot;); } System.Collections.IEnumerator ProvideService(Transform servicePoint, GameObject human) { yield return new WaitForSeconds(2.0f); // Simulate service time // Publish service completion Debug.Log(&quot;Service completed&quot;); } }   ","version":"Next","tagName":"h3"},{"title":"Example 2: Collaborative Task Environment​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#example-2-collaborative-task-environment","content":" using UnityEngine; using System.Collections.Generic; public class CollaborativeTaskManager : MonoBehaviour { [Header(&quot;Task Configuration&quot;)] public List&lt;TaskDefinition&gt; tasks; public Transform[] workstations; public GameObject[] objectsToManipulate; [Header(&quot;Human-Robot Collaboration&quot;)] public float collaborationDistance = 1.5f; public float taskCompletionThreshold = 0.1f; private int currentTaskIndex = 0; private bool taskInProgress = false; void Update() { if (tasks.Count &gt; 0 &amp;&amp; !taskInProgress) { StartNextTask(); } } void StartNextTask() { if (currentTaskIndex &gt;= tasks.Count) return; TaskDefinition currentTask = tasks[currentTaskIndex]; Debug.Log($&quot;Starting task: {currentTask.taskName}&quot;); // Set up the task environment SetupTaskEnvironment(currentTask); taskInProgress = true; } void SetupTaskEnvironment(TaskDefinition task) { // Position objects according to task requirements for (int i = 0; i &lt; task.objectPositions.Count; i++) { if (i &lt; objectsToManipulate.Length) { objectsToManipulate[i].transform.position = task.objectPositions[i]; } } // Enable task-specific interactions EnableTaskInteractions(task); } void EnableTaskInteractions(TaskDefinition task) { // Enable specific interaction modes based on task switch (task.taskType) { case TaskType.Assembly: EnableAssemblyMode(); break; case TaskType.Transport: EnableTransportMode(); break; case TaskType.Inspection: EnableInspectionMode(); break; } } void EnableAssemblyMode() { Debug.Log(&quot;Assembly mode enabled - human and robot can collaborate on assembly tasks&quot;); } void EnableTransportMode() { Debug.Log(&quot;Transport mode enabled - robot can assist with object transport&quot;); } void EnableInspectionMode() { Debug.Log(&quot;Inspection mode enabled - robot can assist with quality inspection&quot;); } } [System.Serializable] public class TaskDefinition { public string taskName; public TaskType taskType; public List&lt;Vector3&gt; objectPositions; public List&lt;Transform&gt; targetPositions; public string completionCriteria; } public enum TaskType { Assembly, Transport, Inspection, Maintenance }   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: HRI Environment Creation​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#exercise-1-hri-environment-creation","content":" Create a Unity scene that includes:  A human-centered environment (e.g., kitchen, office, or living room)Properly scaled furniture for humanoid robot interactionInteractive elements for human-robot collaborationRealistic lighting and materials  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: ROS Integration​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#exercise-2-ros-integration","content":" Implement ROS communication in Unity that:  Subscribes to joint states and visualizes robot in UnityPublishes camera images from Unity to ROSIntegrates speech recognition and text-to-speechHandles basic navigation and manipulation commands  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: VLA Pipeline Integration​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#exercise-3-vla-pipeline-integration","content":" Create a complete Unity scene that demonstrates:  Vision system integration with Unity camerasLanguage processing for human commandsAction execution for robot behaviorsReal-time interaction between human and robot avatars  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#summary","content":" Unity provides a powerful platform for creating realistic human-robot interaction scenarios that complement traditional robotics simulation. The high visual fidelity and interactive capabilities of Unity make it ideal for developing and testing humanoid robots in human-centered environments. Integration with ROS 2 enables the full Vision-Language-Action pipeline to be tested in photorealistic settings, supporting the development of robots that can operate effectively in spaces designed for human use.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 11 - Unity for Human–Robot Interaction","url":"/docs/chapters/module-3-simulation/chapter-11-unity-hri#further-reading","content":" &quot;Unity Robotics Package Documentation&quot; - Official Unity Robotics Hub guide&quot;Human-Robot Interaction: Fundamentals and Implementation&quot; by Chen and Sauser&quot;Unity in Action&quot; by Joe Hocking (for Unity fundamentals)&quot;ROS Robotics Projects&quot; by Anil Mahtani (for ROS-Unity integration)&quot;Computer Graphics: Principles and Practice&quot; for realistic rendering ","version":"Next","tagName":"h2"},{"title":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","type":0,"sectionRef":"#","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#learning-objectives","content":" Use RViz2 for robot visualization and debuggingApply RQt for custom GUI developmentImplement TF2 for coordinate frame management  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#introduction","content":" ROS 2 provides powerful tools for visualization, debugging, and coordinate management that are essential for developing and operating humanoid robots. RViz2 enables 3D visualization of robot state and sensor data, RQt provides a framework for custom GUI development, and TF2 (Transform Library 2) manages coordinate transformations critical for humanoid perception and action. This chapter explores these essential tools with special attention to their application in humanoid robotics, supporting the Vision-Language-Action pipeline through proper visualization and coordinate management.  ","version":"Next","tagName":"h2"},{"title":"RViz2: 3D Visualization for Humanoid Robots​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#rviz2-3d-visualization-for-humanoid-robots","content":" RViz2 is the 3D visualization tool for ROS 2, crucial for humanoid robot development and debugging.  ","version":"Next","tagName":"h2"},{"title":"Core Components of RViz2​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#core-components-of-rviz2","content":" Displays Panel​  RobotModel: Visualizes the robot's URDF model with joint positionsTF: Shows coordinate frames and their relationshipsLaserScan: Visualizes laser range finder dataImage: Displays camera imagesPointCloud: Shows 3D point cloud dataMarker: Custom visualization objectsAxes: Shows coordinate frame orientations  Views Panel​  FXY (Top-down): Top-down view of the robotOrbit: Free camera that orbits around a targetThird Person Follower: Camera follows the robotFirst Person: Camera attached to a robot link  ","version":"Next","tagName":"h3"},{"title":"Setting Up RViz2 for Humanoid Robots​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#setting-up-rviz2-for-humanoid-robots","content":" Basic Configuration File​  Create config/humanoid_view.rviz:  Panels: - Class: rviz_common/Displays Help Height: 78 Name: Displays Property Tree Widget: Expanded: - /Global Options1 - /Status1 - /RobotModel1 - /TF1 - /TF1/Frames1 - /Camera1 - /PointCloud1 Splitter Ratio: 0.5 Tree Height: 608 - Class: rviz_common/Selection Name: Selection - Class: rviz_common/Tool Properties Expanded: - /2D Goal Pose1 - /Publish Point1 Name: Tool Properties Splitter Ratio: 0.5886790156364441 - Class: rviz_common/Views Expanded: - /Current View1 Name: Views Splitter Ratio: 0.5 Visualization Manager: Class: &quot;&quot; Displays: - Alpha: 0.5 Cell Size: 1 Class: rviz_default_plugins/Grid Color: 160; 160; 164 Enabled: true Line Style: Line Width: 0.029999999329447746 Value: Lines Name: Grid Normal Cell Count: 0 Offset: X: 0 Y: 0 Z: 0 Plane: XY Plane Cell Count: 10 Reference Frame: &lt;Fixed Frame&gt; Value: true - Alpha: 1 Class: rviz_default_plugins/RobotModel Collision Enabled: false Description File: &quot;&quot; Description Source: Topic Description Topic: Depth: 5 Durability Policy: Volatile History Policy: Keep Last Reliability Policy: Reliable Value: /robot_description Enabled: true Links: All Links Enabled: true Expand Joint Details: false Expand Link Details: false Expand Tree: false Link Tree Style: Links in Alphabetic Order Name: RobotModel TF Prefix: &quot;&quot; Update Interval: 0 Value: true Visual Enabled: true - Class: rviz_default_plugins/TF Enabled: true Frame Timeout: 15 Frames: All Enabled: true Marker Scale: 0.30000001192092896 Name: TF Show Arrows: true Show Axes: true Show Names: false Tree: {} Update Interval: 0 Value: true - Class: rviz_default_plugins/Camera Enabled: true Image Topic: Depth: 5 Durability Policy: Volatile History Policy: Keep Last Reliability Policy: Reliable Value: /camera/image_raw Name: Camera Overlay Alpha: 0.5 Queue Size: 2 Transport Hint: raw Value: true Visibility: Grid: true PointCloud: true RobotModel: true TF: true Value: true Zoom Factor: 1 - Alpha: 1 Autocompute Intensity Bounds: true Autocompute Value Bounds: Max Value: 10 Min Value: -10 Value: true Axis: Z Channel Name: intensity Class: rviz_default_plugins/PointCloud2 Color: 255; 255; 255 Color Transformer: RGB8 Decay Time: 0 Enabled: true Invert Rainbow: false Max Color: 255; 255; 255 Max Intensity: 4096 Min Color: 0; 0; 0 Min Intensity: 0 Name: PointCloud Position Transformer: XYZ Queue Size: 10 Selectable: true Size (Pixels): 3 Size (m): 0.009999999776482582 Style: Flat Squares Topic: Depth: 5 Durability Policy: Volatile History Policy: Keep Last Reliability Policy: Reliable Value: /camera/depth/points Use Fixed Frame: true Use rainbow: true Value: true Enabled: true Global Options: Background Color: 48; 48; 48 Fixed Frame: base_link Frame Rate: 30 Name: root Tools: - Class: rviz_default_plugins/Interact Hide Inactive Objects: true - Class: rviz_default_plugins/MoveCamera - Class: rviz_default_plugins/Select - Class: rviz_default_plugins/FocusCamera - Class: rviz_default_plugins/Measure Line color: 128; 128; 0 - Class: rviz_default_plugins/SetInitialPose Topic: Depth: 5 Durability Policy: Volatile History Policy: Keep Last Reliability Policy: Reliable Value: /initialpose - Class: rviz_default_plugins/SetGoal Topic: Depth: 5 Durability Policy: Volatile History Policy: Keep Last Reliability Policy: Reliable Value: /goal_pose - Class: rviz_default_plugins/PublishPoint Single click: true Topic: Depth: 5 Durability Policy: Volatile History Policy: Keep Last Reliability Policy: Reliable Value: /clicked_point Transformation: Current: Class: rviz_default_plugins/TF Value: true Views: Current: Class: rviz_default_plugins/Orbit Distance: 2.5 Enable Stereo Rendering: Stereo Eye Separation: 0.05999999865889549 Stereo Focal Distance: 1 Swap Stereo Eyes: false Value: false Focal Point: X: 0 Y: 0 Z: 0 Focal Shape Fixed Size: true Focal Shape Size: 0.05000000074505806 Invert Z Axis: false Name: Current View Near Clip Distance: 0.009999999776482582 Pitch: 0.5 Target Frame: base_link Value: Orbit (rviz) Yaw: 0.5 Saved: ~ Window Geometry: Camera: collapsed: false Displays: collapsed: false Height: 993 Hide Left Dock: false Hide Right Dock: false QMainWindow State: 000000ff00000000fd00000004000000000000015600000363fc0200000008fb0000001200530065006c0065006300740069006f006e00000001e10000009b0000005c00fffffffb0000001e0054006f006f006c002000500072006f007000650072007400690065007302000001ed000001df00000185000000a3fb000000120056006900650077007300200054006f006f02000001df000002110000018500000122fb000000200054006f006f006c002000500072006f0070006500720074006900650073003203000002880000011d000002210000017afb000000100044006900730070006c006100790073010000003d0000029a000000c900fffffffb0000002000730065006c0065006300740069006f006e00200062007500660066006500720200000138000000aa0000023a00000294fb00000014005700690064006500530074006500720065006f02000000e6000000d2000003ee0000030bfb0000000c004b0069006e0065006300740200000186000001060000030c00000261000000010000010f00000363fc0200000003fb0000001e0054006f006f006c002000500072006f00700065007200740069006500730100000041000000780000000000000000fb0000000a00560069006500770073000000003d00000363000000a400fffffffb0000001200530065006c0065006300740069006f006e010000025a000000b200000000000000000000000200000490000000a9fc0100000001fb0000000a00560069006500770073030000004e00000080000002e10000019700000003000004420000003efc0100000002fb0000000800540069006d00650100000000000004420000000000000000fb0000000800540069006d00650100000000000004500000000000000000000004fc0000036300000004000000040000000800000008fc0000000100000002000000010000000a0054006f006f006c00730100000000ffffffff0000000000000000 Width: 1853 X: 67 Y: 27   ","version":"Next","tagName":"h3"},{"title":"RViz2 Plugins for Humanoid Robotics​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#rviz2-plugins-for-humanoid-robotics","content":" RobotModel Display​  Essential for visualizing the humanoid robot's structure:  Description Source: Set to &quot;Topic&quot; and topic to /robot_descriptionFixed Frame: Set to the robot's base frame (e.g., base_link)Links: Enable visualization of all robot links with proper colors  TF Display​  Critical for understanding coordinate frame relationships:  Frames: Enable all robot frames for debuggingMarker Scale: Adjust to make transforms visibleShow Names: Enable to identify frames during debugging  Camera Display​  For vision system debugging:  Image Topic: Set to the camera's image topic (e.g., /camera/image_raw)Transport Hint: Choose appropriate transport (raw, compressed, etc.)Overlay: Enable to overlay camera view on 3D scene  ","version":"Next","tagName":"h3"},{"title":"Launching RViz2 with Configuration​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#launching-rviz2-with-configuration","content":" &lt;!-- In launch file --&gt; &lt;node pkg=&quot;rviz2&quot; exec=&quot;rviz2&quot; name=&quot;rviz2&quot; args=&quot;-d $(find-pkg-share my_robot_description)/rviz/humanoid_view.rviz&quot;/&gt;   ","version":"Next","tagName":"h3"},{"title":"RQt: Custom GUI Development​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#rqt-custom-gui-development","content":" RQt provides a framework for creating custom GUI tools for ROS 2 applications.  ","version":"Next","tagName":"h2"},{"title":"Core RQt Concepts​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#core-rqt-concepts","content":" Plugin Architecture​  rqt_gui: The main GUI frameworkrqt_py_common: Common Python utilities for pluginsrqt_plot: Plotting tool for data visualizationrqt_console: Message logging and filteringrqt_graph: Node graph visualization  ","version":"Next","tagName":"h3"},{"title":"Creating a Custom RQt Plugin​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#creating-a-custom-rqt-plugin","content":" Plugin Structure​  my_rqt_package/ ├── CMakeLists.txt ├── package.xml ├── setup.py ├── my_rqt_package/ │ ├── __init__.py │ └── my_humanoid_control.py └── resource/ └── MyHumanoidControl.ui   Plugin Implementation​  Create my_rqt_package/my_humanoid_control.py:  from python_qt_binding import loadUi from python_qt_binding.QtWidgets import QWidget, QVBoxLayout, QPushButton, QLabel, QSlider from python_qt_binding.QtCore import Qt import rclpy from rclpy.node import Node from std_msgs.msg import Float64MultiArray from sensor_msgs.msg import JointState from builtin_interfaces.msg import Duration from rqt_gui_py.plugin import Plugin class MyHumanoidControl(Plugin): def __init__(self, context): super(MyHumanoidControl, self).__init__(context) self.setObjectName('MyHumanoidControl') # Create QWidget for the plugin self._widget = QWidget() # Initialize ROS node for the plugin rclpy.init(args=None) self.node = Node('rqt_humanoid_control') # Create UI elements layout = QVBoxLayout() # Joint control section self.joint_label = QLabel('Left Hip Position') layout.addWidget(self.joint_label) self.joint_slider = QSlider(Qt.Horizontal) self.joint_slider.setRange(-100, 100) # -1.0 to 1.0 radians * 100 self.joint_slider.setValue(0) self.joint_slider.valueChanged.connect(self.joint_slider_changed) layout.addWidget(self.joint_slider) # Status label self.status_label = QLabel('Ready') layout.addWidget(self.status_label) # Control buttons self.stand_button = QPushButton('Stand Up') self.stand_button.clicked.connect(self.stand_up) layout.addWidget(self.stand_button) self.walk_button = QPushButton('Walk') self.walk_button.clicked.connect(self.walk) layout.addWidget(self.walk_button) self._widget.setLayout(layout) context.add_widget(self._widget) # Create publishers self.joint_pub = self.node.create_publisher(Float64MultiArray, '/joint_group_position_controller/commands', 10) # Timer for updating ROS self.timer = self.node.create_timer(0.1, self.update_ros) def joint_slider_changed(self, value): # Convert slider value to radians position = value / 100.0 self.status_label.setText(f'Joint position: {position:.2f} rad') # Publish joint command msg = Float64MultiArray() msg.data = [position] # Simplified - real system would have multiple joints self.joint_pub.publish(msg) def stand_up(self): self.status_label.setText('Standing up...') # Send stand up command to robot def walk(self): self.status_label.setText('Walking...') # Send walk command to robot def update_ros(self): # Process ROS callbacks rclpy.spin_once(self.node, timeout_sec=0.01) def shutdown_plugin(self): # Clean up resources self.timer.destroy() self.node.destroy_node() rclpy.shutdown() def save_settings(self, plugin_settings, instance_settings): # Save plugin settings pass def restore_settings(self, plugin_settings, instance_settings): # Restore plugin settings pass   Plugin Configuration​  Create plugin.xml:  &lt;library path=&quot;my_rqt_package&quot;&gt; &lt;class name=&quot;My Humanoid Control&quot; type=&quot;my_rqt_package.my_humanoid_control.MyHumanoidControl&quot; base_class_type=&quot;rqt_gui_py::Plugin&quot;&gt; &lt;description&gt; Custom control panel for humanoid robot &lt;/description&gt; &lt;qtgui&gt; &lt;group&gt; &lt;label&gt;Robot Tools&lt;/label&gt; &lt;icon type=&quot;theme&quot;&gt;folder&lt;/icon&gt; &lt;statustip&gt;Robot tools&lt;/statustip&gt; &lt;/group&gt; &lt;label&gt;My Humanoid Control&lt;/label&gt; &lt;icon type=&quot;theme&quot;&gt;input-gaming&lt;/icon&gt; &lt;statustip&gt;Custom control panel for humanoid robot&lt;/statustip&gt; &lt;/qtgui&gt; &lt;/class&gt; &lt;/library&gt;   Package Configuration​  Update setup.py to include the plugin:  from setuptools import setup package_name = 'my_rqt_package' setup( name=package_name, version='0.0.0', packages=[package_name], package_dir={'': 'src'}, data_files=[ ('share/ament_index/resource_index/packages', ['resource/' + package_name]), ('share/' + package_name, ['package.xml']), ('share/' + package_name + '/resource', ['resource/MyHumanoidControl.ui']), ('lib/' + package_name, ['scripts/my_humanoid_control']), ('share/' + package_name, ['plugin.xml']), ], install_requires=['setuptools'], zip_safe=True, author='Your Name', maintainer='Your Name', maintainer_email='your.email@example.com', keywords=['ROS'], entry_points={ 'console_scripts': [ ], }, )   ","version":"Next","tagName":"h3"},{"title":"TF2: Transform Library for Coordinate Management​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#tf2-transform-library-for-coordinate-management","content":" TF2 (Transform Library 2) is critical for humanoid robots that must manage multiple coordinate frames for vision, navigation, and manipulation.  ","version":"Next","tagName":"h2"},{"title":"TF2 Core Concepts​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#tf2-core-concepts","content":" Coordinate Frames​  For humanoid robots, common frames include:  base_link: Robot's main body frameodom: Odometry frame for navigationmap: Global map framecamera_link: Camera frame for visionleft_hand: End-effector frame for manipulationimu_link: IMU frame for balance feedback  Transform Messages​  tf2_msgs/TFMessage: Contains multiple transformsgeometry_msgs/TransformStamped: Single transform with timestamp  ","version":"Next","tagName":"h3"},{"title":"TF2 in Python​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#tf2-in-python","content":" import rclpy from rclpy.node import Node from tf2_ros import TransformBroadcaster, TransformListener, Buffer from geometry_msgs.msg import TransformStamped, PointStamped from tf2_geometry_msgs import do_transform_point import tf2_ros class TF2DemoNode(Node): def __init__(self): super().__init__('tf2_demo_node') # Create transform broadcaster self.tf_broadcaster = TransformBroadcaster(self) # Create transform buffer and listener self.tf_buffer = Buffer() self.tf_listener = TransformListener(self.tf_buffer, self) # Timer to broadcast transforms self.timer = self.create_timer(0.1, self.broadcast_transforms) # Timer to lookup transforms self.lookup_timer = self.create_timer(1.0, self.lookup_transforms) def broadcast_transforms(self): &quot;&quot;&quot;Broadcast robot transforms&quot;&quot;&quot; t = TransformStamped() # Header t.header.stamp = self.get_clock().now().to_msg() t.header.frame_id = 'odom' t.child_frame_id = 'base_link' # Transform (simplified - moving in x direction) t.transform.translation.x = 1.0 # This would come from odometry t.transform.translation.y = 0.0 t.transform.translation.z = 0.0 t.transform.rotation.x = 0.0 t.transform.rotation.y = 0.0 t.transform.rotation.z = 0.0 t.transform.rotation.w = 1.0 self.tf_broadcaster.sendTransform(t) def lookup_transforms(self): &quot;&quot;&quot;Lookup and use transforms&quot;&quot;&quot; try: # Lookup transform from base_link to camera_link trans = self.tf_buffer.lookup_transform( 'odom', 'base_link', rclpy.time.Time()) # Use 0 for latest available self.get_logger().info( f'Robot position: x={trans.transform.translation.x:.2f}, ' f'y={trans.transform.translation.y:.2f}' ) except tf2_ros.TransformException as ex: self.get_logger().error(f'Could not transform: {ex}') def main(args=None): rclpy.init(args=args) node = TF2DemoNode() try: rclpy.spin(node) except KeyboardInterrupt: pass finally: node.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()   ","version":"Next","tagName":"h3"},{"title":"TF2 for Humanoid Robot Applications​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#tf2-for-humanoid-robot-applications","content":" Vision Integration​  def transform_point_to_robot_frame(self, camera_point): &quot;&quot;&quot;Transform a point from camera frame to robot base frame&quot;&quot;&quot; point_camera = PointStamped() point_camera.header.frame_id = 'camera_link' point_camera.header.stamp = self.get_clock().now().to_msg() point_camera.point.x = camera_point[0] point_camera.point.y = camera_point[1] point_camera.point.z = camera_point[2] try: # Transform to base frame point_base = self.tf_buffer.transform( point_camera, 'base_link', timeout=rclpy.duration.Duration(seconds=1.0) ) return [point_base.point.x, point_base.point.y, point_base.point.z] except tf2_ros.TransformException as e: self.get_logger().error(f'Transform failed: {e}') return None   Manipulation Planning​  def get_end_effector_pose(self, end_effector_frame): &quot;&quot;&quot;Get the pose of an end effector in base frame&quot;&quot;&quot; try: transform = self.tf_buffer.lookup_transform( 'base_link', end_effector_frame, rclpy.time.Time(), timeout=rclpy.duration.Duration(seconds=1.0) ) return transform except tf2_ros.TransformException as e: self.get_logger().error(f'Could not get end effector pose: {e}') return None   ","version":"Next","tagName":"h3"},{"title":"Integration with Vision-Language-Action Pipeline​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#integration-with-vision-language-action-pipeline","content":" ","version":"Next","tagName":"h2"},{"title":"RViz2 for VLA Debugging​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#rviz2-for-vla-debugging","content":" Vision: Camera feeds and point clouds for perception debuggingLanguage: Display recognized objects and commandsAction: Robot trajectory visualization and execution monitoring  ","version":"Next","tagName":"h3"},{"title":"TF2 for VLA Coordination​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#tf2-for-vla-coordination","content":" Vision-Action: Transform detected objects to manipulation frameLanguage-Action: Transform navigation goals from map to robot frameMulti-frame: Coordinate between vision, planning, and execution frames  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Humanoid Monitoring Dashboard​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#example-1-humanoid-monitoring-dashboard","content":" import sys from PyQt5.QtWidgets import QApplication, QMainWindow, QVBoxLayout, QWidget, QLabel, QProgressBar from PyQt5.QtCore import QTimer import rclpy from rclpy.node import Node from sensor_msgs.msg import JointState from std_msgs.msg import Float64MultiArray class HumanoidMonitor(Node): def __init__(self): super().__init__('humanoid_monitor') # Subscriptions self.joint_sub = self.create_subscription( JointState, '/joint_states', self.joint_callback, 10 ) self.status_sub = self.create_subscription( Float64MultiArray, '/balance_status', self.balance_callback, 10 ) # Data storage self.joint_positions = {} self.balance_stable = True def joint_callback(self, msg): for i, name in enumerate(msg.name): if i &lt; len(msg.position): self.joint_positions[name] = msg.position[i] def balance_callback(self, msg): # Balance status: 0=unstable, 1=stable if len(msg.data) &gt; 0: self.balance_stable = msg.data[0] &gt; 0.5 class MonitorWindow(QMainWindow): def __init__(self, ros_node): super().__init__() self.ros_node = ros_node self.initUI() # Timer to update UI self.timer = QTimer() self.timer.timeout.connect(self.update_ui) self.timer.start(100) # Update every 100ms def initUI(self): self.setWindowTitle('Humanoid Robot Monitor') self.setGeometry(100, 100, 400, 300) central_widget = QWidget() self.setCentralWidget(central_widget) layout = QVBoxLayout() # Balance status self.balance_label = QLabel('Balance Status: Stable') layout.addWidget(self.balance_label) # Joint count self.joint_count_label = QLabel('Joints: 0') layout.addWidget(self.joint_count_label) # Battery level (simulated) self.battery_label = QLabel('Battery: 100%') self.battery_bar = QProgressBar() self.battery_bar.setRange(0, 100) self.battery_bar.setValue(100) layout.addWidget(self.battery_label) layout.addWidget(self.battery_bar) central_widget.setLayout(layout) def update_ui(self): # Update balance status status = &quot;Stable&quot; if self.ros_node.balance_stable else &quot;Unstable&quot; self.balance_label.setText(f'Balance Status: {status}') # Update joint count joint_count = len(self.ros_node.joint_positions) self.joint_count_label.setText(f'Joints: {joint_count}') # Process ROS callbacks rclpy.spin_once(self.ros_node, timeout_sec=0.01) def main(): rclpy.init() # Create ROS node monitor_node = HumanoidMonitor() # Create Qt application app = QApplication(sys.argv) window = MonitorWindow(monitor_node) window.show() try: sys.exit(app.exec_()) except KeyboardInterrupt: pass finally: monitor_node.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()   ","version":"Next","tagName":"h3"},{"title":"Example 2: TF2 for Navigation and Manipulation​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#example-2-tf2-for-navigation-and-manipulation","content":" import rclpy from rclpy.node import Node from geometry_msgs.msg import PoseStamped, PointStamped from tf2_ros import Buffer, TransformListener from tf2_geometry_msgs import do_transform_point import tf2_ros class NavigationPlanner(Node): def __init__(self): super().__init__('navigation_planner') # TF buffer and listener self.tf_buffer = Buffer() self.tf_listener = TransformListener(self.tf_buffer, self) # Publishers for navigation self.nav_goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10) # Timer for demonstration self.timer = self.create_timer(5.0, self.plan_navigation) def plan_navigation(self): &quot;&quot;&quot;Plan navigation to a point in camera frame&quot;&quot;&quot; # Assume we detected an object in camera frame target_in_camera = PointStamped() target_in_camera.header.frame_id = 'camera_link' target_in_camera.header.stamp = self.get_clock().now().to_msg() target_in_camera.point.x = 1.0 # 1m in front of camera target_in_camera.point.y = 0.0 target_in_camera.point.z = 0.0 try: # Transform to map frame for navigation target_in_map = self.tf_buffer.transform( target_in_camera, 'map', timeout=rclpy.duration.Duration(seconds=1.0) ) # Create navigation goal goal = PoseStamped() goal.header.frame_id = 'map' goal.header.stamp = self.get_clock().now().to_msg() goal.pose.position = target_in_map.point goal.pose.orientation.w = 1.0 # No rotation self.nav_goal_pub.publish(goal) self.get_logger().info(f'Navigating to: x={goal.pose.position.x:.2f}, y={goal.pose.position.y:.2f}') except tf2_ros.TransformException as e: self.get_logger().error(f'Transform error: {e}') def main(args=None): rclpy.init(args=args) node = NavigationPlanner() try: rclpy.spin(node) except KeyboardInterrupt: pass finally: node.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Visualization Requirements (Key Standard II)​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#visualization-requirements-key-standard-ii","content":" Proper use of Docusaurus Admonitions in documentationMermaid diagrams for illustrating TF frame relationshipsClear examples for complex concepts  ","version":"Next","tagName":"h3"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#real-time-validation-principle-iv","content":" Efficient TF lookups for real-time applicationsProper QoS profiles for visualization dataPerformance considerations for GUI applications  ","version":"Next","tagName":"h3"},{"title":"Anthropomorphic Focus (Principle II)​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#anthropomorphic-focus-principle-ii","content":" Humanoid-specific coordinate frame managementIntegration with bipedal locomotion systemsManipulation frame considerations  ","version":"Next","tagName":"h3"},{"title":"Best Practices for Humanoid Robotics​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#best-practices-for-humanoid-robotics","content":" ","version":"Next","tagName":"h2"},{"title":"1. TF2 Best Practices​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#1-tf2-best-practices","content":" Use appropriate frame naming conventionsSet proper timeouts for transform lookupsHandle transform exceptions gracefullyUse static transforms for fixed relationships  ","version":"Next","tagName":"h3"},{"title":"2. RViz2 Best Practices​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#2-rviz2-best-practices","content":" Create configuration files for common viewsUse appropriate QoS profiles for visualization topicsOptimize visualization performance for real-time useInclude relevant displays for humanoid debugging  ","version":"Next","tagName":"h3"},{"title":"3. RQt Best Practices​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#3-rqt-best-practices","content":" Design intuitive interfaces for robot operatorsInclude safety features and validationUse appropriate data types for real-time performanceImplement proper error handling  ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Custom RViz2 Configuration​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#exercise-1-custom-rviz2-configuration","content":" Create a custom RViz2 configuration file for a humanoid robot that includes:  Robot model display with proper URDFTF tree visualization showing all relevant framesCamera feed for vision system monitoringPoint cloud display for 3D perceptionAppropriate fixed frame and view settings  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: RQt Plugin Development​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#exercise-2-rqt-plugin-development","content":" Develop a custom RQt plugin for humanoid robot control that includes:  Joint position sliders for key jointsBalance status indicatorEmergency stop buttonWalking gait controlsProper error handling and safety features  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: TF2 Integration​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#exercise-3-tf2-integration","content":" Implement TF2 functionality for a humanoid robot that:  Manages coordinate frames for vision, navigation, and manipulationTransforms points between camera and base framesHandles exceptions and timeouts appropriatelyIntegrates with the VLA pipeline for object manipulation  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#summary","content":" RViz2, RQt, and TF2 are essential tools for humanoid robot development, providing visualization, custom interfaces, and coordinate management capabilities. These tools enable effective debugging and operation of complex humanoid systems, supporting the Vision-Language-Action pipeline through proper visualization and coordinate transformation. Understanding these tools is crucial for developing and operating humanoid robots that can function effectively in human-centered environments.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 8 - ROS 2 Tools: Rviz, RQt, TF2","url":"/docs/chapters/module-2-ros/chapter-8-ros2-tools#further-reading","content":" &quot;Programming Robots with ROS&quot; by Quigley et al. (Visualization chapter)&quot;Mastering ROS for Robotics Programming&quot; by Jayanam (RViz and TF chapters)&quot;ROS Robot Programming&quot; by Kim et al.&quot;TF2 tutorials&quot; - ROS Wiki&quot;RQt tutorials&quot; - ROS Wiki ","version":"Next","tagName":"h2"},{"title":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","type":0,"sectionRef":"#","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#learning-objectives","content":" Install and configure NVIDIA Isaac Sim for humanoid roboticsCreate high-fidelity simulation environmentsImplement physics and sensor simulation for humanoid robots  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#introduction","content":" NVIDIA Isaac Sim represents the state-of-the-art in high-fidelity robotics simulation, specifically designed to support the &quot;Sim-to-Real Rigor&quot; principle from our project constitution. Built on NVIDIA's Omniverse platform, Isaac Sim provides photorealistic rendering, accurate physics simulation, and comprehensive sensor modeling that enables effective training and validation of humanoid robots before deployment to physical hardware. This chapter covers the fundamentals of Isaac Sim, with special attention to its application in humanoid robotics, including the realistic physics simulation and sensor modeling required for the Vision-Language-Action pipeline.  ","version":"Next","tagName":"h2"},{"title":"Isaac Sim Overview​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#isaac-sim-overview","content":" ","version":"Next","tagName":"h2"},{"title":"Key Features and Capabilities​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#key-features-and-capabilities","content":" NVIDIA Isaac Sim provides several key capabilities that distinguish it from other simulation platforms:  PhysX Physics Engine​  High-fidelity physics: Accurate simulation of rigid body dynamics, contacts, and constraintsMulti-GPU support: Leverages NVIDIA GPUs for accelerated physics computationRealistic contact modeling: Advanced friction, restitution, and contact properties  RTX Rendering​  Photorealistic rendering: RTX ray tracing for realistic lighting and materialsSynthetic data generation: High-quality training data for computer vision systemsSensor simulation: Accurate camera, LIDAR, and other sensor models  Omniverse Integration​  USD-based workflows: Universal Scene Description for complex scene managementReal-time collaboration: Multiple users can work on the same simulationExtensible architecture: Python API for custom extensions and behaviors  ","version":"Next","tagName":"h3"},{"title":"Architecture and Components​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#architecture-and-components","content":" Isaac Sim consists of several key components:  Isaac Sim App: The main simulation applicationOmniverse Kit: Core platform for 3D simulationPhysX Engine: Physics simulation backendRTX Renderer: High-fidelity graphics renderingROS 2 Bridge: Integration with Robot Operating System 2Extensions: Modular components for specific functionality  ","version":"Next","tagName":"h3"},{"title":"Installing and Configuring Isaac Sim​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#installing-and-configuring-isaac-sim","content":" ","version":"Next","tagName":"h2"},{"title":"System Requirements​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#system-requirements","content":" Before installing Isaac Sim, ensure your system meets the requirements:  GPU: NVIDIA RTX 4070 Ti (12GB) or higher (as specified in our constitution)RAM: 32GB or more recommendedOS: Ubuntu 22.04 LTS (as specified in our constitution)CUDA: Compatible version for your GPUDocker: For containerized deployment (optional but recommended)  ","version":"Next","tagName":"h3"},{"title":"Installation Methods​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#installation-methods","content":" Method 1: Omniverse Launcher (Recommended)​  Download and install the Omniverse Launcher from NVIDIA Developer websiteSearch for &quot;Isaac Sim&quot; in the extensions catalogInstall the latest version (recommended: Isaac Sim 2023.1.1 or later)  Method 2: Docker Container (Production)​  # Pull the latest Isaac Sim container docker pull nvcr.io/nvidia/isaac-sim:latest # Run Isaac Sim with GPU support docker run --gpus all -it --rm \\ --network=host \\ --env &quot;ACCEPT_EULA=Y&quot; \\ --env &quot;NVIDIA_VISIBLE_DEVICES=all&quot; \\ --env &quot;NVIDIA_DRIVER_CAPABILITIES=all&quot; \\ --volume $(pwd)/isaac_sim_data:/isaac_sim_data \\ --volume /tmp/.X11-unix:/tmp/.X11-unix \\ --env &quot;DISPLAY=$DISPLAY&quot; \\ nvcr.io/nvidia/isaac-sim:latest   Method 3: Standalone Installation​  # Download Isaac Sim from NVIDIA Developer website # Follow the installation instructions for your platform # Ensure proper GPU drivers and CUDA are installed   ","version":"Next","tagName":"h3"},{"title":"Initial Configuration​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#initial-configuration","content":" After installation, configure Isaac Sim for humanoid robotics:  Launch Isaac Sim from the Omniverse LauncherSet up workspace directory for your projectsConfigure extensions needed for robotics simulation  ","version":"Next","tagName":"h3"},{"title":"Essential Extensions for Humanoid Robotics​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#essential-extensions-for-humanoid-robotics","content":" Enable these extensions for humanoid robot simulation:  Isaac ROS Bridge: ROS 2 integrationIsaac Sensors: Advanced sensor simulationIsaac Assets: Robot and environment assetsIsaac Navigation: Path planning and navigationIsaac Manipulation: Grasping and manipulation tools  ","version":"Next","tagName":"h3"},{"title":"Creating High-Fidelity Humanoid Environments​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#creating-high-fidelity-humanoid-environments","content":" ","version":"Next","tagName":"h2"},{"title":"USD Scene Structure​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#usd-scene-structure","content":" Isaac Sim uses Universal Scene Description (USD) for scene management:  humanoid_scenes/ ├── humanoid_test.usd # Main scene file ├── assets/ │ ├── robots/ │ │ ├── atlas.usd # Humanoid robot models │ │ └── simple_humanoid.usd │ ├── environments/ │ │ ├── kitchen.usd # Environment models │ │ ├── office.usd │ │ └── warehouse.usd │ └── objects/ │ ├── furniture.usd # Interactive objects │ └── tools.usd └── configs/ ├── robot_config.json # Robot configuration └── scene_config.json # Scene configuration   ","version":"Next","tagName":"h3"},{"title":"Basic Scene Setup​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#basic-scene-setup","content":" Create a simple humanoid scene in USD format:  # humanoid_basic.usda #usda 1.0 def Xform &quot;World&quot; { def Xform &quot;GroundPlane&quot; { def PhysicsMaterial &quot;GroundMaterial&quot; { float inputs:dynamicFriction = 0.5 float inputs:staticFriction = 0.5 float inputs:restitution = 0.1 } def Cube &quot;GroundPlaneCube&quot; { double3 xformOp:translate = (0, 0, -0.5) uniform double3 size = (100, 100, 1) rel physics:material = &lt;/World/GroundPlane/GroundMaterial&gt; rel xformOp:ordered = [&quot;xformOp:translate&quot;] } } def Xform &quot;Lighting&quot; { def DistantLight &quot;DistantLight&quot; { float intensity = 300 color3f color = (1, 1, 1) float3 direction = (-0.5, -0.5, -1) } def DomeLight &quot;DomeLight&quot; { float intensity = 0.5 asset inputs:texture:file = @hdri.hdr@ } } # Humanoid robot will be added here }   ","version":"Next","tagName":"h3"},{"title":"Physics Configuration for Humanoid Simulation​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#physics-configuration-for-humanoid-simulation","content":" Configure PhysX parameters for realistic humanoid physics:  # Physics scene configuration def PhysicsScene &quot;PhysicsScene&quot; { PhysicsSceneAPI &quot;PhysicsScene&quot; ( apiSchemas = [&quot;PhysicsSceneAPI&quot;] ) { float physics:gravity = -9.81 float physics:defaultPositionIterationCount = 8 float physics:defaultVelocityIterationCount = 4 float physics:simulationSubSteps = 1 } # Contact offset configuration for stable contacts def PhysicsMaterial &quot;HumanoidMaterial&quot; { float inputs:dynamicFriction = 0.7 float inputs:staticFriction = 0.8 float inputs:restitution = 0.1 float inputs:contactOffset = 0.001 float inputs:restOffset = 0.0 } }   ","version":"Next","tagName":"h3"},{"title":"Importing and Configuring Humanoid Robots​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#importing-and-configuring-humanoid-robots","content":" ","version":"Next","tagName":"h2"},{"title":"Using the URDF Importer​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#using-the-urdf-importer","content":" Isaac Sim includes a powerful URDF importer for bringing in humanoid robot models:  # Import URDF robot into Isaac Sim import omni from omni.isaac.core.utils.nucleus import get_assets_root_path from omni.isaac.core.utils.stage import add_reference_to_stage from omni.isaac.core.utils.prims import get_prim_at_path from pxr import UsdPhysics # Import humanoid robot from URDF def import_humanoid_robot(urdf_path, prim_path=&quot;/World/HumanoidRobot&quot;): # Add URDF reference to stage add_reference_to_stage( usd_path=urdf_path, prim_path=prim_path ) # Configure physics properties for the robot configure_robot_physics(prim_path) def configure_robot_physics(robot_path): # Get the robot prim robot_prim = get_prim_at_path(robot_path) # Set up joint properties for humanoid locomotion setup_joint_properties(robot_path) # Configure collision properties setup_collision_properties(robot_path) def setup_joint_properties(robot_path): # Configure joints for realistic humanoid movement import omni.kit.commands # Example: Configure hip joint with proper limits and dynamics omni.kit.commands.execute( &quot;ChangeGenericJointProperties&quot;, path=f&quot;{robot_path}/left_hip_joint&quot;, lower_limit=-1.57, upper_limit=1.57, stiffness=1000.0, damping=100.0, armature=0.1 )   ","version":"Next","tagName":"h3"},{"title":"Physics Joints Configuration​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#physics-joints-configuration","content":" For realistic humanoid movement, configure joints with appropriate properties:  def setup_humanoid_joints(robot_path): &quot;&quot;&quot;Configure joints for realistic humanoid physics&quot;&quot;&quot; # Hip joints - critical for bipedal locomotion configure_joint( f&quot;{robot_path}/left_hip_joint&quot;, joint_type=&quot;Joint&quot;, lower_limit=-1.57, # -90 degrees upper_limit=1.57, # 90 degrees stiffness=2000.0, damping=200.0, max_force=500.0 ) # Knee joints - important for walking configure_joint( f&quot;{robot_path}/left_knee_joint&quot;, joint_type=&quot;Joint&quot;, lower_limit=0.0, # No backward bending upper_limit=2.35, # ~135 degrees stiffness=1500.0, damping=150.0, max_force=400.0 ) # Ankle joints - crucial for balance configure_joint( f&quot;{robot_path}/left_ankle_joint&quot;, joint_type=&quot;Joint&quot;, lower_limit=-0.52, # -30 degrees upper_limit=0.52, # 30 degrees stiffness=800.0, damping=80.0, max_force=200.0 ) # Shoulder joints - for manipulation configure_joint( f&quot;{robot_path}/left_shoulder_joint&quot;, joint_type=&quot;Joint&quot;, lower_limit=-2.09, # -120 degrees upper_limit=1.57, # 90 degrees stiffness=1000.0, damping=100.0, max_force=300.0 )   ","version":"Next","tagName":"h3"},{"title":"Advanced Sensor Simulation​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#advanced-sensor-simulation","content":" ","version":"Next","tagName":"h2"},{"title":"Camera Simulation for Vision Systems​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#camera-simulation-for-vision-systems","content":" Configure realistic camera sensors for the Vision component of the VLA pipeline:  from omni.isaac.sensor import Camera import numpy as np def setup_robot_cameras(robot_path): &quot;&quot;&quot;Set up cameras for humanoid robot vision system&quot;&quot;&quot; # Head-mounted RGB camera head_camera = Camera( prim_path=f&quot;{robot_path}/head_camera&quot;, frequency=30, # Hz resolution=(640, 480), position=np.array([0.1, 0.0, 0.0]), # Offset from head center orientation=np.array([0, 0, 0, 1]) # Default orientation ) # Configure camera properties for realistic simulation head_camera.add_motion_vectors_to_frame() head_camera.add_ground_truth_to_frame( &quot;/World/GroundPlane&quot;, &quot;/World/Objects&quot; ) # Depth camera for 3D perception depth_camera = Camera( prim_path=f&quot;{robot_path}/depth_camera&quot;, frequency=30, resolution=(640, 480), position=np.array([0.1, 0.0, 0.0]), orientation=np.array([0, 0, 0, 1]) ) # Add depth information to frame depth_camera.add_distance_to_image_plane_to_frame() return head_camera, depth_camera def setup_perception_pipeline(camera): &quot;&quot;&quot;Set up perception pipeline for the camera&quot;&quot;&quot; # Add various perception outputs camera.add_distortion_to_frame() # Lens distortion camera.add_fisheye_to_frame() # Fisheye effects if needed camera.add_occupancy_grid_to_frame() # For navigation   ","version":"Next","tagName":"h3"},{"title":"IMU Simulation for Balance Feedback​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#imu-simulation-for-balance-feedback","content":" Configure IMU sensors critical for the Real-Time Validation principle:  def setup_imu_sensors(robot_path): &quot;&quot;&quot;Set up IMU sensors for balance feedback&quot;&quot;&quot; # Main IMU in torso for balance control torso_imu = RigidBodyImu( prim_path=f&quot;{robot_path}/torso_imu&quot;, position=np.array([0.0, 0.0, 0.2]), # Upper torso orientation=np.array([0, 0, 0, 1]), update_frequency=1000, # High frequency for balance (1000Hz) noise_density=2e-4, # Gyro noise density random_walk=2e-5, # Gyro random walk bias_correlation_time=1000, velocity_random_walk=1.7e-2, # Accel noise bias_acc_correlation_time=300 ) # Additional IMUs for better state estimation head_imu = RigidBodyImu( prim_path=f&quot;{robot_path}/head_imu&quot;, position=np.array([0.0, 0.0, 0.5]), # Head position update_frequency=500 # Lower frequency, less critical ) return torso_imu, head_imu class RigidBodyImu: &quot;&quot;&quot;Custom IMU class for humanoid applications&quot;&quot;&quot; def __init__(self, prim_path, position, orientation, update_frequency=100, **kwargs): self.prim_path = prim_path self.position = position self.orientation = orientation self.update_frequency = update_frequency self.noise_params = kwargs # Initialize IMU in Isaac Sim self._create_imu() def _create_imu(self): &quot;&quot;&quot;Create the IMU sensor in the simulation&quot;&quot;&quot; # Implementation would create the actual IMU in Isaac Sim pass def get_imu_data(self): &quot;&quot;&quot;Get IMU data with realistic noise and bias&quot;&quot;&quot; # Return realistic IMU measurements pass   ","version":"Next","tagName":"h3"},{"title":"Force/Torque Sensors for Manipulation​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#forcetorque-sensors-for-manipulation","content":" Configure force/torque sensors for dexterous manipulation:  def setup_force_torque_sensors(robot_path): &quot;&quot;&quot;Set up force/torque sensors for manipulation&quot;&quot;&quot; # Sensors in robot hands for manipulation feedback left_hand_ft = setup_hand_force_torque(f&quot;{robot_path}/left_hand&quot;) right_hand_ft = setup_hand_force_torque(f&quot;{robot_path}/right_hand&quot;) # Sensors in feet for balance detection left_foot_ft = setup_foot_force_torque(f&quot;{robot_path}/left_foot&quot;) right_foot_ft = setup_foot_force_torque(f&quot;{robot_path}/right_foot&quot;) return { 'left_hand': left_hand_ft, 'right_hand': right_hand_ft, 'left_foot': left_foot_ft, 'right_foot': right_foot_ft } def setup_hand_force_torque(hand_path): &quot;&quot;&quot;Set up force/torque sensor in hand&quot;&quot;&quot; from omni.isaac.core.sensors import ForceSensor ft_sensor = ForceSensor( prim_path=f&quot;{hand_path}/ft_sensor&quot;, position=np.array([0.0, 0.0, -0.05]), # At the end of the hand update_frequency=100, # 100Hz for manipulation force_range=[-100, 100], # Up to 100N in each direction torque_range=[-10, 10] # Up to 10 Nm torque ) return ft_sensor def setup_foot_force_torque(foot_path): &quot;&quot;&quot;Set up force/torque sensor in foot for balance&quot;&quot;&quot; from omni.isaac.core.sensors import ForceSensor ft_sensor = ForceSensor( prim_path=f&quot;{foot_path}/ft_sensor&quot;, position=np.array([0.0, 0.0, -0.01]), # Top of foot update_frequency=1000, # High frequency for balance (matches IMU) force_range=[-1000, 1000], # Higher range for body weight support torque_range=[-50, 50] ) return ft_sensor   ","version":"Next","tagName":"h3"},{"title":"ROS 2 Integration​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#ros-2-integration","content":" ","version":"Next","tagName":"h2"},{"title":"Setting up ROS Bridge​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#setting-up-ros-bridge","content":" Isaac Sim provides excellent ROS 2 integration for the Vision-Language-Action pipeline:  import rospy from sensor_msgs.msg import JointState, Imu, Image from geometry_msgs.msg import Twist from std_msgs.msg import String from cv_bridge import CvBridge import numpy as np class IsaacSimROSBridge: def __init__(self): # Initialize ROS node rospy.init_node('isaac_sim_ros_bridge', anonymous=True) # Bridge instance self.bridge = CvBridge() # Publishers self.joint_pub = rospy.Publisher('/joint_states', JointState, queue_size=10) self.imu_pub = rospy.Publisher('/imu/data', Imu, queue_size=10) self.camera_pub = rospy.Publisher('/camera/image_raw', Image, queue_size=10) # Subscribers self.cmd_sub = rospy.Subscriber('/cmd_vel', Twist, self.cmd_vel_callback) self.speech_sub = rospy.Subscriber('/speech_commands', String, self.speech_callback) # Timer for publishing sensor data self.publish_timer = rospy.Timer(rospy.Duration(0.01), self.publish_sensor_data) # 100Hz # Robot state tracking self.robot_state = {} def cmd_vel_callback(self, msg): &quot;&quot;&quot;Handle velocity commands from ROS&quot;&quot;&quot; # Convert ROS velocity command to Isaac Sim robot control self.apply_velocity_command(msg.linear, msg.angular) def speech_callback(self, msg): &quot;&quot;&quot;Handle speech commands from ROS&quot;&quot;&quot; # Process speech command and update robot behavior self.process_speech_command(msg.data) def publish_sensor_data(self, event): &quot;&quot;&quot;Publish sensor data to ROS topics&quot;&quot;&quot; # Publish joint states joint_msg = self.create_joint_state_msg() self.joint_pub.publish(joint_msg) # Publish IMU data imu_msg = self.create_imu_msg() self.imu_pub.publish(imu_msg) # Publish camera data camera_msg = self.create_camera_msg() self.camera_pub.publish(camera_msg) def create_joint_state_msg(self): &quot;&quot;&quot;Create joint state message from Isaac Sim robot&quot;&quot;&quot; msg = JointState() msg.header.stamp = rospy.Time.now() msg.header.frame_id = &quot;base_link&quot; # Get joint positions from Isaac Sim joint_names, joint_positions, joint_velocities = self.get_robot_joint_states() msg.name = joint_names msg.position = joint_positions msg.velocity = joint_velocities return msg def create_imu_msg(self): &quot;&quot;&quot;Create IMU message from Isaac Sim IMU sensor&quot;&quot;&quot; msg = Imu() msg.header.stamp = rospy.Time.now() msg.header.frame_id = &quot;torso_imu&quot; # Get IMU data from Isaac Sim orientation, angular_velocity, linear_acceleration = self.get_imu_data() # Set orientation (simplified) msg.orientation.x = orientation[0] msg.orientation.y = orientation[1] msg.orientation.z = orientation[2] msg.orientation.w = orientation[3] # Set angular velocity msg.angular_velocity.x = angular_velocity[0] msg.angular_velocity.y = angular_velocity[1] msg.angular_velocity.z = angular_velocity[2] # Set linear acceleration msg.linear_acceleration.x = linear_acceleration[0] msg.linear_acceleration.y = linear_acceleration[1] msg.linear_acceleration.z = linear_acceleration[2] return msg def create_camera_msg(self): &quot;&quot;&quot;Create camera message from Isaac Sim camera&quot;&quot;&quot; # Get camera image from Isaac Sim image_data, width, height = self.get_camera_image() # Convert to ROS Image message msg = self.bridge.cv2_to_imgmsg(image_data, encoding=&quot;rgb8&quot;) msg.header.stamp = rospy.Time.now() msg.header.frame_id = &quot;camera_link&quot; return msg def get_robot_joint_states(self): &quot;&quot;&quot;Get current robot joint states from Isaac Sim&quot;&quot;&quot; # This would interface with Isaac Sim's physics engine # Implementation depends on the specific robot configuration joint_names = [&quot;left_hip_joint&quot;, &quot;left_knee_joint&quot;, &quot;left_ankle_joint&quot;, &quot;right_hip_joint&quot;, &quot;right_knee_joint&quot;, &quot;right_ankle_joint&quot;] joint_positions = [0.0] * len(joint_names) # Placeholder joint_velocities = [0.0] * len(joint_names) # Placeholder return joint_names, joint_positions, joint_velocities def get_imu_data(self): &quot;&quot;&quot;Get IMU data from Isaac Sim&quot;&quot;&quot; # Placeholder implementation orientation = [0.0, 0.0, 0.0, 1.0] # Quaternion (x, y, z, w) angular_velocity = [0.0, 0.0, 0.0] # rad/s linear_acceleration = [0.0, 0.0, -9.8] # m/s^2 return orientation, angular_velocity, linear_acceleration def get_camera_image(self): &quot;&quot;&quot;Get camera image from Isaac Sim&quot;&quot;&quot; # Placeholder implementation # In practice, this would get the actual rendered image from Isaac Sim width, height = 640, 480 image_data = np.zeros((height, width, 3), dtype=np.uint8) # Placeholder return image_data, width, height   ","version":"Next","tagName":"h3"},{"title":"High-Fidelity Physics Simulation​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#high-fidelity-physics-simulation","content":" ","version":"Next","tagName":"h2"},{"title":"Advanced PhysX Configuration​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#advanced-physx-configuration","content":" Configure PhysX for maximum realism in humanoid simulation:  def configure_advanced_physics(): &quot;&quot;&quot;Configure advanced PhysX parameters for humanoid simulation&quot;&quot;&quot; # Set global physics parameters from omni.physx import get_physx_interface physx = get_physx_interface() # Configure solver settings for stability physx.set_parameter(&quot;SolverType&quot;, 0) # 0=PBD, 1=TGS physx.set_parameter(&quot;EnableEnhancedDeterminism&quot;, True) physx.set_parameter(&quot;BounceThresholdVelocity&quot;, 0.5) # Configure collision settings physx.set_parameter(&quot;SleepThreshold&quot;, 0.005) physx.set_parameter(&quot;StabilizationThreshold&quot;, 0.02) # Configure broadphase settings physx.set_parameter(&quot;BroadphaseType&quot;, 0) # 0=SAP, 1=MBP, 2=PV32 def setup_humanoid_collision_geometry(robot_path): &quot;&quot;&quot;Set up detailed collision geometry for humanoid robot&quot;&quot;&quot; # Define collision shapes for each body part collision_config = { 'torso': { 'shape': 'capsule', 'dimensions': [0.15, 0.6], # radius, height 'position': [0, 0, 0.3] }, 'head': { 'shape': 'sphere', 'dimensions': [0.1], # radius 'position': [0, 0, 0.75] }, 'upper_arm': { 'shape': 'capsule', 'dimensions': [0.05, 0.3], # radius, length 'position': [0, 0, 0.15] }, 'lower_arm': { 'shape': 'capsule', 'dimensions': [0.04, 0.3], 'position': [0, 0, 0.15] }, 'thigh': { 'shape': 'capsule', 'dimensions': [0.08, 0.4], 'position': [0, 0, 0.2] }, 'calf': { 'shape': 'capsule', 'dimensions': [0.07, 0.4], 'position': [0, 0, 0.2] }, 'foot': { 'shape': 'box', 'dimensions': [0.2, 0.1, 0.05], 'position': [0.05, 0, -0.025] # Offset for natural foot position } } # Apply collision geometry to robot parts for part_name, config in collision_config.items(): apply_collision_geometry(f&quot;{robot_path}/{part_name}&quot;, config) def apply_collision_geometry(prim_path, config): &quot;&quot;&quot;Apply collision geometry to a robot part&quot;&quot;&quot; from omni.isaac.core.utils.prims import get_prim_at_path from omni.isaac.core.utils.stage import get_current_stage from pxr import UsdPhysics, Gf, Sdf stage = get_current_stage() prim = stage.GetPrimAtPath(prim_path) if not prim.IsValid(): print(f&quot;Prim {prim_path} not found&quot;) return # Create collision API for the prim UsdPhysics.CollisionAPI.Apply(prim) # Set collision properties collision_api = UsdPhysics.CollisionAPI(prim) collision_api.CreateCollisionEnabledAttr(True) # Create collision approximation based on shape if config['shape'] == 'capsule': # Create capsule collision shape collision_api.CreateApproximationAttr(&quot;capsule&quot;) elif config['shape'] == 'sphere': collision_api.CreateApproximationAttr(&quot;sphere&quot;) elif config['shape'] == 'box': collision_api.CreateApproximationAttr(&quot;convexHull&quot;) def setup_realistic_contact_materials(): &quot;&quot;&quot;Set up realistic contact materials for humanoid interaction&quot;&quot;&quot; # Different materials for different robot parts material_config = { 'foot_sole': { 'static_friction': 0.8, 'dynamic_friction': 0.7, 'restitution': 0.1, 'compliance': 1e-6 }, 'hand_grip': { 'static_friction': 0.9, 'dynamic_friction': 0.8, 'restitution': 0.05, 'compliance': 1e-5 }, 'body': { 'static_friction': 0.3, 'dynamic_friction': 0.2, 'restitution': 0.2, 'compliance': 1e-4 } } return material_config   ","version":"Next","tagName":"h3"},{"title":"Performance Optimization​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#performance-optimization","content":" ","version":"Next","tagName":"h2"},{"title":"Simulation Optimization Techniques​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#simulation-optimization-techniques","content":" For high-fidelity humanoid simulation while maintaining performance:  def optimize_simulation_performance(): &quot;&quot;&quot;Optimize Isaac Sim performance for humanoid simulation&quot;&quot;&quot; # Reduce unnecessary physics updates for static objects disable_physics_for_static_objects() # Optimize rendering settings optimize_rendering_settings() # Configure simulation sub-stepping configure_sub_stepping() # Set up level-of-detail for complex models setup_lod_system() def disable_physics_for_static_objects(): &quot;&quot;&quot;Disable physics for objects that don't need it&quot;&quot;&quot; from omni.isaac.core.utils.prims import get_prim_at_path from pxr import UsdPhysics # Example: Disable physics for ground plane if it's static ground_prim = get_prim_at_path(&quot;/World/GroundPlane&quot;) if ground_prim.IsValid(): rigid_body_api = UsdPhysics.RigidBodyAPI(ground_prim) if rigid_body_api: rigid_body_api.GetRigidBodyEnabledAttr().Set(False) def optimize_rendering_settings(): &quot;&quot;&quot;Optimize rendering for performance&quot;&quot;&quot; import omni from omni import kit # Reduce rendering quality for non-visual sensors config = { &quot;rtx-defaults&quot;: { &quot;enabled&quot;: True, &quot;maxDiffuseBounces&quot;: 2, &quot;maxReflectionRefractionBounces&quot;: 2, &quot;maxScatterBounces&quot;: 2 }, &quot;renderer-lighting&quot;: { &quot;enable&quot;: True, &quot;enableDenoising&quot;: False # Disable for performance } } for key, value in config.items(): for subkey, subvalue in value.items(): kit.app.get_framework().get_setting_store().set_value( f&quot;{key}.{subkey}&quot;, subvalue ) def configure_sub_stepping(): &quot;&quot;&quot;Configure sub-stepping for stable simulation&quot;&quot;&quot; from omni.physx import get_physx_interface physx = get_physx_interface() # Use sub-stepping for complex contact scenarios physx.set_parameter(&quot;SimulationSubsteps&quot;, 4) physx.set_parameter(&quot;MaxBiasClamp&quot;, 10.0) def setup_lod_system(): &quot;&quot;&quot;Set up level-of-detail for performance&quot;&quot;&quot; # Implementation would involve creating LOD groups # for complex models that are far from the camera pass   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Sim-to-Real Rigor (Principle III)​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#sim-to-real-rigor-principle-iii","content":" High-fidelity physics simulation using PhysX engineRealistic sensor modeling matching hardware specificationsAccurate contact dynamics for humanoid locomotion  ","version":"Next","tagName":"h3"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#real-time-validation-principle-iv","content":" High-frequency IMU simulation (1000Hz) for balance feedbackProper QoS profiles for real-time sensor dataPerformance optimization for real-time operation  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#target-hardware-optimization","content":" Configuration optimized for RTX 4070 Ti and Isaac Sim requirementsGPU-accelerated physics and renderingPerformance settings appropriate for target hardware  ","version":"Next","tagName":"h3"},{"title":"Visualization Requirements (Key Standard II)​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#visualization-requirements-key-standard-ii","content":" High-fidelity rendering with RTX technologyPhotorealistic environments for synthetic data generationProper material and lighting setup  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Humanoid Balance Testing Environment​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#example-1-humanoid-balance-testing-environment","content":" def create_balance_test_environment(): &quot;&quot;&quot;Create a specialized environment for humanoid balance testing&quot;&quot;&quot; # Import basic scene from omni.isaac.core.utils.stage import add_reference_to_stage from omni.isaac.core.utils.nucleus import get_assets_root_path # Create the scene stage_path = &quot;/Isaac/Environments/Simple_Room/simple_room.usd&quot; add_reference_to_stage(get_assets_root_path() + stage_path, &quot;/World&quot;) # Add balance testing elements create_narrow_beam(&quot;/World/NarrowBeam&quot;) create_sloped_surface(&quot;/World/Slope&quot;) create_unstable_surface(&quot;/World/UnstableSurface&quot;) # Add safety boundaries create_safety_boundary(&quot;/World/Boundary&quot;) def create_narrow_beam(prim_path): &quot;&quot;&quot;Create a narrow beam for balance testing&quot;&quot;&quot; from omni.isaac.core.utils.prims import create_prim from omni.isaac.core.utils.stage import get_current_stage from pxr import Gf # Create a narrow beam for balance testing create_prim( prim_path=prim_path, prim_type=&quot;Cylinder&quot;, position=(2.0, 0.0, 0.05), orientation=(0.0, 0.0, 0.0, 1.0), scale=(1.0, 0.05, 0.05) # Long, narrow beam ) # Add collision and physics properties stage = get_current_stage() beam_prim = stage.GetPrimAtPath(prim_path) from pxr import UsdPhysics UsdPhysics.CollisionAPI.Apply(beam_prim) rigid_body_api = UsdPhysics.RigidBodyAPI.Apply(beam_prim) rigid_body_api.GetRigidBodyEnabledAttr().Set(False) # Static object def create_sloped_surface(prim_path): &quot;&quot;&quot;Create a sloped surface for balance testing&quot;&quot;&quot; from omni.isaac.core.utils.prims import create_prim # Create a sloped plane create_prim( prim_path=prim_path, prim_type=&quot;Plane&quot;, position=(0.0, 2.0, 0.0), orientation=(0.0, 0.0, 0.2, 0.98), # 20 degree slope scale=(2.0, 2.0, 1.0) ) def setup_balance_control_demo(): &quot;&quot;&quot;Set up a complete balance control demonstration&quot;&quot;&quot; # Import humanoid robot robot_path = &quot;/World/HumanoidRobot&quot; urdf_path = &quot;path/to/humanoid.urdf&quot; # Replace with actual path # Import and configure robot import_humanoid_robot(urdf_path, robot_path) # Configure physics for balance configure_robot_physics(robot_path) # Add sensors setup_robot_cameras(robot_path) setup_imu_sensors(robot_path) # Set up ROS bridge for control ros_bridge = IsaacSimROSBridge() print(&quot;Balance control demo environment ready!&quot;)   ","version":"Next","tagName":"h3"},{"title":"Example 2: Manipulation Training Environment​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#example-2-manipulation-training-environment","content":" def create_manipulation_training_environment(): &quot;&quot;&quot;Create an environment for manipulation skill training&quot;&quot;&quot; # Set up a kitchen-like environment setup_kitchen_environment() # Add training objects with various properties add_training_objects() # Configure lighting for vision tasks configure_vision_lighting() # Set up data collection for synthetic dataset setup_data_collection() def setup_kitchen_environment(): &quot;&quot;&quot;Set up a kitchen environment for manipulation tasks&quot;&quot;&quot; # Create counter with appropriate height create_counter(&quot;/World/Counter&quot;, position=(1.5, 0, 0.9)) # Add cabinets create_cabinet(&quot;/World/Cabinet&quot;, position=(1.5, 1, 0.9)) # Add table create_table(&quot;/World/Table&quot;, position=(-1.5, 0, 0.75)) def create_counter(prim_path, position): &quot;&quot;&quot;Create a kitchen counter&quot;&quot;&quot; from omni.isaac.core.utils.prims import create_prim create_prim( prim_path=prim_path, prim_type=&quot;Cube&quot;, position=position, scale=(1.2, 0.6, 0.8) ) # Add physics properties from omni.isaac.core.utils.stage import get_current_stage from pxr import UsdPhysics stage = get_current_stage() counter_prim = stage.GetPrimAtPath(prim_path) UsdPhysics.CollisionAPI.Apply(counter_prim) def add_training_objects(): &quot;&quot;&quot;Add various objects for manipulation training&quot;&quot;&quot; objects_config = [ {&quot;name&quot;: &quot;RedCup&quot;, &quot;type&quot;: &quot;Cylinder&quot;, &quot;size&quot;: (0.05, 0.1), &quot;color&quot;: (1, 0, 0)}, {&quot;name&quot;: &quot;BlueBox&quot;, &quot;type&quot;: &quot;Cube&quot;, &quot;size&quot;: (0.08, 0.08, 0.08), &quot;color&quot;: (0, 0, 1)}, {&quot;name&quot;: &quot;GreenBottle&quot;, &quot;type&quot;: &quot;Cylinder&quot;, &quot;size&quot;: (0.03, 0.15), &quot;color&quot;: (0, 1, 0)}, {&quot;name&quot;: &quot;YellowPyramid&quot;, &quot;type&quot;: &quot;Cone&quot;, &quot;size&quot;: (0.06, 0.1), &quot;color&quot;: (1, 1, 0)} ] positions = [(1.6, -0.2, 0.95), (1.6, 0.1, 0.95), (1.6, 0.3, 0.95), (1.6, 0.0, 1.05)] for i, obj_config in enumerate(objects_config): create_training_object( f&quot;/World/Objects/{obj_config['name']}&quot;, obj_config, positions[i] ) def create_training_object(prim_path, config, position): &quot;&quot;&quot;Create a training object with specific properties&quot;&quot;&quot; from omni.isaac.core.utils.prims import create_prim from omni.isaac.core.utils.stage import get_current_stage from pxr import UsdPhysics, Gf # Create the object if config[&quot;type&quot;] == &quot;Cylinder&quot;: create_prim( prim_path=prim_path, prim_type=&quot;Cylinder&quot;, position=position, scale=(config[&quot;size&quot;][0], config[&quot;size&quot;][0], config[&quot;size&quot;][1]) ) elif config[&quot;type&quot;] == &quot;Cube&quot;: create_prim( prim_path=prim_path, prim_type=&quot;Cube&quot;, position=position, scale=config[&quot;size&quot;] ) # Add physics properties stage = get_current_stage() obj_prim = stage.GetPrimAtPath(prim_path) UsdPhysics.CollisionAPI.Apply(obj_prim) rigid_body_api = UsdPhysics.RigidBodyAPI.Apply(obj_prim) # Set mass based on size volume = calculate_volume(config[&quot;type&quot;], config[&quot;size&quot;]) mass = volume * 1000 # Assume density of 1000 kg/m^3 (water-like) rigid_body_api.CreateMassAttr(mass) def calculate_volume(obj_type, size): &quot;&quot;&quot;Calculate volume of an object&quot;&quot;&quot; if obj_type == &quot;Cylinder&quot;: radius, height = size[0], size[2] if len(size) &gt; 2 else size[1] return 3.14159 * radius * radius * height elif obj_type == &quot;Cube&quot;: return size[0] * size[1] * size[2] if len(size) &gt; 2 else size[0] * size[0] * size[1] return 0.1 # Default volume   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Physics Configuration​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#exercise-1-physics-configuration","content":" Configure a humanoid robot model in Isaac Sim with:  Proper joint limits and dynamics for bipedal locomotionRealistic mass distribution and inertial propertiesAppropriate collision geometry for stable simulationPerformance optimization settings  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Sensor Integration​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#exercise-2-sensor-integration","content":" Set up a complete sensor suite for a humanoid robot that includes:  RGB and depth cameras with realistic parametersHigh-frequency IMU for balance feedback (1000Hz)Force/torque sensors in hands and feetProper ROS 2 integration for all sensors  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Environment Creation​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#exercise-3-environment-creation","content":" Create a complex environment for humanoid robot training that includes:  Multiple rooms with human-centered furnitureVarious surfaces with different friction propertiesInteractive objects for manipulation tasksProper lighting for vision system training  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#summary","content":" NVIDIA Isaac Sim provides state-of-the-art simulation capabilities essential for developing humanoid robots that can operate effectively in real-world environments. The high-fidelity physics simulation, realistic sensor modeling, and photorealistic rendering support the Vision-Language-Action pipeline and enable effective sim-to-real transfer of learned behaviors. Understanding Isaac Sim fundamentals is crucial for creating digital twins that accurately represent the challenges of humanoid robotics in human-centered environments.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 12 - NVIDIA Isaac Sim Fundamentals","url":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals#further-reading","content":" &quot;NVIDIA Isaac Sim Documentation&quot; - Official Isaac Sim user guide&quot;PhysX SDK Guide&quot; - NVIDIA PhysX physics engine documentation&quot;Universal Scene Description (USD) Specification&quot;&quot;Omniverse Developer Documentation&quot;&quot;Robotics Simulation with Isaac Sim&quot; - NVIDIA Developer articles ","version":"Next","tagName":"h2"},{"title":"Chapter 9 - Gazebo Simulation Setup","type":0,"sectionRef":"#","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#learning-objectives","content":" Install and configure Gazebo for humanoid simulationCreate simulation environments for humanoid robotsIntegrate with ROS 2 for control and perception  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#introduction","content":" Gazebo provides a high-fidelity physics simulation environment essential for developing and testing humanoid robots before deployment to physical hardware. As part of our &quot;Sim-to-Real Rigor&quot; principle from the project constitution, Gazebo enables training of humanoid robots in virtual environments with realistic physics, sensor simulation, and environmental interactions. This chapter covers the setup and configuration of Gazebo for humanoid robot simulation, with special attention to the physics and sensor simulation requirements that support the Vision-Language-Action pipeline and enable safe testing of bipedal locomotion and dexterous manipulation capabilities.  ","version":"Next","tagName":"h2"},{"title":"Gazebo Overview​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#gazebo-overview","content":" Gazebo is a 3D simulation environment that provides:  Physics Engine: Realistic simulation of rigid body dynamicsSensor Simulation: Cameras, LIDAR, IMUs, force/torque sensorsEnvironment Modeling: Complex 3D worlds with realistic lightingROS Integration: Seamless integration with ROS 2 for control and perception  ","version":"Next","tagName":"h2"},{"title":"Key Components​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#key-components","content":" Gazebo Classic: The traditional Gazebo simulation environmentIgnition Gazebo: The newer, more modular version (Garden and newer)Gazebo GUI: Visual interface for simulation monitoringGazebo Server: Headless simulation backend  ","version":"Next","tagName":"h3"},{"title":"Installing Gazebo​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#installing-gazebo","content":" ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#prerequisites","content":" Before installing Gazebo, ensure ROS 2 is properly installed:  # Verify ROS 2 installation source /opt/ros/humble/setup.bash # Or your ROS 2 distribution echo $ROS_DISTRO   ","version":"Next","tagName":"h3"},{"title":"Installing Gazebo for ROS 2 Humble​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#installing-gazebo-for-ros-2-humble","content":" # Install Gazebo with ROS 2 integration sudo apt update sudo apt install ros-humble-gazebo-ros-pkgs ros-humble-gazebo-plugins sudo apt install gazebo   ","version":"Next","tagName":"h3"},{"title":"Alternative: Installing Ignition Gazebo (Garden)​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#alternative-installing-ignition-gazebo-garden","content":" For the latest features and better performance:  # Add Ignition repository sudo curl -sSL https://packages.osrfoundation.org/gazebo.gpg -o /usr/share/keyrings/gazebo-archive-keyring.gpg echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/gazebo-archive-keyring.gpg] http://packages.osrfoundation.org/gazebo/ubuntu-stable $(lsb_release -cs) main&quot; | sudo tee /etc/apt/sources.list.d/gazebo.list &gt; /dev/null sudo apt update sudo apt install ignition-garden   ","version":"Next","tagName":"h3"},{"title":"Basic Gazebo Configuration​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#basic-gazebo-configuration","content":" ","version":"Next","tagName":"h2"},{"title":"Environment Variables​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#environment-variables","content":" Set up environment variables for Gazebo:  # Add to ~/.bashrc or ~/.zshrc export GAZEBO_MODEL_PATH=$GAZEBO_MODEL_PATH:~/.gazebo/models:~/ros2_ws/src/my_robot_description/models export GAZEBO_RESOURCE_PATH=$GAZEBO_RESOURCE_PATH:~/ros2_ws/src/my_robot_description/worlds export GAZEBO_PLUGIN_PATH=$GAZEBO_PLUGIN_PATH:~/ros2_ws/install/lib export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/ros2_ws/install/lib   ","version":"Next","tagName":"h3"},{"title":"Gazebo World Files​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#gazebo-world-files","content":" Create a basic world file worlds/humanoid_test.world:  &lt;?xml version=&quot;1.0&quot; ?&gt; &lt;sdf version=&quot;1.6&quot;&gt; &lt;world name=&quot;humanoid_test&quot;&gt; &lt;!-- Include a default ground plane --&gt; &lt;include&gt; &lt;uri&gt;model://ground_plane&lt;/uri&gt; &lt;/include&gt; &lt;!-- Include a default light source --&gt; &lt;include&gt; &lt;uri&gt;model://sun&lt;/uri&gt; &lt;/include&gt; &lt;!-- Add some objects for testing --&gt; &lt;model name=&quot;table&quot;&gt; &lt;pose&gt;2 0 0.5 0 0 0&lt;/pose&gt; &lt;link name=&quot;link&quot;&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;1 0.8 0.8&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;1 0.8 0.8&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;material&gt; &lt;ambient&gt;0.8 0.6 0.2 1&lt;/ambient&gt; &lt;diffuse&gt;0.8 0.6 0.2 1&lt;/diffuse&gt; &lt;/material&gt; &lt;/visual&gt; &lt;/link&gt; &lt;/model&gt; &lt;!-- Add a simple box for manipulation --&gt; &lt;model name=&quot;box&quot;&gt; &lt;pose&gt;2.2 0.1 0.9 0 0 0&lt;/pose&gt; &lt;link name=&quot;link&quot;&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;0.1 0.1 0.1&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;0.1 0.1 0.1&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;material&gt; &lt;ambient&gt;0.2 0.8 0.2 1&lt;/ambient&gt; &lt;diffuse&gt;0.2 0.8 0.2 1&lt;/diffuse&gt; &lt;/material&gt; &lt;/visual&gt; &lt;inertial&gt; &lt;mass&gt;0.1&lt;/mass&gt; &lt;inertia&gt; &lt;ixx&gt;0.001&lt;/ixx&gt; &lt;ixy&gt;0&lt;/ixy&gt; &lt;ixz&gt;0&lt;/ixz&gt; &lt;iyy&gt;0.001&lt;/iyy&gt; &lt;iyz&gt;0&lt;/iyz&gt; &lt;izz&gt;0.001&lt;/izz&gt; &lt;/inertia&gt; &lt;/inertial&gt; &lt;/link&gt; &lt;/model&gt; &lt;/world&gt; &lt;/sdf&gt;   ","version":"Next","tagName":"h3"},{"title":"Integrating Humanoid Robot with Gazebo​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#integrating-humanoid-robot-with-gazebo","content":" ","version":"Next","tagName":"h2"},{"title":"URDF to SDF Conversion​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#urdf-to-sdf-conversion","content":" Gazebo can work directly with URDF files through the libgazebo_ros_xacro plugin:  Create a launch file launch/humanoid_gazebo.launch.py:  from launch import LaunchDescription from launch.actions import IncludeLaunchDescription from launch.launch_description_sources import PythonLaunchDescriptionSource from launch.substitutions import PathJoinSubstitution from launch_ros.actions import Node from launch_ros.substitutions import FindPackageShare def generate_launch_description(): # Launch Gazebo with custom world gazebo = IncludeLaunchDescription( PythonLaunchDescriptionSource([ PathJoinSubstitution([ FindPackageShare('gazebo_ros'), 'launch', 'gazebo.launch.py' ]) ]), launch_arguments={ 'world': PathJoinSubstitution([ FindPackageShare('my_robot_description'), 'worlds', 'humanoid_test.world' ]) }.items() ) # Spawn robot in Gazebo spawn_entity = Node( package='gazebo_ros', executable='spawn_entity.py', arguments=[ '-topic', 'robot_description', '-entity', 'humanoid_robot', '-x', '0', '-y', '0', '-z', '1.0' # Start 1m above ground for testing ], output='screen' ) # Robot state publisher robot_state_publisher = Node( package='robot_state_publisher', executable='robot_state_publisher', name='robot_state_publisher', output='screen', parameters=[{ 'use_sim_time': True, 'robot_description': Command([ PathJoinSubstitution([FindPackageShare('my_robot_description'), 'urdf', 'humanoid.urdf.xacro']) ]) }] ) return LaunchDescription([ gazebo, robot_state_publisher, spawn_entity ])   ","version":"Next","tagName":"h3"},{"title":"Adding Gazebo Plugins to URDF​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#adding-gazebo-plugins-to-urdf","content":" Enhance your humanoid URDF with Gazebo-specific plugins:  &lt;?xml version=&quot;1.0&quot;?&gt; &lt;robot xmlns:xacro=&quot;http://www.ros.org/wiki/xacro&quot; name=&quot;humanoid_with_gazebo&quot;&gt; &lt;!-- Include the basic robot definition --&gt; &lt;xacro:include filename=&quot;humanoid.urdf.xacro&quot;/&gt; &lt;!-- Gazebo plugins for ROS control --&gt; &lt;gazebo&gt; &lt;plugin name=&quot;gazebo_ros_control&quot; filename=&quot;libgazebo_ros_control.so&quot;&gt; &lt;robotNamespace&gt;/humanoid_robot&lt;/robotNamespace&gt; &lt;robotSimType&gt;gazebo_ros_control/DefaultRobotHWSim&lt;/robotSimType&gt; &lt;/plugin&gt; &lt;/gazebo&gt; &lt;!-- Gazebo plugins for sensors --&gt; &lt;gazebo reference=&quot;camera_link&quot;&gt; &lt;sensor type=&quot;camera&quot; name=&quot;camera_sensor&quot;&gt; &lt;always_on&gt;true&lt;/always_on&gt; &lt;update_rate&gt;30&lt;/update_rate&gt; &lt;camera name=&quot;head_camera&quot;&gt; &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt; &lt;!-- 60 degrees --&gt; &lt;image&gt; &lt;format&gt;R8G8B8&lt;/format&gt; &lt;width&gt;640&lt;/width&gt; &lt;height&gt;480&lt;/height&gt; &lt;/image&gt; &lt;clip&gt; &lt;near&gt;0.1&lt;/near&gt; &lt;far&gt;10&lt;/far&gt; &lt;/clip&gt; &lt;/camera&gt; &lt;plugin name=&quot;camera_controller&quot; filename=&quot;libgazebo_ros_camera.so&quot;&gt; &lt;frame_name&gt;camera_link&lt;/frame_name&gt; &lt;min_depth&gt;0.1&lt;/min_depth&gt; &lt;max_depth&gt;10.0&lt;/max_depth&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt; &lt;!-- IMU sensor for balance feedback --&gt; &lt;gazebo reference=&quot;torso&quot;&gt; &lt;sensor name=&quot;imu_sensor&quot; type=&quot;imu&quot;&gt; &lt;always_on&gt;true&lt;/always_on&gt; &lt;update_rate&gt;100&lt;/update_rate&gt; &lt;!-- High rate for balance control --&gt; &lt;imu&gt; &lt;angular_velocity&gt; &lt;x&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;2e-4&lt;/stddev&gt; &lt;/noise&gt; &lt;/x&gt; &lt;y&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;2e-4&lt;/stddev&gt; &lt;/noise&gt; &lt;/y&gt; &lt;z&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;2e-4&lt;/stddev&gt; &lt;/noise&gt; &lt;/z&gt; &lt;/angular_velocity&gt; &lt;linear_acceleration&gt; &lt;x&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;1.7e-2&lt;/stddev&gt; &lt;/noise&gt; &lt;/x&gt; &lt;y&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;1.7e-2&lt;/stddev&gt; &lt;/noise&gt; &lt;/y&gt; &lt;z&gt; &lt;noise type=&quot;gaussian&quot;&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;1.7e-2&lt;/stddev&gt; &lt;/noise&gt; &lt;/z&gt; &lt;/linear_acceleration&gt; &lt;/imu&gt; &lt;plugin filename=&quot;libgazebo_ros_imu_sensor.so&quot; name=&quot;imu_plugin&quot;&gt; &lt;topicName&gt;imu/data&lt;/topicName&gt; &lt;bodyName&gt;torso&lt;/bodyName&gt; &lt;updateRateHZ&gt;100.0&lt;/updateRateHZ&gt; &lt;gaussianNoise&gt;0.0&lt;/gaussianNoise&gt; &lt;frameName&gt;torso&lt;/frameName&gt; &lt;initialOrientationAsReference&gt;false&lt;/initialOrientationAsReference&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt; &lt;!-- Force/Torque sensor in left foot for balance detection --&gt; &lt;gazebo reference=&quot;left_foot&quot;&gt; &lt;sensor name=&quot;left_foot_ft_sensor&quot; type=&quot;force_torque&quot;&gt; &lt;always_on&gt;true&lt;/always_on&gt; &lt;update_rate&gt;100&lt;/update_rate&gt; &lt;force_torque&gt; &lt;frame&gt;child&lt;/frame&gt; &lt;measure_direction&gt;child_to_parent&lt;/measure_direction&gt; &lt;/force_torque&gt; &lt;plugin name=&quot;left_foot_ft_plugin&quot; filename=&quot;libgazebo_ros_ft_sensor.so&quot;&gt; &lt;topicName&gt;left_foot/force_torque&lt;/topicName&gt; &lt;frameName&gt;left_foot&lt;/frameName&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt; &lt;!-- Joint state publisher for Gazebo --&gt; &lt;gazebo&gt; &lt;plugin name=&quot;joint_state_publisher&quot; filename=&quot;libgazebo_ros_joint_state_publisher.so&quot;&gt; &lt;update_rate&gt;30&lt;/update_rate&gt; &lt;joint_name&gt;left_hip_joint&lt;/joint_name&gt; &lt;joint_name&gt;left_knee_joint&lt;/joint_name&gt; &lt;joint_name&gt;left_ankle_joint&lt;/joint_name&gt; &lt;joint_name&gt;right_hip_joint&lt;/joint_name&gt; &lt;joint_name&gt;right_knee_joint&lt;/joint_name&gt; &lt;joint_name&gt;right_ankle_joint&lt;/joint_name&gt; &lt;joint_name&gt;left_shoulder_joint&lt;/joint_name&gt; &lt;joint_name&gt;left_elbow_joint&lt;/joint_name&gt; &lt;/plugin&gt; &lt;/gazebo&gt; &lt;/robot&gt;   ","version":"Next","tagName":"h3"},{"title":"Physics Configuration for Humanoid Simulation​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#physics-configuration-for-humanoid-simulation","content":" ","version":"Next","tagName":"h2"},{"title":"Physics Engine Parameters​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#physics-engine-parameters","content":" Configure physics parameters for realistic humanoid simulation in your world file:  &lt;physics type=&quot;ode&quot;&gt; &lt;max_step_size&gt;0.001&lt;/max_step_size&gt; &lt;!-- Smaller steps for stability --&gt; &lt;real_time_factor&gt;1.0&lt;/real_time_factor&gt; &lt;real_time_update_rate&gt;1000&lt;/real_time_update_rate&gt; &lt;gravity&gt;0 0 -9.8&lt;/gravity&gt; &lt;ode&gt; &lt;solver&gt; &lt;type&gt;quick&lt;/type&gt; &lt;iters&gt;100&lt;/iters&gt; &lt;!-- More iterations for stability --&gt; &lt;sor&gt;1.3&lt;/sor&gt; &lt;/solver&gt; &lt;constraints&gt; &lt;cfm&gt;0.000001&lt;/cfm&gt; &lt;erp&gt;0.2&lt;/erp&gt; &lt;contact_max_correcting_vel&gt;100&lt;/contact_max_correcting_vel&gt; &lt;contact_surface_layer&gt;0.001&lt;/contact_surface_layer&gt; &lt;/constraints&gt; &lt;/ode&gt; &lt;/physics&gt;   ","version":"Next","tagName":"h3"},{"title":"Material Properties for Realistic Simulation​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#material-properties-for-realistic-simulation","content":" Add material properties for realistic contact simulation:  &lt;material name=&quot;gazebo/Blue&quot;&gt; &lt;ambient&gt;0.0 0.0 0.8 1.0&lt;/ambient&gt; &lt;diffuse&gt;0.0 0.0 0.8 1.0&lt;/diffuse&gt; &lt;specular&gt;0.0 0.0 0.8 1.0&lt;/specular&gt; &lt;/material&gt; &lt;material name=&quot;gazebo/Red&quot;&gt; &lt;ambient&gt;0.8 0.0 0.0 1.0&lt;/ambient&gt; &lt;diffuse&gt;0.8 0.0 0.0 1.0&lt;/diffuse&gt; &lt;specular&gt;0.8 0.0 0.0 1.0&lt;/specular&gt; &lt;/material&gt;   ","version":"Next","tagName":"h3"},{"title":"Control Integration with ROS 2​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#control-integration-with-ros-2","content":" ","version":"Next","tagName":"h2"},{"title":"ROS 2 Control Configuration​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#ros-2-control-configuration","content":" Create a control configuration file config/humanoid_control.yaml:  controller_manager: ros__parameters: update_rate: 100 # Hz joint_state_broadcaster: type: joint_state_broadcaster/JointStateBroadcaster left_leg_controller: type: position_controllers/JointGroupPositionController right_leg_controller: type: position_controllers/JointGroupPositionController torso_controller: type: position_controllers/JointGroupPositionController left_leg_controller: ros__parameters: joints: - left_hip_joint - left_knee_joint - left_ankle_joint right_leg_controller: ros__parameters: joints: - right_hip_joint - right_knee_joint - right_ankle_joint torso_controller: ros__parameters: joints: - neck_joint - left_shoulder_joint - left_elbow_joint   ","version":"Next","tagName":"h3"},{"title":"Launching with Controllers​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#launching-with-controllers","content":" Update your launch file to include ROS 2 control:  from launch import LaunchDescription from launch.actions import IncludeLaunchDescription, RegisterEventHandler from launch.event_handlers import OnProcessExit from launch.launch_description_sources import PythonLaunchDescriptionSource from launch.substitutions import PathJoinSubstitution from launch_ros.actions import Node from launch_ros.substitutions import FindPackageShare from launch.substitutions import Command def generate_launch_description(): # Launch Gazebo with custom world gazebo = IncludeLaunchDescription( PythonLaunchDescriptionSource([ PathJoinSubstitution([ FindPackageShare('gazebo_ros'), 'launch', 'gazebo.launch.py' ]) ]), launch_arguments={ 'world': PathJoinSubstitution([ FindPackageShare('my_robot_description'), 'worlds', 'humanoid_test.world' ]) }.items() ) # Robot state publisher robot_state_publisher = Node( package='robot_state_publisher', executable='robot_state_publisher', name='robot_state_publisher', output='screen', parameters=[{ 'use_sim_time': True, 'robot_description': Command([ 'xacro ', PathJoinSubstitution([FindPackageShare('my_robot_description'), 'urdf', 'humanoid_with_gazebo.urdf.xacro']) ]) }] ) # Spawn robot in Gazebo spawn_entity = Node( package='gazebo_ros', executable='spawn_entity.py', arguments=[ '-topic', 'robot_description', '-entity', 'humanoid_robot', '-x', '0', '-y', '0', '-z', '1.0' ], output='screen' ) # Load controller configurations joint_state_broadcaster_spawner = Node( package=&quot;controller_manager&quot;, executable=&quot;spawner&quot;, arguments=[&quot;joint_state_broadcaster&quot;, &quot;-c&quot;, &quot;/controller_manager&quot;], ) left_leg_controller_spawner = Node( package=&quot;controller_manager&quot;, executable=&quot;spawner&quot;, arguments=[&quot;left_leg_controller&quot;, &quot;-c&quot;, &quot;/controller_manager&quot;], ) right_leg_controller_spawner = Node( package=&quot;controller_manager&quot;, executable=&quot;spawner&quot;, arguments=[&quot;right_leg_controller&quot;, &quot;-c&quot;, &quot;/controller_manager&quot;], ) torso_controller_spawner = Node( package=&quot;controller_manager&quot;, executable=&quot;spawner&quot;, arguments=[&quot;torso_controller&quot;, &quot;-c&quot;, &quot;/controller_manager&quot;], ) # Create launch description and add actions return LaunchDescription([ gazebo, robot_state_publisher, spawn_entity, RegisterEventHandler( OnProcessExit( target_action=spawn_entity, on_exit=[joint_state_broadcaster_spawner], ) ), RegisterEventHandler( OnProcessExit( target_action=joint_state_broadcaster_spawner, on_exit=[left_leg_controller_spawner], ) ), RegisterEventHandler( OnProcessExit( target_action=left_leg_controller_spawner, on_exit=[right_leg_controller_spawner], ) ), RegisterEventHandler( OnProcessExit( target_action=right_leg_controller_spawner, on_exit=[torso_controller_spawner], ) ), ])   ","version":"Next","tagName":"h3"},{"title":"Vision System Integration​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#vision-system-integration","content":" ","version":"Next","tagName":"h2"},{"title":"Camera Configuration for VLA Pipeline​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#camera-configuration-for-vla-pipeline","content":" For the Vision-Language-Action pipeline, configure cameras with appropriate parameters:  &lt;gazebo reference=&quot;camera_link&quot;&gt; &lt;sensor type=&quot;depth&quot; name=&quot;rgbd_camera&quot;&gt; &lt;always_on&gt;true&lt;/always_on&gt; &lt;update_rate&gt;30&lt;/update_rate&gt; &lt;camera name=&quot;head_camera&quot;&gt; &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt; &lt;!-- 60 degrees --&gt; &lt;image&gt; &lt;format&gt;R8G8B8&lt;/format&gt; &lt;width&gt;640&lt;/width&gt; &lt;height&gt;480&lt;/height&gt; &lt;/image&gt; &lt;clip&gt; &lt;near&gt;0.1&lt;/near&gt; &lt;far&gt;10.0&lt;/far&gt; &lt;/clip&gt; &lt;noise&gt; &lt;type&gt;gaussian&lt;/type&gt; &lt;mean&gt;0.0&lt;/mean&gt; &lt;stddev&gt;0.007&lt;/stddev&gt; &lt;/noise&gt; &lt;/camera&gt; &lt;plugin name=&quot;camera_controller&quot; filename=&quot;libgazebo_ros_camera.so&quot;&gt; &lt;frame_name&gt;camera_link&lt;/frame_name&gt; &lt;min_depth&gt;0.1&lt;/min_depth&gt; &lt;max_depth&gt;10.0&lt;/max_depth&gt; &lt;update_rate&gt;30.0&lt;/update_rate&gt; &lt;hack_baseline&gt;0.07&lt;/hack_baseline&gt; &lt;distortion_k1&gt;0.0&lt;/distortion_k1&gt; &lt;distortion_k2&gt;0.0&lt;/distortion_k2&gt; &lt;distortion_k3&gt;0.0&lt;/distortion_k3&gt; &lt;distortion_t1&gt;0.0&lt;/distortion_t1&gt; &lt;distortion_t2&gt;0.0&lt;/distortion_t2&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt;   ","version":"Next","tagName":"h3"},{"title":"Simulation Optimization for Humanoid Robots​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#simulation-optimization-for-humanoid-robots","content":" ","version":"Next","tagName":"h2"},{"title":"Performance Considerations​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#performance-considerations","content":" For humanoid robots with many degrees of freedom:  Reduce Update Rates: Use appropriate update rates for different sensorsSimplify Collision Geometry: Use simpler shapes for collision detectionLimit Physics Steps: Balance accuracy with performance  ","version":"Next","tagName":"h3"},{"title":"Stability Tips for Bipedal Simulation​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#stability-tips-for-bipedal-simulation","content":" Proper Mass Distribution: Ensure realistic inertial propertiesAppropriate Joint Limits: Prevent impossible configurationsSufficient Damping: Add damping to prevent oscillationsSmall Time Steps: Use smaller physics steps for stability  ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Sim-to-Real Rigor (Principle III)​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#sim-to-real-rigor-principle-iii","content":" High-fidelity physics simulation for realistic humanoid behaviorProper sensor simulation matching real hardwarePhysics parameters tuned for accurate real-world behavior  ","version":"Next","tagName":"h3"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#real-time-validation-principle-iv","content":" High update rates for IMU sensors (100Hz) for balance feedbackProper QoS profiles for real-time sensor dataPerformance optimization for real-time simulation  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#target-hardware-optimization","content":" Simulation parameters optimized for deployment to Jetson Orin NanoEfficient sensor simulation within computational constraints  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Humanoid Balance Testing Environment​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#example-1-humanoid-balance-testing-environment","content":" Create a specialized world for balance testing:  &lt;?xml version=&quot;1.0&quot; ?&gt; &lt;sdf version=&quot;1.6&quot;&gt; &lt;world name=&quot;balance_test&quot;&gt; &lt;!-- Physics configuration --&gt; &lt;physics type=&quot;ode&quot;&gt; &lt;max_step_size&gt;0.001&lt;/max_step_size&gt; &lt;real_time_factor&gt;1.0&lt;/real_time_factor&gt; &lt;real_time_update_rate&gt;1000&lt;/real_time_update_rate&gt; &lt;gravity&gt;0 0 -9.8&lt;/gravity&gt; &lt;/physics&gt; &lt;!-- Ground plane with texture --&gt; &lt;include&gt; &lt;uri&gt;model://ground_plane&lt;/uri&gt; &lt;/include&gt; &lt;!-- Lighting --&gt; &lt;include&gt; &lt;uri&gt;model://sun&lt;/uri&gt; &lt;/include&gt; &lt;!-- Balance testing features --&gt; &lt;model name=&quot;narrow_beam&quot;&gt; &lt;pose&gt;2 0 0.05 0 0 0&lt;/pose&gt; &lt;link name=&quot;link&quot;&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;2 0.1 0.1&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;2 0.1 0.1&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;material&gt; &lt;ambient&gt;0.5 0.5 0.5 1&lt;/ambient&gt; &lt;diffuse&gt;0.5 0.5 0.5 1&lt;/diffuse&gt; &lt;/material&gt; &lt;/visual&gt; &lt;/link&gt; &lt;/model&gt; &lt;!-- Sloped surface for testing --&gt; &lt;model name=&quot;slope&quot;&gt; &lt;pose&gt;-2 0 0 0 0.2 0&lt;/pose&gt; &lt;link name=&quot;link&quot;&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;1 2 0.2&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;1 2 0.2&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;material&gt; &lt;ambient&gt;0.3 0.3 0.7 1&lt;/ambient&gt; &lt;diffuse&gt;0.3 0.3 0.7 1&lt;/diffuse&gt; &lt;/material&gt; &lt;/visual&gt; &lt;/link&gt; &lt;/model&gt; &lt;/world&gt; &lt;/sdf&gt;   ","version":"Next","tagName":"h3"},{"title":"Example 2: ROS 2 Node for Simulation Control​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#example-2-ros-2-node-for-simulation-control","content":" Create a node to test humanoid control in simulation:  import rclpy from rclpy.node import Node from sensor_msgs.msg import JointState, Imu from geometry_msgs.msg import Twist from std_msgs.msg import Float64MultiArray from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy import numpy as np class SimulationController(Node): def __init__(self): super().__init__('simulation_controller') # QoS profile for simulation (can be best effort) sim_qos = QoSProfile( depth=10, reliability=ReliabilityPolicy.BEST_EFFORT, durability=HistoryPolicy.VOLATILE ) # Subscriptions self.imu_sub = self.create_subscription( Imu, '/imu/data', self.imu_callback, sim_qos ) self.joint_sub = self.create_subscription( JointState, '/joint_states', self.joint_callback, sim_qos ) # Publishers self.left_leg_pub = self.create_publisher( Float64MultiArray, '/left_leg_controller/commands', 10 ) self.right_leg_pub = self.create_publisher( Float64MultiArray, '/right_leg_controller/commands', 10 ) # Balance control timer self.control_timer = self.create_timer(0.01, self.balance_control) # 100Hz # State variables self.current_orientation = None self.current_joint_positions = {} self.get_logger().info('Simulation controller initialized') def imu_callback(self, msg): &quot;&quot;&quot;Process IMU data for balance feedback&quot;&quot;&quot; self.current_orientation = msg.orientation # Extract roll, pitch, yaw for balance control # Simplified - use proper quaternion to RPY conversion in practice def joint_callback(self, msg): &quot;&quot;&quot;Update current joint positions&quot;&quot;&quot; for i, name in enumerate(msg.name): if i &lt; len(msg.position): self.current_joint_positions[name] = msg.position[i] def balance_control(self): &quot;&quot;&quot;Simple balance control algorithm&quot;&quot;&quot; if self.current_orientation is None: return # Simplified balance control - in practice, use proper control theory # Extract pitch from orientation (simplified) pitch = 2 * (self.current_orientation.z * self.current_orientation.w - self.current_orientation.x * self.current_orientation.y) # Calculate correction based on pitch error correction = -pitch * 10.0 # Proportional control # Send commands to legs to maintain balance left_cmd = Float64MultiArray() left_cmd.data = [0.0, 0.0, correction] # hip, knee, ankle right_cmd = Float64MultiArray() right_cmd.data = [0.0, 0.0, correction] # hip, knee, ankle self.left_leg_pub.publish(left_cmd) self.right_leg_pub.publish(right_cmd) def main(args=None): rclpy.init(args=args) controller = SimulationController() try: rclpy.spin(controller) except KeyboardInterrupt: controller.get_logger().info('Shutting down simulation controller') finally: controller.destroy_node() rclpy.shutdown() if __name__ == '__main__': main()   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Gazebo Environment Creation​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#exercise-1-gazebo-environment-creation","content":" Create a Gazebo world file that includes:  A humanoid-sized room with furnitureMultiple surfaces with different friction propertiesObstacles for navigation testingProper lighting for camera simulation  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Sensor Integration​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#exercise-2-sensor-integration","content":" Enhance your humanoid URDF with:  RGB-D camera for vision processingIMU sensors for balance feedbackForce/torque sensors in feetProper Gazebo plugins for each sensor  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Control System Integration​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#exercise-3-control-system-integration","content":" Implement a complete simulation setup with:  ROS 2 control configuration for all humanoid jointsProper launch files for starting simulationBalance control node for bipedal stabilityPerformance optimization for real-time operation  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#summary","content":" Gazebo provides the essential simulation environment for developing humanoid robots, enabling safe testing of complex behaviors before deployment to physical hardware. The high-fidelity physics and sensor simulation support the Vision-Language-Action pipeline and enable proper validation of bipedal locomotion and dexterous manipulation capabilities. Understanding Gazebo setup and configuration is crucial for effective humanoid robot development within our Sim-to-Real Rigor framework.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 9 - Gazebo Simulation Setup","url":"/docs/chapters/module-3-simulation/chapter-9-gazebo-setup#further-reading","content":" &quot;Gazebo Tutorial&quot; - Official Gazebo documentation&quot;Programming Robots with ROS&quot; by Quigley et al. (Simulation chapter)&quot;Mastering ROS for Robotics Programming&quot; by Jayanam (Gazebo section)&quot;Robotics, Vision and Control&quot; by Peter Corke (Simulation chapter)&quot;Gazebo and ROS Integration Guide&quot; - ROS Wiki ","version":"Next","tagName":"h2"},{"title":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","type":0,"sectionRef":"#","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#learning-objectives","content":" Use Isaac SDK for perception system developmentGenerate synthetic data for computer vision trainingImplement perception pipelines for humanoid robots  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#introduction","content":" The Isaac SDK provides powerful tools for developing perception systems and generating synthetic data that are essential for the Vision component of the Vision-Language-Action pipeline in humanoid robotics. With its advanced rendering capabilities and realistic sensor simulation, Isaac Sim enables the generation of high-quality synthetic datasets that can be used to train computer vision models for humanoid robots. This chapter explores the Isaac SDK's perception tools and synthetic data generation capabilities, with special attention to creating datasets that match the requirements for humanoid robot vision systems and support the Sim-to-Real Rigor principle from our project constitution.  ","version":"Next","tagName":"h2"},{"title":"Isaac SDK Overview​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#isaac-sdk-overview","content":" ","version":"Next","tagName":"h2"},{"title":"Key Components for Perception​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#key-components-for-perception","content":" The Isaac SDK includes several key components specifically designed for perception system development:  Isaac Sim Perception Tools​  Synthetic Data Generation: Create labeled training data for computer visionSensor Simulation: Accurate modeling of cameras, LIDAR, and other sensorsGround Truth Annotation: Automatic generation of semantic segmentation, depth maps, and bounding boxesDomain Randomization: Techniques to improve sim-to-real transfer  Isaac ROS Integration​  ROS 2 Bridge: Seamless integration with Robot Operating System 2Sensor Message Types: Support for standard ROS sensor message formatsPerception Pipelines: Integration with ROS perception frameworks  ","version":"Next","tagName":"h3"},{"title":"Synthetic Data Generation Pipeline​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#synthetic-data-generation-pipeline","content":" The synthetic data generation pipeline in Isaac SDK consists of several stages:  Scene Configuration: Setting up realistic environments with objectsSensor Placement: Positioning virtual sensors to capture relevant viewsDomain Randomization: Varying environmental parameters for robustnessData Collection: Capturing images, depth maps, and annotationsPost-Processing: Converting data to formats suitable for training  ","version":"Next","tagName":"h3"},{"title":"Setting Up Perception Systems​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#setting-up-perception-systems","content":" ","version":"Next","tagName":"h2"},{"title":"Basic Perception Setup​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#basic-perception-setup","content":" # perception_setup.py import omni from omni.isaac.core import World from omni.isaac.core.utils.stage import add_reference_to_stage from omni.isaac.core.utils.nucleus import get_assets_root_path from omni.isaac.sensor import Camera import numpy as np import carb class HumanoidPerceptionSystem: def __init__(self): # Initialize Isaac Sim world self.world = World(stage_units_in_meters=1.0) # Perception components self.cameras = {} self.perception_pipelines = {} # Data collection parameters self.collection_enabled = False self.data_buffer = [] # Domain randomization parameters self.domain_randomization = { 'lighting': True, 'textures': True, 'object_poses': True, 'camera_poses': True } def setup_robot_cameras(self, robot_path): &quot;&quot;&quot;Set up cameras for humanoid robot perception&quot;&quot;&quot; # Head-mounted RGB camera for primary vision head_camera = Camera( prim_path=f&quot;{robot_path}/head_camera&quot;, frequency=30, resolution=(640, 480), position=np.array([0.1, 0.0, 0.1]), # Offset from head orientation=np.array([0, 0, 0, 1]) ) # Add perception outputs head_camera.add_ground_truth_to_frame( &quot;/World/StaticObjects&quot;, &quot;/World/DynamicObjects&quot; ) # Enable semantic segmentation head_camera.add_semantic_segmentation_to_frame( &quot;/World/StaticObjects&quot;, &quot;/World/DynamicObjects&quot; ) # Enable depth map generation head_camera.add_distance_to_image_plane_to_frame() # Enable surface normals head_camera.add_surface_normals_to_frame() self.cameras['head'] = head_camera # Chest-mounted wide-angle camera for environment awareness chest_camera = Camera( prim_path=f&quot;{robot_path}/chest_camera&quot;, frequency=15, # Lower frequency for wider FOV resolution=(800, 600), position=np.array([0.0, 0.0, 0.3]), # Chest level orientation=np.array([0, 0, 0, 1]) ) # Configure wide FOV (120 degrees) from omni.isaac.core.utils.prims import get_prim_at_path camera_prim = get_prim_at_path(f&quot;{robot_path}/chest_camera&quot;) from pxr import UsdGeom UsdGeom.CameraAPI(camera_prim).GetHorizontalApertureAttr().Set(43.2) # 120 degree FOV UsdGeom.CameraAPI(camera_prim).GetFocalLengthAttr().Set(18.0) # Corresponding focal length # Add same perception outputs as head camera chest_camera.add_ground_truth_to_frame( &quot;/World/StaticObjects&quot;, &quot;/World/DynamicObjects&quot; ) chest_camera.add_semantic_segmentation_to_frame( &quot;/World/StaticObjects&quot;, &quot;/World/DynamicObjects&quot; ) chest_camera.add_distance_to_image_plane_to_frame() self.cameras['chest'] = chest_camera return head_camera, chest_camera def setup_perception_pipeline(self): &quot;&quot;&quot;Set up perception processing pipeline&quot;&quot;&quot; # Define perception tasks perception_tasks = { 'object_detection': { 'input': 'rgb', 'output': 'bounding_boxes', 'model': 'yolo' }, 'semantic_segmentation': { 'input': 'rgb', 'output': 'class_masks', 'model': 'deeplab' }, 'depth_estimation': { 'input': 'stereo_pair', # Or use depth from simulation 'output': 'depth_map', 'model': 'monodepth' }, 'pose_estimation': { 'input': 'rgb', 'output': 'object_poses', 'model': 'keypoint_rcnn' } } # Initialize pipeline components for task_name, task_config in perception_tasks.items(): self.perception_pipelines[task_name] = self.initialize_perception_task( task_name, task_config ) def initialize_perception_task(self, task_name, config): &quot;&quot;&quot;Initialize a specific perception task&quot;&quot;&quot; if task_name == 'object_detection': return ObjectDetectionPipeline(config) elif task_name == 'semantic_segmentation': return SemanticSegmentationPipeline(config) elif task_name == 'depth_estimation': return DepthEstimationPipeline(config) elif task_name == 'pose_estimation': return PoseEstimationPipeline(config) else: raise ValueError(f&quot;Unknown perception task: {task_name}&quot;) class PerceptionPipeline: &quot;&quot;&quot;Base class for perception pipelines&quot;&quot;&quot; def __init__(self, config): self.config = config self.enabled = True def process(self, input_data): &quot;&quot;&quot;Process input data and return results&quot;&quot;&quot; raise NotImplementedError class ObjectDetectionPipeline(PerceptionPipeline): def process(self, rgb_image): &quot;&quot;&quot;Detect objects in RGB image&quot;&quot;&quot; # In simulation, we can use ground truth # In real implementation, this would use a trained model bounding_boxes = self.generate_bounding_boxes(rgb_image) return bounding_boxes def generate_bounding_boxes(self, image): &quot;&quot;&quot;Generate bounding boxes from ground truth&quot;&quot;&quot; # This would interface with Isaac Sim's ground truth system return [] class SemanticSegmentationPipeline(PerceptionPipeline): def process(self, rgb_image): &quot;&quot;&quot;Generate semantic segmentation&quot;&quot;&quot; # Use Isaac Sim's semantic segmentation segmentation_map = self.get_semantic_segmentation(rgb_image) return segmentation_map def get_semantic_segmentation(self, image): &quot;&quot;&quot;Get semantic segmentation from Isaac Sim&quot;&quot;&quot; # Implementation would use Isaac Sim's segmentation API return np.zeros_like(image) class DepthEstimationPipeline(PerceptionPipeline): def process(self, depth_data): &quot;&quot;&quot;Process depth information&quot;&quot;&quot; # In simulation, depth is available directly return depth_data class PoseEstimationPipeline(PerceptionPipeline): def process(self, rgb_image, depth_data): &quot;&quot;&quot;Estimate object poses&quot;&quot;&quot; # Combine RGB and depth for pose estimation poses = self.estimate_poses(rgb_image, depth_data) return poses def estimate_poses(self, rgb_image, depth_data): &quot;&quot;&quot;Estimate poses using RGB-D data&quot;&quot;&quot; # Implementation would use Isaac Sim's pose estimation tools return []   ","version":"Next","tagName":"h3"},{"title":"Synthetic Data Generation​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#synthetic-data-generation","content":" ","version":"Next","tagName":"h2"},{"title":"Basic Data Collection Setup​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#basic-data-collection-setup","content":" # synthetic_data_generator.py import omni from omni.isaac.core.utils.prims import get_prim_at_path from omni.isaac.synthetic_utils import SyntheticDataHelper from PIL import Image import numpy as np import json import os from datetime import datetime class SyntheticDataGenerator: def __init__(self, output_dir=&quot;synthetic_data&quot;): self.output_dir = output_dir self.data_counter = 0 # Create output directory structure os.makedirs(f&quot;{output_dir}/images&quot;, exist_ok=True) os.makedirs(f&quot;{output_dir}/labels&quot;, exist_ok=True) os.makedirs(f&quot;{output_dir}/depth&quot;, exist_ok=True) os.makedirs(f&quot;{output_dir}/seg&quot;, exist_ok=True) # Data annotation formats self.annotation_formats = { 'coco': self.save_as_coco, 'yolo': self.save_as_yolo, 'kitti': self.save_as_kitti } # Domain randomization parameters self.randomization_params = { 'lighting': { 'intensity_range': (0.5, 2.0), 'color_temperature_range': (4000, 8000) }, 'textures': { 'roughness_range': (0.0, 1.0), 'metallic_range': (0.0, 1.0) }, 'object_poses': { 'position_jitter': 0.1, 'rotation_jitter': 0.1 } } def collect_data_batch(self, camera, num_samples=100, annotation_format='coco'): &quot;&quot;&quot;Collect a batch of synthetic data&quot;&quot;&quot; print(f&quot;Collecting {num_samples} samples...&quot;) batch_data = [] for i in range(num_samples): # Apply domain randomization self.apply_domain_randomization() # Render frame omni.timeline.get_timeline_interface().update_current_time(1.0/30.0) # 30 FPS # Get data from camera rgb_image = self.get_rgb_image(camera) depth_map = self.get_depth_map(camera) seg_map = self.get_segmentation_map(camera) ground_truth = self.get_ground_truth(camera) # Save data sample_id = f&quot;{self.data_counter:06d}&quot; self.save_sample(sample_id, rgb_image, depth_map, seg_map, ground_truth) # Store annotation annotation = self.create_annotation(sample_id, ground_truth) batch_data.append(annotation) self.data_counter += 1 if i % 10 == 0: print(f&quot;Collected {i+1}/{num_samples} samples&quot;) # Save annotations in specified format self.save_annotations(batch_data, annotation_format) print(f&quot;Data collection completed. {num_samples} samples saved to {self.output_dir}&quot;) return batch_data def get_rgb_image(self, camera): &quot;&quot;&quot;Get RGB image from camera&quot;&quot;&quot; # Get the latest RGB frame rgb_data = camera.get_rgb() return rgb_data def get_depth_map(self, camera): &quot;&quot;&quot;Get depth map from camera&quot;&quot;&quot; # Get the latest depth frame depth_data = camera.get_depth() return depth_data def get_segmentation_map(self, camera): &quot;&quot;&quot;Get semantic segmentation map from camera&quot;&quot;&quot; # Get the latest segmentation frame seg_data = camera.get_semantic_segmentation() return seg_data def get_ground_truth(self, camera): &quot;&quot;&quot;Get ground truth annotations from Isaac Sim&quot;&quot;&quot; # This would interface with Isaac Sim's ground truth system ground_truth = { 'bounding_boxes': [], 'object_poses': [], 'class_labels': [], 'instance_ids': [] } return ground_truth def apply_domain_randomization(self): &quot;&quot;&quot;Apply domain randomization to improve sim-to-real transfer&quot;&quot;&quot; # Randomize lighting if self.randomization_params['lighting']: self.randomize_lighting() # Randomize textures if self.randomization_params['textures']: self.randomize_textures() # Randomize object poses if self.randomization_params['object_poses']: self.randomize_object_poses() def randomize_lighting(self): &quot;&quot;&quot;Randomize lighting conditions&quot;&quot;&quot; from omni.isaac.core.utils.prims import get_prim_at_path from pxr import UsdLux # Get all lights in the scene light_prims = [prim for prim in omni.usd.get_context().get_stage().TraverseAll() if prim.IsA(UsdLux.DistantLight) or prim.IsA(UsdLux.DomeLight)] for light_prim in light_prims: if light_prim.IsA(UsdLux.DistantLight): # Randomize intensity intensity_range = self.randomization_params['lighting']['intensity_range'] new_intensity = np.random.uniform(*intensity_range) light_api = UsdLux.DistantLight(light_prim) light_api.GetIntensityAttr().Set(new_intensity) def randomize_textures(self): &quot;&quot;&quot;Randomize material properties&quot;&quot;&quot; # Implementation would randomize material properties # such as roughness, metallic, and color values pass def randomize_object_poses(self): &quot;&quot;&quot;Randomize object positions and orientations&quot;&quot;&quot; # Get all dynamic objects in the scene # Add small random perturbations to their poses pass def save_sample(self, sample_id, rgb_image, depth_map, seg_map, ground_truth): &quot;&quot;&quot;Save a single data sample&quot;&quot;&quot; # Save RGB image rgb_pil = Image.fromarray(rgb_image) rgb_pil.save(f&quot;{self.output_dir}/images/{sample_id}.png&quot;) # Save depth map depth_normalized = ((depth_map - depth_map.min()) / (depth_map.max() - depth_map.min()) * 255).astype(np.uint8) depth_pil = Image.fromarray(depth_normalized) depth_pil.save(f&quot;{self.output_dir}/depth/{sample_id}.png&quot;) # Save segmentation map seg_normalized = ((seg_map - seg_map.min()) / (seg_map.max() - seg_map.min()) * 255).astype(np.uint8) seg_pil = Image.fromarray(seg_normalized) seg_pil.save(f&quot;{self.output_dir}/seg/{sample_id}.png&quot;) # Save raw data as numpy arrays for more precise processing np.save(f&quot;{self.output_dir}/depth/{sample_id}_raw.npy&quot;, depth_map) np.save(f&quot;{self.output_dir}/seg/{sample_id}_raw.npy&quot;, seg_map) def create_annotation(self, sample_id, ground_truth): &quot;&quot;&quot;Create annotation for a sample&quot;&quot;&quot; annotation = { 'id': sample_id, 'file_name': f&quot;{sample_id}.png&quot;, 'width': 640, # Assuming 640x480 resolution 'height': 480, 'annotations': ground_truth } return annotation def save_annotations(self, batch_data, format_type='coco'): &quot;&quot;&quot;Save annotations in specified format&quot;&quot;&quot; if format_type in self.annotation_formats: self.annotation_formats[format_type](batch_data) else: raise ValueError(f&quot;Unknown annotation format: {format_type}&quot;) def save_as_coco(self, batch_data): &quot;&quot;&quot;Save annotations in COCO format&quot;&quot;&quot; coco_format = { 'info': { 'year': datetime.now().year, 'version': '1.0', 'description': 'Synthetic Humanoid Perception Dataset', 'contributor': 'Isaac Sim Synthetic Data Generator', 'date_created': datetime.now().isoformat() }, 'images': [], 'annotations': [], 'categories': [] } # Convert our data to COCO format for i, sample in enumerate(batch_data): coco_format['images'].append({ 'id': i, 'file_name': sample['file_name'], 'width': sample['width'], 'height': sample['height'], 'date_captured': datetime.now().isoformat() }) # Save to file with open(f&quot;{self.output_dir}/annotations.json&quot;, 'w') as f: json.dump(coco_format, f, indent=2) def save_as_yolo(self, batch_data): &quot;&quot;&quot;Save annotations in YOLO format&quot;&quot;&quot; # YOLO format implementation pass def save_as_kitti(self, batch_data): &quot;&quot;&quot;Save annotations in KITTI format&quot;&quot;&quot; # KITTI format implementation pass   ","version":"Next","tagName":"h3"},{"title":"Advanced Perception Pipelines​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#advanced-perception-pipelines","content":" ","version":"Next","tagName":"h2"},{"title":"Multi-Modal Perception​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#multi-modal-perception","content":" # multimodal_perception.py import numpy as np from scipy.spatial.transform import Rotation as R import cv2 class MultiModalPerception: def __init__(self, camera_intrinsics, robot_state): self.camera_intrinsics = camera_intrinsics self.robot_state = robot_state # Initialize different perception modules self.vision_module = VisionPerception() self.depth_module = DepthPerception() self.fusion_module = SensorFusion() # Object detection and tracking self.object_detector = ObjectDetectionSystem() self.object_tracker = MultiObjectTracker() def process_multimodal_input(self, rgb_image, depth_map, imu_data, joint_states): &quot;&quot;&quot;Process multimodal sensor input&quot;&quot;&quot; # Process visual information visual_features = self.vision_module.extract_features(rgb_image) object_detections = self.object_detector.detect_objects(rgb_image) # Process depth information point_cloud = self.depth_module.generate_point_cloud(rgb_image, depth_map) surface_normals = self.depth_module.estimate_surface_normals(point_cloud) # Fuse sensor information fused_state = self.fusion_module.fuse_sensors( visual_features, point_cloud, imu_data, joint_states ) # Update object tracking tracked_objects = self.object_tracker.update_tracks( object_detections, fused_state ) return { 'fused_state': fused_state, 'tracked_objects': tracked_objects, 'point_cloud': point_cloud, 'surface_normals': surface_normals } class VisionPerception: def __init__(self): # Initialize vision models self.feature_extractor = self.load_feature_extractor() self.classifier = self.load_classifier() def extract_features(self, image): &quot;&quot;&quot;Extract visual features from image&quot;&quot;&quot; # This would typically use a CNN or other deep learning model features = np.random.random((256,)) # Placeholder return features def load_feature_extractor(self): &quot;&quot;&quot;Load pre-trained feature extraction model&quot;&quot;&quot; # Implementation would load a pre-trained model return None def load_classifier(self): &quot;&quot;&quot;Load object classification model&quot;&quot;&quot; # Implementation would load a classification model return None class DepthPerception: def __init__(self): self.camera_matrix = None def generate_point_cloud(self, rgb_image, depth_map): &quot;&quot;&quot;Generate 3D point cloud from RGB-D data&quot;&quot;&quot; height, width = depth_map.shape camera_matrix = self.camera_matrix # Create coordinate grids x, y = np.meshgrid(np.arange(width), np.arange(height)) # Convert to 3D coordinates x_3d = (x - camera_matrix[0, 2]) * depth_map / camera_matrix[0, 0] y_3d = (y - camera_matrix[1, 2]) * depth_map / camera_matrix[1, 1] # Stack into point cloud point_cloud = np.stack([x_3d, y_3d, depth_map], axis=-1) return point_cloud def estimate_surface_normals(self, point_cloud): &quot;&quot;&quot;Estimate surface normals from point cloud&quot;&quot;&quot; # Simple normal estimation using neighboring points normals = np.zeros_like(point_cloud) # Implementation would estimate normals using point cloud processing # This is a simplified version for i in range(1, point_cloud.shape[0]-1): for j in range(1, point_cloud.shape[1]-1): # Get neighboring points p_center = point_cloud[i, j] p_right = point_cloud[i, j+1] p_down = point_cloud[i+1, j] # Compute normal using cross product v1 = p_right - p_center v2 = p_down - p_center normal = np.cross(v1, v2) normal = normal / (np.linalg.norm(normal) + 1e-8) # Normalize normals[i, j] = normal return normals class SensorFusion: def __init__(self): # Initialize fusion algorithms self.kalman_filter = self.initialize_kalman_filter() def initialize_kalman_filter(self): &quot;&quot;&quot;Initialize Kalman filter for sensor fusion&quot;&quot;&quot; # Implementation would create a Kalman filter return None def fuse_sensors(self, visual_features, point_cloud, imu_data, joint_states): &quot;&quot;&quot;Fuse information from multiple sensors&quot;&quot;&quot; # Combine all sensor data into a unified state estimate fused_state = { 'position': self.estimate_position(imu_data, joint_states), 'orientation': self.estimate_orientation(imu_data), 'velocity': self.estimate_velocity(imu_data), 'environment_map': self.build_environment_map(point_cloud), 'object_map': self.build_object_map(visual_features) } return fused_state def estimate_position(self, imu_data, joint_states): &quot;&quot;&quot;Estimate robot position using IMU and joint data&quot;&quot;&quot; # Integrate IMU acceleration data # Use forward kinematics from joint states position = np.array([0.0, 0.0, 0.0]) # Placeholder return position def estimate_orientation(self, imu_data): &quot;&quot;&quot;Estimate robot orientation using IMU&quot;&quot;&quot; # Integrate gyroscope data, correct with accelerometer orientation = np.array([0.0, 0.0, 0.0, 1.0]) # Quaternion return orientation def estimate_velocity(self, imu_data): &quot;&quot;&quot;Estimate robot velocity using IMU&quot;&quot;&quot; velocity = np.array([0.0, 0.0, 0.0]) # Placeholder return velocity def build_environment_map(self, point_cloud): &quot;&quot;&quot;Build environment map from point cloud&quot;&quot;&quot; # Implementation would create a 3D map return {} def build_object_map(self, visual_features): &quot;&quot;&quot;Build object map from visual features&quot;&quot;&quot; # Implementation would identify and map objects return {} class ObjectDetectionSystem: def __init__(self): self.detection_model = self.load_detection_model() self.confidence_threshold = 0.5 def load_detection_model(self): &quot;&quot;&quot;Load object detection model&quot;&quot;&quot; # Implementation would load a pre-trained model return None def detect_objects(self, image): &quot;&quot;&quot;Detect objects in image&quot;&quot;&quot; # This would run the detection model # Return bounding boxes, class labels, and confidence scores detections = [ { 'bbox': [x, y, w, h], 'class': 'object_class', 'confidence': 0.8, 'mask': None # Segmentation mask if available } for x, y, w, h in [(50, 50, 100, 100), (200, 150, 80, 80)] # Example detections ] return detections class MultiObjectTracker: def __init__(self): self.trackers = {} self.next_id = 0 def update_tracks(self, detections, state): &quot;&quot;&quot;Update object tracks with new detections&quot;&quot;&quot; # Implementation would use data association and tracking algorithms # such as SORT, Deep SORT, or other multi-object tracking methods tracked_objects = [] for detection in detections: # Simple assignment based on overlap assigned = False for track_id, track in self.trackers.items(): if self.bbox_overlap(track['bbox'], detection['bbox']) &gt; 0.3: # Update existing track track['bbox'] = detection['bbox'] track['class'] = detection['class'] track['confidence'] = detection['confidence'] tracked_objects.append({ 'id': track_id, 'bbox': track['bbox'], 'class': track['class'], 'confidence': track['confidence'] }) assigned = True break if not assigned: # Create new track new_id = self.next_id self.trackers[new_id] = { 'bbox': detection['bbox'], 'class': detection['class'], 'confidence': detection['confidence'], 'age': 0 } tracked_objects.append({ 'id': new_id, 'bbox': detection['bbox'], 'class': detection['class'], 'confidence': detection['confidence'] }) self.next_id += 1 return tracked_objects def bbox_overlap(self, bbox1, bbox2): &quot;&quot;&quot;Calculate overlap between two bounding boxes&quot;&quot;&quot; x1, y1, w1, h1 = bbox1 x2, y2, w2, h2 = bbox2 # Calculate intersection left = max(x1, x2) top = max(y1, y2) right = min(x1 + w1, x2 + w2) bottom = min(y1 + h1, y2 + h2) if right &lt; left or bottom &lt; top: return 0.0 intersection = (right - left) * (bottom - top) area1 = w1 * h1 area2 = w2 * h2 union = area1 + area2 - intersection return intersection / union if union &gt; 0 else 0.0   ","version":"Next","tagName":"h3"},{"title":"Humanoid-Specific Perception Tasks​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#humanoid-specific-perception-tasks","content":" ","version":"Next","tagName":"h2"},{"title":"Manipulation Perception​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#manipulation-perception","content":" # manipulation_perception.py import numpy as np from scipy.spatial.transform import Rotation as R class ManipulationPerception: def __init__(self, robot_config): self.robot_config = robot_config self.gripper_camera = None self.arm_joints = robot_config.get_arm_joints() def perceive_graspable_objects(self, scene_rgb, scene_depth): &quot;&quot;&quot;Identify objects that can be grasped by the humanoid&quot;&quot;&quot; # Detect objects in the scene detections = self.detect_objects(scene_rgb) # Filter for graspable objects graspable_objects = [] for detection in detections: if self.is_graspable(detection, scene_depth): grasp_poses = self.compute_grasp_poses(detection, scene_depth) detection['grasp_poses'] = grasp_poses graspable_objects.append(detection) return graspable_objects def detect_objects(self, rgb_image): &quot;&quot;&quot;Detect objects using perception system&quot;&quot;&quot; # Use the multi-modal perception system detector = ObjectDetectionSystem() return detector.detect_objects(rgb_image) def is_graspable(self, object_detection, depth_map): &quot;&quot;&quot;Determine if an object is graspable&quot;&quot;&quot; # Check if object is within reach object_center_3d = self.project_to_3d( object_detection['bbox'], depth_map ) # Check if object is reachable by the arm if self.is_reachable(object_center_3d): # Check object size is appropriate for hand bbox = object_detection['bbox'] width, height = bbox[2], bbox[3] object_size = np.sqrt(width**2 + height**2) if 20 &lt; object_size &lt; 200: # Reasonable size range return True return False def compute_grasp_poses(self, object_detection, depth_map): &quot;&quot;&quot;Compute potential grasp poses for an object&quot;&quot;&quot; # Get 3D position of object bbox = object_detection['bbox'] x, y, w, h = bbox # Sample multiple points on the object grasp_candidates = [] # Center grasp center_3d = self.project_to_3d( (x + w/2, y + h/2, w, h), depth_map ) # Generate grasp poses around the object for angle in np.linspace(0, 2*np.pi, 8): offset_x = 0.05 * np.cos(angle) # 5cm offset offset_y = 0.05 * np.sin(angle) grasp_pose = { 'position': center_3d + np.array([offset_x, offset_y, 0]), 'orientation': self.compute_preferred_orientation( center_3d, angle ), 'approach_direction': np.array([0, 0, -1]), # From above 'grasp_type': 'top_grasp' } grasp_candidates.append(grasp_pose) return grasp_candidates def project_to_3d(self, bbox, depth_map): &quot;&quot;&quot;Project 2D bounding box center to 3D&quot;&quot;&quot; x, y, w, h = bbox center_x, center_y = int(x + w/2), int(y + h/2) # Get depth at center depth = depth_map[center_y, center_x] # Project to 3D using camera intrinsics # This is a simplified version - would need actual camera matrix fx, fy = 616.171, 616.171 # Typical for 640x480 camera cx, cy = 319.5, 239.5 x_3d = (center_x - cx) * depth / fx y_3d = (center_y - cy) * depth / fy z_3d = depth return np.array([x_3d, y_3d, z_3d]) def compute_preferred_orientation(self, object_pos, approach_angle): &quot;&quot;&quot;Compute preferred grasp orientation&quot;&quot;&quot; # For simple objects, approach from the top or side # depending on object shape and orientation # Default orientation - gripper pointing down rotation = R.from_euler('xyz', [0, 0, approach_angle]).as_quat() return rotation def is_reachable(self, object_position): &quot;&quot;&quot;Check if object is within robot's reach&quot;&quot;&quot; # Get current end-effector position current_ee_pos = self.get_end_effector_position() # Calculate distance distance = np.linalg.norm(object_position - current_ee_pos) # Check if within reach (simplified - would need proper IK) max_reach = self.robot_config.get_max_arm_reach() return distance &lt; max_reach def get_end_effector_position(self): &quot;&quot;&quot;Get current end-effector position from robot state&quot;&quot;&quot; # This would interface with the robot's forward kinematics return np.array([0.5, 0.0, 1.0]) # Placeholder class NavigationPerception: def __init__(self, robot_config): self.robot_config = robot_config self.navigation_map = None def perceive_navigable_environment(self, rgb_image, depth_map): &quot;&quot;&quot;Perceive the navigable environment for humanoid navigation&quot;&quot;&quot; # Create traversability map traversability_map = self.create_traversability_map(depth_map) # Detect obstacles obstacles = self.detect_obstacles(rgb_image, depth_map) # Detect walkable surfaces walkable_surfaces = self.detect_walkable_surfaces(depth_map) # Detect stairs, ramps, and other navigation challenges navigation_features = self.detect_navigation_features(rgb_image, depth_map) return { 'traversability_map': traversability_map, 'obstacles': obstacles, 'walkable_surfaces': walkable_surfaces, 'navigation_features': navigation_features } def create_traversability_map(self, depth_map): &quot;&quot;&quot;Create a traversability map from depth data&quot;&quot;&quot; # Analyze surface normals to determine walkability height, width = depth_map.shape traversability = np.ones((height, width)) # 1.0 = traversable, 0.0 = not traversable # Calculate surface normals for i in range(1, height-1): for j in range(1, width-1): # Simple normal estimation dz_dx = (depth_map[i, j+1] - depth_map[i, j-1]) / 2 dz_dy = (depth_map[i+1, j] - depth_map[i-1, j]) / 2 normal = np.array([-dz_dx, -dz_dy, 1.0]) normal = normal / np.linalg.norm(normal) # Check if surface is too steep (humanoid can handle ~30 degrees) angle_with_vertical = np.arccos(np.abs(normal[2])) max_walkable_angle = np.radians(30) # 30 degrees if angle_with_vertical &gt; max_walkable_angle: traversability[i, j] = 0.0 # Too steep return traversability def detect_obstacles(self, rgb_image, depth_map): &quot;&quot;&quot;Detect obstacles in the environment&quot;&quot;&quot; # Combine visual and depth information obstacles = [] # Use depth to find obstacles # Objects closer than robot height are potential obstacles robot_height = self.robot_config.get_height() obstacle_mask = depth_map &lt; robot_height # Find connected components as individual obstacles from scipy import ndimage labeled_obstacles, num_obstacles = ndimage.label(obstacle_mask) for i in range(1, num_obstacles + 1): # Get bounding box of each obstacle obstacle_pixels = np.where(labeled_obstacles == i) if len(obstacle_pixels[0]) &gt; 10: # Only consider substantial obstacles min_row, max_row = obstacle_pixels[0].min(), obstacle_pixels[0].max() min_col, max_col = obstacle_pixels[1].min(), obstacle_pixels[1].max() obstacle = { 'bbox': [min_col, min_row, max_col - min_col, max_row - min_row], 'center_3d': self.project_to_3d( [min_col + (max_col - min_col)/2, min_row + (max_row - min_row)/2, 0, 0], depth_map ), 'type': 'obstacle' } obstacles.append(obstacle) return obstacles def detect_walkable_surfaces(self, depth_map): &quot;&quot;&quot;Detect walkable surfaces&quot;&quot;&quot; # Surfaces that are flat and at appropriate height walkable_surfaces = [] # Look for surfaces at foot level (0.1m above ground) ground_level = depth_map.min() foot_level = ground_level + 0.1 # Find surfaces within a range of foot level surface_mask = np.abs(depth_map - foot_level) &lt; 0.05 # 5cm tolerance # Find connected components as individual surfaces from scipy import ndimage labeled_surfaces, num_surfaces = ndimage.label(surface_mask) for i in range(1, num_surfaces + 1): surface_pixels = np.where(labeled_surfaces == i) if len(surface_pixels[0]) &gt; 100: # Only consider substantial surfaces min_row, max_row = surface_pixels[0].min(), surface_pixels[0].max() min_col, max_col = surface_pixels[1].min(), surface_pixels[1].max() surface = { 'bbox': [min_col, min_row, max_col - min_col, max_row - min_row], 'center_3d': self.project_to_3d( [min_col + (max_col - min_col)/2, min_row + (max_row - min_row)/2, 0, 0], depth_map ), 'type': 'walkable_surface' } walkable_surfaces.append(surface) return walkable_surfaces def detect_navigation_features(self, rgb_image, depth_map): &quot;&quot;&quot;Detect special navigation features like stairs, doors, etc.&quot;&quot;&quot; navigation_features = { 'stairs': [], 'doors': [], 'ramps': [], 'elevators': [], 'narrow_passages': [] } # This would use specialized detection algorithms # For now, we'll simulate detection of some features # Detect stairs based on depth discontinuities stairs = self.detect_stairs(depth_map) navigation_features['stairs'] = stairs # Detect doors based on visual patterns (simplified) doors = self.detect_doors(rgb_image) navigation_features['doors'] = doors return navigation_features def detect_stairs(self, depth_map): &quot;&quot;&quot;Detect stairs in depth map&quot;&quot;&quot; stairs = [] # Look for regular step patterns in depth # This is a simplified approach height, width = depth_map.shape # Look for horizontal bands of consistent depth (steps) for row in range(0, height, 20): # Check every 20 pixels row_depths = depth_map[row:row+20, :] unique_depths = np.unique(row_depths) # Look for discrete depth levels that might indicate steps if len(unique_depths) &gt; 2: # Multiple depth levels # Check if they form a regular pattern depth_diffs = np.diff(np.sort(unique_depths)) # If differences are roughly consistent, might be stairs if np.std(depth_diffs) &lt; np.mean(depth_diffs) * 0.3: stair_region = { 'bbox': [0, row, width, 20], 'depth_levels': unique_depths.tolist(), 'type': 'stairs' } stairs.append(stair_region) return stairs def detect_doors(self, rgb_image): &quot;&quot;&quot;Detect doors in RGB image&quot;&quot;&quot; doors = [] # Use color and shape cues to detect doors # This is a simplified approach using color segmentation # Convert to HSV for better color detection hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV) # Look for common door colors (brown, white, metal-like) # Brown door detection lower_brown = np.array([10, 50, 50]) upper_brown = np.array([30, 255, 255]) brown_mask = cv2.inRange(hsv, lower_brown, upper_brown) # White door detection lower_white = np.array([0, 0, 200]) upper_white = np.array([180, 50, 255]) white_mask = cv2.inRange(hsv, lower_white, upper_white) combined_mask = cv2.bitwise_or(brown_mask, white_mask) # Find contours that could be doors from scipy import ndimage labeled_doors, num_doors = ndimage.label(combined_mask &gt; 0) for i in range(1, num_doors + 1): door_pixels = np.where(labeled_doors == i) if len(door_pixels[0]) &gt; 500: # Only consider substantial regions min_row, max_row = door_pixels[0].min(), door_pixels[0].max() min_col, max_col = door_pixels[1].min(), door_pixels[1].max() # Check aspect ratio (doors are typically taller than wide) height, width = max_row - min_row, max_col - min_col aspect_ratio = height / width if width &gt; 0 else 0 if 1.5 &lt; aspect_ratio &lt; 4.0: # Reasonable door aspect ratio door = { 'bbox': [min_col, min_row, width, height], 'aspect_ratio': aspect_ratio, 'type': 'door' } doors.append(door) return doors   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Sim-to-Real Rigor (Principle III)​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#sim-to-real-rigor-principle-iii","content":" Synthetic data generation with domain randomization for sim-to-real transferRealistic sensor simulation matching hardware specificationsPerception pipelines designed for real-world deployment  ","version":"Next","tagName":"h3"},{"title":"VLA Convergence Mandate (Principle I)​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#vla-convergence-mandate-principle-i","content":" Vision system integration with the VLA pipelineSemantic segmentation and object detection for scene understandingMulti-modal perception for action planning  ","version":"Next","tagName":"h3"},{"title":"Visualization Requirements (Key Standard II)​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#visualization-requirements-key-standard-ii","content":" Use of Mermaid diagrams for perception pipeline architectureProper code formatting and documentation standardsClear examples for complex perception systems  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#target-hardware-optimization","content":" Efficient perception algorithms suitable for Jetson Orin deploymentOptimized data processing pipelines for embedded systemsPerformance considerations for real-time operation  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Synthetic Dataset for Object Detection​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#example-1-synthetic-dataset-for-object-detection","content":" def create_object_detection_dataset(): &quot;&quot;&quot;Create a synthetic dataset for training object detection models&quot;&quot;&quot; # Initialize the synthetic data generator generator = SyntheticDataGenerator(output_dir=&quot;datasets/object_detection&quot;) # Set up a scene with common household objects setup_household_scene() # Configure domain randomization for robust training generator.randomization_params = { 'lighting': { 'intensity_range': (0.3, 2.5), 'color_temperature_range': (3000, 8000) }, 'textures': { 'roughness_range': (0.1, 0.9), 'metallic_range': (0.0, 0.3) }, 'object_poses': { 'position_jitter': 0.15, 'rotation_jitter': 0.2 } } # Collect data for different object categories object_categories = [ &quot;cup&quot;, &quot;bottle&quot;, &quot;box&quot;, &quot;phone&quot;, &quot;book&quot;, &quot;fruit&quot;, &quot;toy&quot;, &quot;tool&quot;, &quot;container&quot; ] for category in object_categories: print(f&quot;Collecting data for {category}...&quot;) # Place objects of this category in scene place_category_objects(category) # Collect 500 samples for this category generator.collect_data_batch( camera=get_active_camera(), num_samples=500, annotation_format='coco' ) print(&quot;Object detection dataset creation completed!&quot;) def setup_household_scene(): &quot;&quot;&quot;Set up a household environment for data collection&quot;&quot;&quot; # Implementation would create a kitchen/living room scene # with appropriate furniture and surfaces pass def place_category_objects(category): &quot;&quot;&quot;Place objects of a specific category in the scene&quot;&quot;&quot; # Implementation would place objects of the specified category # in various positions and orientations pass def get_active_camera(): &quot;&quot;&quot;Get the active camera for data collection&quot;&quot;&quot; # Implementation would return the configured camera pass   ","version":"Next","tagName":"h3"},{"title":"Example 2: Humanoid Manipulation Perception Pipeline​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#example-2-humanoid-manipulation-perception-pipeline","content":" def setup_manipulation_perception_pipeline(): &quot;&quot;&quot;Set up perception pipeline for humanoid manipulation tasks&quot;&quot;&quot; # Initialize robot configuration robot_config = { 'arm_joints': ['shoulder', 'elbow', 'wrist'], 'hand_type': 'anthropomorphic', 'max_arm_reach': 1.2, # meters 'gripper_aperture': 0.1 # meters } # Initialize perception modules manipulation_perceptor = ManipulationPerception(robot_config) vision_system = MultiModalPerception( camera_intrinsics=get_camera_intrinsics(), robot_state=get_robot_state() ) def perception_callback(rgb_image, depth_map, imu_data, joint_states): &quot;&quot;&quot;Callback function for perception processing&quot;&quot;&quot; # Process multimodal input multimodal_output = vision_system.process_multimodal_input( rgb_image, depth_map, imu_data, joint_states ) # Identify graspable objects graspable_objects = manipulation_perceptor.perceive_graspable_objects( rgb_image, depth_map ) # Update manipulation planning update_manipulation_planner(graspable_objects, multimodal_output) return { 'graspable_objects': graspable_objects, 'environment_state': multimodal_output } return perception_callback def update_manipulation_planner(objects, state): &quot;&quot;&quot;Update manipulation planner with perception results&quot;&quot;&quot; # This would interface with the manipulation planning system # to select appropriate grasp poses and motion plans pass def get_camera_intrinsics(): &quot;&quot;&quot;Get camera intrinsic parameters&quot;&quot;&quot; # Return camera matrix and distortion coefficients camera_matrix = np.array([ [616.171, 0, 319.5], [0, 616.171, 239.5], [0, 0, 1] ]) return camera_matrix def get_robot_state(): &quot;&quot;&quot;Get current robot state&quot;&quot;&quot; # Return current joint states, end-effector pose, etc. return {}   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Perception Pipeline Implementation​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#exercise-1-perception-pipeline-implementation","content":" Implement a complete perception pipeline that:  Integrates RGB and depth data for 3D object detectionUses semantic segmentation for scene understandingImplements multi-object tracking for dynamic environmentsProvides outputs suitable for humanoid manipulation planning  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Synthetic Data Generation​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#exercise-2-synthetic-data-generation","content":" Create a synthetic dataset generator that:  Implements domain randomization for robust trainingGenerates multiple annotation formats (COCO, YOLO, KITTI)Creates diverse scenarios for humanoid navigationOptimizes for sim-to-real transfer learning  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Multi-Modal Fusion​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#exercise-3-multi-modal-fusion","content":" Develop a sensor fusion system that:  Combines visual, depth, and IMU dataImplements Kalman filtering for state estimationProvides robust perception in challenging conditionsIntegrates with the VLA pipeline for action planning  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#summary","content":" The Isaac SDK provides powerful tools for developing perception systems and generating synthetic data essential for humanoid robotics. The synthetic data generation capabilities, combined with realistic sensor simulation and domain randomization, enable the creation of robust perception systems that can operate effectively in real-world environments. Understanding these tools is crucial for implementing the Vision component of the Vision-Language-Action pipeline and achieving effective sim-to-real transfer in humanoid robot applications.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","url":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk#further-reading","content":" &quot;Isaac Sim Synthetic Data Generation Guide&quot; - NVIDIA Developer documentation&quot;Computer Vision for Robotics&quot; by Jähne and Scharr&quot;Robotics, Vision and Control&quot; by Peter Corke (Perception chapter)&quot;Multiple View Geometry in Computer Vision&quot; by Hartley and Zisserman&quot;Deep Learning for Perception&quot; - Recent research papers and tutorials ","version":"Next","tagName":"h2"},{"title":"Chapter 14 - Computer Vision for Robotics","type":0,"sectionRef":"#","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#learning-objectives","content":" Implement computer vision techniques for roboticsApply vision algorithms to humanoid robot perceptionIntegrate vision with action planning systems  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#introduction","content":" Computer vision forms the foundation of the Vision component in the Vision-Language-Action (VLA) pipeline, which is central to our project's VLA Convergence Mandate principle. For humanoid robots operating in human-centered environments, computer vision systems must be robust, real-time capable, and specifically adapted to the unique challenges of humanoid perception. This chapter explores computer vision techniques tailored for robotics applications, with special emphasis on humanoid robot perception systems that must operate under the constraints of embedded hardware like the NVIDIA Jetson Orin Nano while maintaining the real-time performance requirements for safety and stability.  ","version":"Next","tagName":"h2"},{"title":"Robotics-Specific Computer Vision Challenges​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#robotics-specific-computer-vision-challenges","content":" ","version":"Next","tagName":"h2"},{"title":"Motion and Dynamics​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#motion-and-dynamics","content":" Unlike traditional computer vision applications, robotics vision systems must operate under continuous motion:  Ego-motion compensation: The robot's own movement affects visual inputTemporal consistency: Maintaining consistent object tracking during robot motionMotion blur: Fast robot movements can cause image blurRolling shutter effects: Common in robot-mounted cameras  ","version":"Next","tagName":"h3"},{"title":"Real-Time Constraints​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#real-time-constraints","content":" Robotics vision systems have strict timing requirements:  Control loop frequencies: Vision processing must align with control frequencies (often 30-100Hz)Predictable latency: Deterministic processing times for safe operationResource efficiency: Optimized for embedded hardware constraints  ","version":"Next","tagName":"h3"},{"title":"Physical Interaction Context​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#physical-interaction-context","content":" Robotics vision is always in service of physical interaction:  Action-oriented perception: Vision output directly drives motor actions3D understanding: Depth and spatial relationships are criticalManipulation planning: Need to understand graspable surfaces and object properties  ","version":"Next","tagName":"h3"},{"title":"Essential Vision Techniques for Robotics​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#essential-vision-techniques-for-robotics","content":" ","version":"Next","tagName":"h2"},{"title":"Feature Detection and Matching​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#feature-detection-and-matching","content":" For humanoid robots navigating and interacting in human environments, robust feature detection is essential:  import cv2 import numpy as np from typing import List, Tuple, Optional class RobotFeatureDetector: def __init__(self, detector_type: str = &quot;orb&quot;, matching_threshold: float = 0.75): &quot;&quot;&quot; Initialize feature detector for robotic applications Args: detector_type: Type of detector ('orb', 'sift', 'akaze') matching_threshold: Threshold for good matches &quot;&quot;&quot; self.detector_type = detector_type self.matching_threshold = matching_threshold # Initialize detector based on type if detector_type == &quot;orb&quot;: self.detector = cv2.ORB_create( nfeatures=500, scaleFactor=1.2, nlevels=8, edgeThreshold=31, patchSize=31, fastThreshold=20 ) elif detector_type == &quot;sift&quot;: self.detector = cv2.SIFT_create( nfeatures=400, contrastThreshold=0.04, edgeThreshold=10, sigma=1.6 ) elif detector_type == &quot;akaze&quot;: self.detector = cv2.AKAZE_create() # Initialize matcher self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING if detector_type == &quot;orb&quot; else cv2.NORM_L2, crossCheck=False) def detect_and_compute(self, image: np.ndarray) -&gt; Tuple[List[cv2.KeyPoint], np.ndarray]: &quot;&quot;&quot; Detect features and compute descriptors Args: image: Input image (grayscale recommended) Returns: Tuple of (keypoints, descriptors) &quot;&quot;&quot; if len(image.shape) == 3: gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) else: gray = image keypoints, descriptors = self.detector.detectAndCompute(gray, None) if descriptors is None: return [], np.array([]) return keypoints, descriptors def match_features(self, desc1: np.ndarray, desc2: np.ndarray) -&gt; List[cv2.DMatch]: &quot;&quot;&quot; Match features between two descriptor sets Args: desc1: Descriptors from first image desc2: Descriptors from second image Returns: List of good matches &quot;&quot;&quot; if desc1.size == 0 or desc2.size == 0: return [] matches = self.matcher.knnMatch(desc1, desc2, k=2) # Apply Lowe's ratio test good_matches = [] for match_pair in matches: if len(match_pair) == 2: m, n = match_pair if m.distance &lt; self.matching_threshold * n.distance: good_matches.append(m) return good_matches def estimate_motion(self, prev_image: np.ndarray, curr_image: np.ndarray) -&gt; Optional[np.ndarray]: &quot;&quot;&quot; Estimate ego-motion between two images Args: prev_image: Previous image frame curr_image: Current image frame Returns: 3x3 transformation matrix or None if insufficient matches &quot;&quot;&quot; # Detect features in both images prev_kp, prev_desc = self.detect_and_compute(prev_image) curr_kp, curr_desc = self.detect_and_compute(curr_image) if prev_desc is None or curr_desc is None: return None # Match features matches = self.match_features(prev_desc, curr_desc) if len(matches) &lt; 10: # Need minimum matches for reliable estimation return None # Get corresponding points prev_points = np.float32([prev_kp[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2) curr_points = np.float32([curr_kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2) # Estimate transformation (try homography first, then fundamental matrix) transformation, mask = cv2.findHomography( prev_points, curr_points, cv2.RANSAC, ransacReprojThreshold=5.0 ) return transformation class VisualOdometry: def __init__(self, feature_detector: RobotFeatureDetector): self.feature_detector = feature_detector self.prev_image = None self.accumulated_transform = np.eye(3) self.position = np.array([0.0, 0.0, 0.0]) # x, y, theta def process_frame(self, image: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]: &quot;&quot;&quot; Process a frame and update position estimate Args: image: Current camera frame Returns: Tuple of (current_position, transformation_matrix) &quot;&quot;&quot; if self.prev_image is None: self.prev_image = image.copy() return self.position, np.eye(3) # Estimate motion transform = self.feature_detector.estimate_motion(self.prev_image, image) if transform is not None: # Update accumulated transformation self.accumulated_transform = self.accumulated_transform @ transform # Extract position from transformation dx = transform[0, 2] dy = transform[1, 2] dtheta = np.arctan2(transform[1, 0], transform[0, 0]) # Update position (simplified - assumes small rotations) self.position[0] += dx self.position[1] += dy self.position[2] += dtheta self.prev_image = image.copy() return self.position, self.accumulated_transform   ","version":"Next","tagName":"h3"},{"title":"Object Detection for Robotics​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#object-detection-for-robotics","content":" For humanoid robots, object detection must be adapted to the physical interaction context:  import torch import torchvision from torchvision import transforms from typing import Dict, List, Tuple import time class RoboticObjectDetector: def __init__(self, model_type: str = &quot;yolo&quot;, confidence_threshold: float = 0.5): &quot;&quot;&quot; Initialize object detector for robotic applications Args: model_type: Type of model ('yolo', 'faster_rcnn', 'ssd') confidence_threshold: Minimum confidence for detections &quot;&quot;&quot; self.model_type = model_type self.confidence_threshold = confidence_threshold self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Load pre-trained model if model_type == &quot;faster_rcnn&quot;: self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn( weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT ) elif model_type == &quot;ssd&quot;: self.model = torchvision.models.detection.ssd300_vgg16( weights=torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT ) elif model_type == &quot;yolo&quot;: # For YOLO, we'd typically use ultralytics or similar # Here we'll use a generic approach self.model = self._load_yolo_model() self.model.to(self.device) self.model.eval() # Preprocessing transforms self.transform = transforms.Compose([ transforms.ToTensor(), ]) # COCO class names (first 20 for common objects) self.coco_names = [ '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush' ] def _load_yolo_model(self): &quot;&quot;&quot;Load YOLO model (placeholder - would use actual YOLO implementation)&quot;&quot;&quot; # In practice, this would load a YOLO model # For now, we'll use Faster R-CNN as a substitute return torchvision.models.detection.fasterrcnn_resnet50_fpn( weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT ) def detect_objects(self, image: np.ndarray, return_time: bool = False) -&gt; Dict: &quot;&quot;&quot; Detect objects in image with timing information Args: image: Input image (H, W, C, RGB format) return_time: Whether to return processing time Returns: Dictionary with detections and metadata &quot;&quot;&quot; start_time = time.time() # Preprocess image if isinstance(image, np.ndarray): # Convert from numpy to tensor image_tensor = self.transform(image).unsqueeze(0) # Add batch dimension else: image_tensor = image image_tensor = image_tensor.to(self.device) # Run inference with torch.no_grad(): predictions = self.model(image_tensor) # Process predictions pred = predictions[0] # Get first (and only) image results # Filter by confidence scores = pred['scores'].cpu().numpy() keep_indices = scores &gt;= self.confidence_threshold filtered_boxes = pred['boxes'][keep_indices].cpu().numpy() filtered_labels = pred['labels'][keep_indices].cpu().numpy() filtered_scores = scores[keep_indices] # Convert to robot-friendly format detections = [] for i in range(len(filtered_boxes)): box = filtered_boxes[i] label = int(filtered_labels[i]) score = float(filtered_scores[i]) detection = { 'bbox': [float(x) for x in box], # [x1, y1, x2, y2] 'label': self.coco_names[label] if label &lt; len(self.coco_names) else f'unknown_{label}', 'confidence': score, 'class_id': label } detections.append(detection) end_time = time.time() processing_time = end_time - start_time if return_time else None result = { 'detections': detections, 'image_shape': image.shape if isinstance(image, np.ndarray) else image_tensor.shape[-2:], 'confidence_threshold': self.confidence_threshold } if return_time: result['processing_time'] = processing_time return result def get_robot_reachable_objects(self, image: np.ndarray, camera_intrinsics: np.ndarray, robot_position: np.ndarray, max_distance: float = 1.5) -&gt; List[Dict]: &quot;&quot;&quot; Get objects that are potentially reachable by the robot Args: image: Input image camera_intrinsics: 3x3 camera intrinsic matrix robot_position: Robot position in world coordinates [x, y, z] max_distance: Maximum reach distance in meters Returns: List of reachable objects with 3D positions &quot;&quot;&quot; # Get object detections detection_result = self.detect_objects(image) detections = detection_result['detections'] reachable_objects = [] for detection in detections: bbox = detection['bbox'] # Get 2D center of bounding box center_x = int((bbox[0] + bbox[2]) / 2) center_y = int((bbox[1] + bbox[3]) / 2) # Note: This is simplified - would need actual depth information # For now, we'll assume objects are at a known distance or use depth from other sensors # In practice, this would integrate with depth information object_3d = { **detection, 'pixel_coords': [center_x, center_y], 'estimated_distance': self._estimate_distance_from_size(detection, image.shape) } # Check if object is within reach if object_3d['estimated_distance'] &lt;= max_distance: reachable_objects.append(object_3d) return reachable_objects def _estimate_distance_from_size(self, detection: Dict, image_shape: Tuple) -&gt; float: &quot;&quot;&quot; Estimate distance based on object size in image This is a simplified approach - in practice, would use depth sensor data &quot;&quot;&quot; bbox = detection['bbox'] bbox_width = bbox[2] - bbox[0] bbox_height = bbox[3] - bbox[1] # Use a simple size-distance relationship (calibrated for common objects) # This is a placeholder - real implementation would use calibrated data image_width = image_shape[1] if len(image_shape) == 3 else image_shape[1] object_width_ratio = bbox_width / image_width # Simplified distance estimation (in meters) # In practice, this would use more sophisticated geometric relationships estimated_distance = 1.0 / (object_width_ratio + 0.01) # Add small value to avoid division by zero return min(estimated_distance, 5.0) # Cap at 5 meters   ","version":"Next","tagName":"h3"},{"title":"3D Vision and Depth Processing​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#3d-vision-and-depth-processing","content":" ","version":"Next","tagName":"h2"},{"title":"Depth-Based Perception for Humanoid Robots​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#depth-based-perception-for-humanoid-robots","content":" Humanoid robots require sophisticated 3D perception for navigation and manipulation:  import open3d as o3d import numpy as np from scipy.spatial.transform import Rotation as R from typing import List, Tuple, Optional class DepthProcessor: def __init__(self, camera_intrinsics: np.ndarray, voxel_size: float = 0.01): &quot;&quot;&quot; Initialize depth processor for 3D perception Args: camera_intrinsics: 3x3 camera intrinsic matrix voxel_size: Size of voxels for point cloud processing &quot;&quot;&quot; self.camera_intrinsics = camera_intrinsics self.voxel_size = voxel_size # Extract intrinsic parameters self.fx = camera_intrinsics[0, 0] self.fy = camera_intrinsics[1, 1] self.cx = camera_intrinsics[0, 2] self.cy = camera_intrinsics[1, 2] def depth_to_pointcloud(self, depth_image: np.ndarray, rgb_image: Optional[np.ndarray] = None) -&gt; o3d.geometry.PointCloud: &quot;&quot;&quot; Convert depth image to point cloud Args: depth_image: Depth image (H, W) in meters rgb_image: Optional RGB image for color information Returns: Open3D point cloud &quot;&quot;&quot; height, width = depth_image.shape # Create coordinate grids y, x = np.mgrid[0:height, 0:width] # Convert to 3D coordinates x_3d = (x - self.cx) * depth_image / self.fx y_3d = (y - self.cy) * depth_image / self.fy z_3d = depth_image # Stack into point cloud points = np.stack([x_3d, y_3d, z_3d], axis=-1).reshape(-1, 3) # Remove invalid points (where depth is 0 or invalid) valid_mask = (z_3d &gt; 0) &amp; (z_3d &lt; 10) # Valid range: 0 to 10 meters valid_points = points[valid_mask.flatten()] # Create point cloud pcd = o3d.geometry.PointCloud() pcd.points = o3d.utility.Vector3dVector(valid_points) # Add colors if RGB image provided if rgb_image is not None: # Reshape and filter colors to match valid points colors = rgb_image.reshape(-1, 3) / 255.0 # Normalize to [0,1] valid_colors = colors[valid_mask.flatten()] pcd.colors = o3d.utility.Vector3dVector(valid_colors) return pcd def segment_planes(self, pointcloud: o3d.geometry.PointCloud, distance_threshold: float = 0.01, ransac_n: int = 3, num_iterations: int = 1000) -&gt; Tuple[np.ndarray, o3d.geometry.PointCloud]: &quot;&quot;&quot; Segment planar surfaces (like floors, tables) from point cloud Args: pointcloud: Input point cloud distance_threshold: Maximum distance to plane ransac_n: Number of points for RANSAC num_iterations: Number of RANSAC iterations Returns: Tuple of (plane_model, inlier_cloud) &quot;&quot;&quot; # Downsample point cloud for efficiency downsampled = pointcloud.voxel_down_sample(voxel_size=self.voxel_size * 2) # Segment plane using RANSAC plane_model, inliers = downsampled.segment_plane( distance_threshold=distance_threshold, ransac_n=ransac_n, num_iterations=num_iterations ) # Extract inlier and outlier point clouds inlier_cloud = downsampled.select_by_index(inliers) outlier_cloud = downsampled.select_by_index(inliers, invert=True) return plane_model, inlier_cloud def find_objects_on_surface(self, pointcloud: o3d.geometry.PointCloud, surface_model: np.ndarray, min_distance: float = 0.02, max_distance: float = 0.5) -&gt; List[o3d.geometry.PointCloud]: &quot;&quot;&quot; Find objects positioned on a surface Args: pointcloud: Input point cloud (with surface points removed) surface_model: Plane model coefficients [a, b, c, d] for ax+by+cz+d=0 min_distance: Minimum distance above surface max_distance: Maximum distance above surface Returns: List of point clouds representing individual objects &quot;&quot;&quot; # Calculate distances from surface for all points points = np.asarray(pointcloud.points) # Plane equation: ax + by + cz + d = 0 a, b, c, d = surface_model distances = np.abs(a * points[:, 0] + b * points[:, 1] + c * points[:, 2] + d) / np.sqrt(a**2 + b**2 + c**2) # Filter points that are at the right height above the surface height_mask = (distances &gt;= min_distance) &amp; (distances &lt;= max_distance) object_points = points[height_mask] if len(object_points) == 0: return [] # Create new point cloud with object points object_pcd = o3d.geometry.PointCloud() object_pcd.points = o3d.utility.Vector3dVector(object_points) # Cluster the points to separate individual objects labels = np.array(object_pcd.cluster_dbscan(eps=self.voxel_size * 5, min_points=10, print_progress=False)) # Group points by cluster label clusters = [] for label in set(labels): if label == -1: # Skip noise points continue cluster_indices = np.where(labels == label)[0] cluster_pcd = object_pcd.select_by_index(cluster_indices) if len(cluster_pcd.points) &gt;= 10: # Only consider substantial clusters clusters.append(cluster_pcd) return clusters def compute_surface_normals(self, pointcloud: o3d.geometry.PointCloud, radius: float = 0.02) -&gt; np.ndarray: &quot;&quot;&quot; Compute surface normals for point cloud Args: pointcloud: Input point cloud radius: Radius for normal computation Returns: Array of surface normals &quot;&quot;&quot; # Estimate normals pointcloud.estimate_normals( search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=radius, max_nn=30) ) return np.asarray(pointcloud.normals) class ManipulationTargetDetector: def __init__(self, depth_processor: DepthProcessor): self.depth_processor = depth_processor def find_graspable_objects(self, depth_image: np.ndarray, rgb_image: np.ndarray, robot_workspace: np.ndarray) -&gt; List[Dict]: &quot;&quot;&quot; Find objects that can be grasped by the robot Args: depth_image: Depth image from robot camera rgb_image: RGB image for color information robot_workspace: 3D workspace bounds [min_x, min_y, min_z, max_x, max_y, max_z] Returns: List of graspable objects with properties &quot;&quot;&quot; # Convert to point cloud pcd = self.depth_processor.depth_to_pointcloud(depth_image, rgb_image) # Remove points outside robot workspace points = np.asarray(pcd.points) workspace_mask = ( (points[:, 0] &gt;= robot_workspace[0]) &amp; (points[:, 0] &lt;= robot_workspace[3]) &amp; (points[:, 1] &gt;= robot_workspace[1]) &amp; (points[:, 1] &lt;= robot_workspace[4]) &amp; (points[:, 2] &gt;= robot_workspace[2]) &amp; (points[:, 2] &lt;= robot_workspace[5]) ) workspace_pcd = pcd.select_by_index(np.where(workspace_mask)[0]) # Segment the floor plane floor_model, floor_points = self.depth_processor.segment_planes(workspace_pcd) # Remove floor points to focus on objects non_floor_pcd = workspace_pcd.select_by_index( np.setdiff1d(np.arange(len(workspace_pcd.points)), np.asarray(floor_points.points), assume_unique=True) ) # Find objects on surfaces objects = self.depth_processor.find_objects_on_surface(non_floor_pcd, floor_model) graspable_objects = [] for i, obj_pcd in enumerate(objects): # Compute object properties points = np.asarray(obj_pcd.points) # Calculate bounding box min_bound = points.min(axis=0) max_bound = points.max(axis=0) center = (min_bound + max_bound) / 2.0 size = max_bound - min_bound # Calculate object height above floor height_above_floor = center[2] - floor_model[3] / floor_model[2] if floor_model[2] != 0 else center[2] # Determine if object is graspable based on size object_volume = size[0] * size[1] * size[2] is_graspable = self._is_graspable(size, object_volume) if is_graspable: # Calculate approach directions based on object shape approach_directions = self._calculate_approach_directions(obj_pcd) graspable_object = { 'id': i, 'center': center.tolist(), 'size': size.tolist(), 'volume': float(object_volume), 'height_above_floor': float(height_above_floor), 'approach_directions': approach_directions, 'is_graspable': True, 'bbox': [min_bound.tolist(), max_bound.tolist()] } graspable_objects.append(graspable_object) return graspable_objects def _is_graspable(self, size: np.ndarray, volume: float) -&gt; bool: &quot;&quot;&quot; Determine if an object is graspable based on size and volume &quot;&quot;&quot; # Check size constraints (min 2cm, max 30cm in any dimension) min_size = 0.02 # 2cm max_size = 0.30 # 30cm if np.any(size &lt; min_size) or np.any(size &gt; max_size): return False # Check volume constraints (min 1cm³, max 5000cm³) min_volume = 1e-6 # 1cm³ in m³ max_volume = 5e-3 # 5000cm³ in m³ if volume &lt; min_volume or volume &gt; max_volume: return False # Check aspect ratio (not too flat or too thin) sorted_dims = np.sort(size) aspect_ratio = sorted_dims[2] / (sorted_dims[0] + 1e-6) # longest / shortest if aspect_ratio &gt; 10: # Too elongated return False return True def _calculate_approach_directions(self, object_pcd: o3d.geometry.PointCloud) -&gt; List[List[float]]: &quot;&quot;&quot; Calculate potential approach directions for grasping &quot;&quot;&quot; # Compute bounding box orientation bbox = object_pcd.get_oriented_bounding_box() # Get the rotation matrix rotation = np.asarray(bbox.R) # Calculate approach directions based on object orientation # Default approach: from above (z direction) approach_directions = [ [0, 0, -1], # From above [1, 0, 0], # From the side (positive x) [-1, 0, 0], # From the side (negative x) [0, 1, 0], # From the side (positive y) [0, -1, 0] # From the side (negative y) ] return approach_directions   ","version":"Next","tagName":"h3"},{"title":"Real-Time Performance Optimization​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#real-time-performance-optimization","content":" ","version":"Next","tagName":"h2"},{"title":"Efficient Vision Processing for Embedded Systems​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#efficient-vision-processing-for-embedded-systems","content":" For deployment on target hardware like the NVIDIA Jetson Orin Nano, optimization is crucial:  import threading import queue from collections import deque import time class OptimizedVisionPipeline: def __init__(self, max_queue_size: int = 3, target_fps: int = 30): &quot;&quot;&quot; Initialize optimized vision pipeline for embedded systems Args: max_queue_size: Maximum frames to queue target_fps: Target processing rate &quot;&quot;&quot; self.max_queue_size = max_queue_size self.target_fps = target_fps self.frame_interval = 1.0 / target_fps # Frame processing queue self.frame_queue = queue.Queue(maxsize=max_queue_size) self.result_queue = queue.Queue(maxsize=max_queue_size) # Processing history for performance monitoring self.processing_times = deque(maxlen=30) # Last 30 frames self.is_running = False # Initialize components self.feature_detector = RobotFeatureDetector() self.object_detector = RoboticObjectDetector() self.depth_processor = DepthProcessor( camera_intrinsics=np.array([[616.171, 0, 319.5], [0, 616.171, 239.5], [0, 0, 1]]) ) def start_processing(self): &quot;&quot;&quot;Start the processing thread&quot;&quot;&quot; self.is_running = True self.processing_thread = threading.Thread(target=self._processing_loop) self.processing_thread.start() def stop_processing(self): &quot;&quot;&quot;Stop the processing thread&quot;&quot;&quot; self.is_running = False if hasattr(self, 'processing_thread'): self.processing_thread.join() def submit_frame(self, image: np.ndarray, depth: Optional[np.ndarray] = None, metadata: Dict = None) -&gt; bool: &quot;&quot;&quot; Submit a frame for processing Args: image: Input image depth: Optional depth image metadata: Additional metadata Returns: True if frame was accepted, False if queue is full &quot;&quot;&quot; try: frame_data = { 'image': image, 'depth': depth, 'metadata': metadata or {}, 'timestamp': time.time() } self.frame_queue.put_nowait(frame_data) return True except queue.Full: return False # Queue is full, drop frame def get_results(self, timeout: float = 0.1) -&gt; Optional[Dict]: &quot;&quot;&quot; Get processing results Args: timeout: Maximum time to wait for results Returns: Processing results or None if no results available &quot;&quot;&quot; try: return self.result_queue.get_nowait() except queue.Empty: return None def _processing_loop(self): &quot;&quot;&quot;Main processing loop running in separate thread&quot;&quot;&quot; while self.is_running: try: # Get frame from queue frame_data = self.frame_queue.get(timeout=0.01) start_time = time.time() # Process the frame results = self._process_single_frame(frame_data) processing_time = time.time() - start_time self.processing_times.append(processing_time) # Add performance metrics results['performance'] = { 'processing_time': processing_time, 'average_processing_time': np.mean(self.processing_times), 'current_fps': 1.0 / processing_time if processing_time &gt; 0 else 0 } # Put results in output queue try: self.result_queue.put_nowait(results) except queue.Full: # Drop results if output queue is full pass except queue.Empty: continue # No frame to process, continue loop def _process_single_frame(self, frame_data: Dict) -&gt; Dict: &quot;&quot;&quot; Process a single frame with all vision components &quot;&quot;&quot; image = frame_data['image'] depth = frame_data.get('depth') metadata = frame_data['metadata'] results = { 'timestamp': frame_data['timestamp'], 'metadata': metadata, 'features': {}, 'objects': {}, 'depth_analysis': {}, 'robot_relevant': {} } # Feature detection keypoints, descriptors = self.feature_detector.detect_and_compute(image) results['features'] = { 'keypoints_count': len(keypoints), 'has_features': len(keypoints) &gt; 10 } # Object detection obj_detections = self.object_detector.detect_objects(image) results['objects'] = { 'detections': obj_detections['detections'], 'count': len(obj_detections['detections']) } # Depth analysis if available if depth is not None: # Convert depth to point cloud pcd = self.depth_processor.depth_to_pointcloud(depth) # Segment surfaces floor_model, floor_points = self.depth_processor.segment_planes(pcd) results['depth_analysis'] = { 'has_floor': floor_model is not None, 'floor_points_count': len(floor_points.points) if floor_points else 0 } # Find graspable objects if robot position is known if 'robot_position' in metadata and 'workspace_bounds' in metadata: robot_pos = np.array(metadata['robot_position']) workspace = np.array(metadata['workspace_bounds']) manip_detector = ManipulationTargetDetector(self.depth_processor) graspable = manip_detector.find_graspable_objects( depth, image, workspace ) results['robot_relevant']['graspable_objects'] = graspable return results def get_performance_stats(self) -&gt; Dict: &quot;&quot;&quot;Get performance statistics&quot;&quot;&quot; if not self.processing_times: return {'average_fps': 0, 'average_processing_time': 0} avg_processing_time = np.mean(self.processing_times) avg_fps = 1.0 / avg_processing_time if avg_processing_time &gt; 0 else 0 return { 'average_fps': avg_fps, 'average_processing_time': avg_processing_time, 'min_processing_time': min(self.processing_times) if self.processing_times else 0, 'max_processing_time': max(self.processing_times) if self.processing_times else 0, 'queue_size': self.frame_queue.qsize() }   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"VLA Convergence Mandate (Principle I)​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#vla-convergence-mandate-principle-i","content":" Vision algorithms specifically designed for the VLA pipelineIntegration with action planning systemsReal-time capable implementations for humanoid robots  ","version":"Next","tagName":"h3"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#real-time-validation-principle-iv","content":" Optimized algorithms suitable for real-time operation on Jetson OrinPerformance monitoring and optimization techniquesEfficient processing pipelines for control systems  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization (Constraint)​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#target-hardware-optimization-constraint","content":" Algorithms optimized for NVIDIA Jetson Orin Nano (8GB) platformEfficient memory usage and computational complexityReal-time performance on embedded systems  ","version":"Next","tagName":"h3"},{"title":"Sim-to-Real Rigor (Principle III)​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#sim-to-real-rigor-principle-iii","content":" Vision systems designed to work with both simulated and real sensorsRobust algorithms that handle sensor noise and uncertaintyValidation techniques for perception accuracy  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Humanoid Navigation Vision System​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#example-1-humanoid-navigation-vision-system","content":" class NavigationVisionSystem: def __init__(self): self.pipeline = OptimizedVisionPipeline(target_fps=20) # Lower FPS for navigation self.obstacle_detector = self._setup_obstacle_detection() self.path_planner = self._setup_path_planning() def _setup_obstacle_detection(self): &quot;&quot;&quot;Set up obstacle detection specifically for navigation&quot;&quot;&quot; # Use a lighter model for real-time navigation detector = RoboticObjectDetector( model_type=&quot;yolo&quot;, # Use YOLO for speed confidence_threshold=0.4 # Lower threshold for more detections ) return detector def _setup_path_planning(self): &quot;&quot;&quot;Set up path planning integration&quot;&quot;&quot; # Path planning will use vision output return PathPlanner() def process_navigation_frame(self, image: np.ndarray, depth: np.ndarray) -&gt; Dict: &quot;&quot;&quot;Process a frame for navigation purposes&quot;&quot;&quot; # Detect obstacles detections = self.obstacle_detector.detect_objects(image) # Analyze traversable areas using depth traversable_map = self._analyze_traversable_areas(depth) # Combine vision and depth information navigation_data = { 'obstacles': self._filter_navigation_obstacles(detections['detections']), 'traversable_areas': traversable_map, 'safe_paths': self._compute_safe_paths(traversable_map) } return navigation_data def _analyze_traversable_areas(self, depth_image: np.ndarray) -&gt; np.ndarray: &quot;&quot;&quot;Analyze depth image to determine traversable areas&quot;&quot;&quot; # Convert depth to point cloud processor = DepthProcessor( camera_intrinsics=np.array([[616.171, 0, 319.5], [0, 616.171, 239.5], [0, 0, 1]]) ) pcd = processor.depth_to_pointcloud(depth_image) # Segment ground plane ground_model, ground_points = processor.segment_planes(pcd) # Analyze surface normals to determine walkability normals = processor.compute_surface_normals(pcd) # Create traversability map height, width = depth_image.shape traversability = np.ones((height, width)) # 1.0 = traversable # Mark steep areas as non-traversable for i, normal in enumerate(normals): if abs(normal[2]) &lt; 0.7: # Surface is too steep if normal z-component &lt; 0.7 # Map back to image coordinates (simplified) pass # Would map 3D point back to 2D image coordinates return traversability def _filter_navigation_obstacles(self, detections) -&gt; List[Dict]: &quot;&quot;&quot;Filter detections to focus on navigation-relevant obstacles&quot;&quot;&quot; navigation_obstacles = [] for detection in detections: # Consider people, furniture, large objects as obstacles if detection['label'] in ['person', 'chair', 'couch', 'table', 'bed', 'dining table']: navigation_obstacles.append(detection) return navigation_obstacles def _compute_safe_paths(self, traversability_map: np.ndarray) -&gt; List[np.ndarray]: &quot;&quot;&quot;Compute safe navigation paths&quot;&quot;&quot; # Implementation would use path planning algorithms # like A*, Dijkstra, or RRT return []   ","version":"Next","tagName":"h3"},{"title":"Example 2: Manipulation Vision for Humanoid Robot​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#example-2-manipulation-vision-for-humanoid-robot","content":" class ManipulationVisionSystem: def __init__(self, robot_config: Dict): self.robot_config = robot_config self.hand_camera = None self.arm_joints = robot_config.get('arm_joints', []) # Initialize specialized manipulation perception self.grasp_detector = ManipulationTargetDetector( DepthProcessor( camera_intrinsics=np.array([[616.171, 0, 319.5], [0, 616.171, 239.5], [0, 0, 1]]) ) ) # Initialize object property estimation self.object_property_estimator = ObjectPropertyEstimator() def perceive_manipulation_targets(self, rgb_image: np.ndarray, depth_image: np.ndarray) -&gt; List[Dict]: &quot;&quot;&quot;Perceive objects suitable for manipulation&quot;&quot;&quot; # Find graspable objects graspable_objects = self.grasp_detector.find_graspable_objects( depth_image, rgb_image, self._get_robot_workspace() ) # Enhance with object properties for obj in graspable_objects: # Estimate object properties like weight, friction, etc. properties = self.object_property_estimator.estimate_properties( obj, rgb_image, depth_image ) obj['properties'] = properties return graspable_objects def _get_robot_workspace(self) -&gt; np.ndarray: &quot;&quot;&quot;Get robot's reachable workspace&quot;&quot;&quot; # Define workspace based on robot kinematics # This would typically come from robot's URDF or kinematic model return np.array([-0.5, -0.5, 0.2, 0.8, 0.5, 1.5]) # x, y, z bounds def select_best_grasp_object(self, objects: List[Dict]) -&gt; Optional[Dict]: &quot;&quot;&quot;Select the best object for grasping based on criteria&quot;&quot;&quot; if not objects: return None # Criteria for selection: # 1. Within reach # 2. Appropriate size # 3. Good grasp poses available # 4. Task relevance (if specified) best_object = None best_score = -1 for obj in objects: score = self._score_object_for_grasping(obj) if score &gt; best_score: best_score = score best_object = obj return best_object def _score_object_for_grasping(self, obj: Dict) -&gt; float: &quot;&quot;&quot;Score an object for grasping suitability&quot;&quot;&quot; score = 0.0 # Size appropriateness (prefer medium-sized objects) size = np.array(obj['size']) size_score = 1.0 / (1.0 + abs(np.mean(size) - 0.1)) # Prefer ~10cm objects score += size_score * 0.3 # Height appropriateness (prefer objects at good heights) height_score = 1.0 / (1.0 + abs(obj['center'][2] - 0.8)) # Prefer ~80cm height score += height_score * 0.2 # Approach direction availability approach_score = len(obj.get('approach_directions', [])) * 0.1 score += min(approach_score, 0.3) # Cap at 0.3 # Volume consideration (not too heavy or too light) volume = obj['volume'] volume_score = 1.0 / (1.0 + abs(volume - 0.001)) # Prefer ~1000cm³ objects score += volume_score * 0.2 return score class ObjectPropertyEstimator: def __init__(self): # Initialize models for property estimation self.material_classifier = self._load_material_classifier() self.weight_estimator = self._load_weight_estimator() def _load_material_classifier(self): &quot;&quot;&quot;Load material classification model&quot;&quot;&quot; # Placeholder - would load actual model return None def _load_weight_estimator(self): &quot;&quot;&quot;Load weight estimation model&quot;&quot;&quot; # Placeholder - would load actual model return None def estimate_properties(self, object_info: Dict, rgb_image: np.ndarray, depth_image: np.ndarray) -&gt; Dict: &quot;&quot;&quot;Estimate object properties for manipulation&quot;&quot;&quot; # Estimate material properties material_properties = self._estimate_material_properties( object_info, rgb_image ) # Estimate physical properties physical_properties = self._estimate_physical_properties( object_info, depth_image ) # Estimate graspability graspability = self._estimate_graspability(object_info) return { 'material': material_properties, 'physical': physical_properties, 'graspability': graspability } def _estimate_material_properties(self, object_info: Dict, rgb_image: np.ndarray) -&gt; Dict: &quot;&quot;&quot;Estimate material properties from visual appearance&quot;&quot;&quot; # Extract region of interest bbox = object_info['bbox'] x1, y1 = int(bbox[0][0]), int(bbox[0][1]) x2, y2 = int(bbox[1][0]), int(bbox[1][1]) roi = rgb_image[y1:y2, x1:x2] # Analyze color, texture, and reflectance properties # This is a simplified approach avg_color = np.mean(roi, axis=(0, 1)) material_properties = { 'color': avg_color.tolist(), 'texture_complexity': self._calculate_texture_complexity(roi), 'estimated_material': self._classify_material(avg_color) } return material_properties def _calculate_texture_complexity(self, image: np.ndarray) -&gt; float: &quot;&quot;&quot;Calculate texture complexity using gradient magnitude&quot;&quot;&quot; gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image gradients = np.sqrt( cv2.Sobel(gray, cv2.CV_64F, 1, 0)**2 + cv2.Sobel(gray, cv2.CV_64F, 0, 1)**2 ) return float(np.mean(gradients)) def _classify_material(self, avg_color: np.ndarray) -&gt; str: &quot;&quot;&quot;Classify material based on color (simplified)&quot;&quot;&quot; r, g, b = avg_color if r &gt; 0.7 and g &gt; 0.7 and b &gt; 0.7: return 'metal' elif r &gt; 0.8 and g &gt; 0.6 and b &lt; 0.3: return 'plastic' elif r &gt; 0.6 and g &gt; 0.4 and b &gt; 0.2: return 'wood' else: return 'unknown' def _estimate_physical_properties(self, object_info: Dict, depth_image: np.ndarray) -&gt; Dict: &quot;&quot;&quot;Estimate physical properties like weight, friction, etc.&quot;&quot;&quot; # Use object size and typical densities to estimate weight size = np.array(object_info['size']) volume = np.prod(size) # Assume typical density for household objects (0.5 g/cm³ = 500 kg/m³) estimated_density = 500 # kg/m³ estimated_weight = volume * estimated_density physical_properties = { 'volume': float(volume), 'estimated_weight': float(estimated_weight), 'estimated_density': estimated_density, 'friction_coefficient': self._estimate_friction_coefficient(object_info) } return physical_properties def _estimate_friction_coefficient(self, object_info: Dict) -&gt; float: &quot;&quot;&quot;Estimate friction coefficient based on material and surface properties&quot;&quot;&quot; # Simplified estimation based on object properties # In practice, this would use more sophisticated models size = np.array(object_info['size']) surface_area = 2 * (size[0]*size[1] + size[1]*size[2] + size[0]*size[2]) # Smaller objects generally have higher effective friction friction_factor = min(1.0, 0.5 / (surface_area + 0.01)) return float(0.3 + friction_factor * 0.4) # Range: 0.3 to 0.7 def _estimate_graspability(self, object_info: Dict) -&gt; Dict: &quot;&quot;&quot;Estimate how graspable the object is&quot;&quot;&quot; size = np.array(object_info['size']) # Calculate graspability metrics min_dimension = np.min(size) max_dimension = np.max(size) aspect_ratio = max_dimension / (min_dimension + 1e-6) graspability = { 'min_dimension': float(min_dimension), 'max_dimension': float(max_dimension), 'aspect_ratio': float(aspect_ratio), 'is_ergonomic': min_dimension &gt; 0.02 and aspect_ratio &lt; 5.0, # &gt;2cm and &lt;5:1 ratio 'preferred_grasp_type': self._determine_grasp_type(size) } return graspability def _determine_grasp_type(self, size: np.ndarray) -&gt; str: &quot;&quot;&quot;Determine preferred grasp type based on object dimensions&quot;&quot;&quot; sorted_dims = np.sort(size) if sorted_dims[0] &lt; 0.02: # Very thin - need precision grasp return 'pinch' elif sorted_dims[2] / sorted_dims[0] &gt; 4: # Elongated - need power grasp return 'power' else: # Medium size - can use various grasps return 'medium'   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Vision Pipeline Optimization​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#exercise-1-vision-pipeline-optimization","content":" Implement an optimized vision pipeline that:  Processes images at 30 FPS on Jetson Orin hardwareImplements multi-threading for parallel processingIncludes performance monitoring and adaptive processingMaintains accuracy while optimizing for speed  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: 3D Perception System​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#exercise-2-3d-perception-system","content":" Create a complete 3D perception system that:  Converts RGB-D data to point cloudsSegments planar surfaces (floors, tables)Detects objects positioned on surfacesEstimates object properties for manipulation  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Robot-Integrated Vision​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#exercise-3-robot-integrated-vision","content":" Develop a vision system integrated with robot control that:  Provides real-time feedback to navigation systemIdentifies graspable objects for manipulationIncorporates robot kinematics into perceptionHandles sensor noise and uncertainty  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#summary","content":" Computer vision for robotics requires specialized techniques that account for the unique requirements of physical interaction systems. Unlike traditional computer vision applications, robotics vision must operate under real-time constraints, handle motion and ego-motion, and provide outputs directly relevant to physical action. For humanoid robots, vision systems must be optimized for embedded hardware while maintaining the accuracy and robustness required for safe operation in human-centered environments. The integration of vision with the broader Vision-Language-Action pipeline enables humanoid robots to perceive, understand, and interact with their environment effectively.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 14 - Computer Vision for Robotics","url":"/docs/chapters/module-4-vla/chapter-14-computer-vision#further-reading","content":" &quot;Computer Vision in Robotics and Automation&quot; by S. Hutchinson and G. Chirikjian&quot;Handbook of Robotics&quot; edited by Siciliano and Khatib (Vision chapter)&quot;Multiple View Geometry in Computer Vision&quot; by Hartley and Zisserman&quot;Real-Time Computer Vision&quot; by Jähne&quot;Robotics: Vision, Manipulation and Control&quot; by Spong, Hutchinson, and Vidyasagar ","version":"Next","tagName":"h2"},{"title":"Chapter 15 - Language Understanding in Robotics","type":0,"sectionRef":"#","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#learning-objectives","content":" Implement language understanding for roboticsApply NLP techniques to humanoid robot interactionIntegrate language with vision and action systems  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#introduction","content":" Language understanding forms the Language component of the Vision-Language-Action (VLA) pipeline, which is central to our project's VLA Convergence Mandate principle. For humanoid robots operating in human-centered environments, language understanding systems must be capable of processing natural language commands, engaging in human-like interaction, and translating high-level linguistic goals into concrete robotic actions. This chapter explores natural language processing techniques specifically tailored for robotics applications, with special emphasis on humanoid robot interaction systems that must operate effectively in human environments using natural language as the primary control interface, as mandated by our constitution.  ","version":"Next","tagName":"h2"},{"title":"Language Understanding in Robotics Context​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#language-understanding-in-robotics-context","content":" ","version":"Next","tagName":"h2"},{"title":"The VLA Pipeline Architecture​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#the-vla-pipeline-architecture","content":" The Vision-Language-Action pipeline in humanoid robots operates as follows:  graph TD A[Natural Language Command] --&gt; B[Language Understanding] B --&gt; C[Action Planning] C --&gt; D[Vision System] D --&gt; E[Action Execution] E --&gt; A   This creates a closed loop where language commands are interpreted, plans are generated, vision systems provide feedback, and actions are executed, with continuous monitoring and adjustment.  ","version":"Next","tagName":"h3"},{"title":"Key Challenges in Robotic Language Understanding​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#key-challenges-in-robotic-language-understanding","content":" Ambiguity Resolution​  Natural language commands often contain ambiguities that must be resolved through context and perception:  Spatial references: &quot;Pick up the cup&quot; - which cup?Temporal references: &quot;Do it again&quot; - what is &quot;it&quot;?Pragmatic understanding: &quot;It's cold&quot; - should the robot adjust temperature?  Situated Language Understanding​  Robotic language understanding must be grounded in the physical environment:  Deixis: Understanding &quot;this&quot;, &quot;that&quot;, &quot;here&quot;, &quot;there&quot; in spatial contextPerceptual grounding: Connecting words to visual objects and locationsAction grounding: Connecting language to executable robot actions  Multi-Modal Integration​  Language understanding in robotics must integrate with other modalities:  Visual context: Understanding commands based on what the robot seesAction context: Understanding commands based on current robot stateTemporal context: Understanding commands based on recent interactions  ","version":"Next","tagName":"h3"},{"title":"Natural Language Processing for Robotics​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#natural-language-processing-for-robotics","content":" ","version":"Next","tagName":"h2"},{"title":"Command Parsing and Semantic Analysis​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#command-parsing-and-semantic-analysis","content":" import re import spacy from typing import List, Dict, Optional, Tuple from dataclasses import dataclass from enum import Enum @dataclass class RobotCommand: &quot;&quot;&quot;Represents a parsed robot command with semantic meaning&quot;&quot;&quot; action: str target_object: Optional[str] = None target_location: Optional[str] = None attributes: Dict[str, str] = None confidence: float = 1.0 class Action(Enum): &quot;&quot;&quot;Enumeration of supported robot actions&quot;&quot;&quot; MOVE = &quot;move&quot; GRASP = &quot;grasp&quot; PLACE = &quot;place&quot; GREET = &quot;greet&quot; FOLLOW = &quot;follow&quot; FIND = &quot;find&quot; REPORT = &quot;report&quot; WAIT = &quot;wait&quot; STOP = &quot;stop&quot; class CommandParser: def __init__(self): &quot;&quot;&quot;Initialize the command parser with spaCy model&quot;&quot;&quot; try: self.nlp = spacy.load(&quot;en_core_web_sm&quot;) except OSError: print(&quot;spaCy model not found. Install with: python -m spacy download en_core_web_sm&quot;) # Fallback to simple rule-based parsing self.nlp = None # Define action patterns for rule-based parsing self.action_patterns = { Action.GRASP: [ r&quot;pick up (.+)&quot;, r&quot;grab (.+)&quot;, r&quot;take (.+)&quot;, r&quot;get (.+)&quot;, r&quot;lift (.+)&quot; ], Action.PLACE: [ r&quot;put (.+) (?:on|in|at) (.+)&quot;, r&quot;place (.+) (?:on|in|at) (.+)&quot;, r&quot;drop (.+) (?:on|in|at) (.+)&quot;, r&quot;set (.+) (?:on|in|at) (.+)&quot; ], Action.MOVE: [ r&quot;go to (.+)&quot;, r&quot;move to (.+)&quot;, r&quot;walk to (.+)&quot;, r&quot;navigate to (.+)&quot;, r&quot;approach (.+)&quot; ], Action.FOLLOW: [ r&quot;follow (.+)&quot;, r&quot;come after (.+)&quot;, r&quot;accompany (.+)&quot; ], Action.FIND: [ r&quot;find (.+)&quot;, r&quot;locate (.+)&quot;, r&quot;search for (.+)&quot;, r&quot;look for (.+)&quot; ], Action.GREET: [ r&quot;hello&quot;, r&quot;hi&quot;, r&quot;greet (.+)&quot;, r&quot;say hello to (.+)&quot; ], Action.REPORT: [ r&quot;what do you see&quot;, r&quot;describe the room&quot;, r&quot;report (.+)&quot;, r&quot;tell me (.+)&quot; ], Action.WAIT: [ r&quot;wait&quot;, r&quot;stop&quot;, r&quot;pause&quot;, r&quot;hold on&quot; ] } def parse_command(self, text: str) -&gt; Optional[RobotCommand]: &quot;&quot;&quot; Parse a natural language command into structured representation Args: text: Natural language command Returns: Parsed RobotCommand or None if parsing fails &quot;&quot;&quot; text = text.strip().lower() if self.nlp: # Use spaCy for advanced parsing return self._parse_with_spacy(text) else: # Fallback to rule-based parsing return self._parse_with_rules(text) def _parse_with_spacy(self, text: str) -&gt; Optional[RobotCommand]: &quot;&quot;&quot;Parse command using spaCy NLP model&quot;&quot;&quot; doc = self.nlp(text) # Extract main action (verb) action = self._extract_action(doc) if not action: return None # Extract target object target_object = self._extract_target_object(doc) # Extract target location target_location = self._extract_target_location(doc) # Extract additional attributes attributes = self._extract_attributes(doc) return RobotCommand( action=action.value, target_object=target_object, target_location=target_location, attributes=attributes ) def _extract_action(self, doc) -&gt; Optional[Action]: &quot;&quot;&quot;Extract the main action from parsed document&quot;&quot;&quot; # Look for root verb or main action for token in doc: if token.pos_ == &quot;VERB&quot;: # Check if this verb corresponds to a known action verb_lemma = token.lemma_.lower() for action_enum in Action: if verb_lemma in action_enum.value: return action_enum # Check for variations if verb_lemma in [&quot;take&quot;, &quot;grasp&quot;, &quot;seize&quot;, &quot;catch&quot;]: return Action.GRASP elif verb_lemma in [&quot;put&quot;, &quot;place&quot;, &quot;set&quot;, &quot;drop&quot;]: return Action.PLACE elif verb_lemma in [&quot;go&quot;, &quot;move&quot;, &quot;walk&quot;, &quot;navigate&quot;, &quot;approach&quot;]: return Action.MOVE elif verb_lemma in [&quot;follow&quot;, &quot;accompany&quot;, &quot;chase&quot;]: return Action.FOLLOW elif verb_lemma in [&quot;find&quot;, &quot;locate&quot;, &quot;search&quot;, &quot;look&quot;]: return Action.FIND elif verb_lemma in [&quot;greet&quot;, &quot;hello&quot;, &quot;hi&quot;, &quot;wave&quot;]: return Action.GREET elif verb_lemma in [&quot;report&quot;, &quot;describe&quot;, &quot;tell&quot;, &quot;say&quot;]: return Action.REPORT elif verb_lemma in [&quot;wait&quot;, &quot;pause&quot;, &quot;stop&quot;, &quot;hold&quot;]: return Action.WAIT return None def _extract_target_object(self, doc) -&gt; Optional[str]: &quot;&quot;&quot;Extract target object from parsed document&quot;&quot;&quot; # Look for direct objects and noun phrases for token in doc: if token.dep_ == &quot;dobj&quot;: # Direct object # Get the full noun phrase return self._get_full_noun_phrase(token) # Look for objects after prepositions if token.pos_ == &quot;NOUN&quot; and token.head.dep_ in [&quot;pobj&quot;, &quot;pcomp&quot;]: return self._get_full_noun_phrase(token) # If no direct object found, look for noun phrases for chunk in doc.noun_chunks: if chunk.root.pos_ == &quot;NOUN&quot;: # Avoid pronouns and articles if chunk.text.lower() not in [&quot;it&quot;, &quot;this&quot;, &quot;that&quot;, &quot;the&quot;, &quot;a&quot;, &quot;an&quot;]: return chunk.text return None def _extract_target_location(self, doc) -&gt; Optional[str]: &quot;&quot;&quot;Extract target location from parsed document&quot;&quot;&quot; # Look for prepositional phrases indicating location for token in doc: if token.pos_ == &quot;ADP&quot; and token.text in [&quot;to&quot;, &quot;at&quot;, &quot;on&quot;, &quot;in&quot;, &quot;by&quot;, &quot;near&quot;]: # Look for the object of the preposition for child in token.children: if child.pos_ == &quot;NOUN&quot; or child.pos_ == &quot;PROPN&quot;: return self._get_full_noun_phrase(child) # Look for noun chunks that might be locations for chunk in doc.noun_chunks: # Check if this could be a location based on context if any(word in chunk.text.lower() for word in [&quot;table&quot;, &quot;kitchen&quot;, &quot;room&quot;, &quot;door&quot;, &quot;chair&quot;, &quot;couch&quot;, &quot;bed&quot;, &quot;desk&quot;]): return chunk.text return None def _extract_attributes(self, doc) -&gt; Dict[str, str]: &quot;&quot;&quot;Extract additional attributes like colors, sizes, etc.&quot;&quot;&quot; attributes = {} for token in doc: if token.pos_ == &quot;ADJ&quot;: # Adjectives # Look for adjectives that modify nouns if token.head.pos_ in [&quot;NOUN&quot;, &quot;PROPN&quot;]: noun_phrase = self._get_full_noun_phrase(token.head) attributes[f&quot;adjective_{token.text}&quot;] = noun_phrase elif token.pos_ == &quot;NUM&quot;: # Numbers # Look for numbers that might indicate quantities attributes[f&quot;number_{token.text}&quot;] = token.head.text return attributes def _get_full_noun_phrase(self, token) -&gt; str: &quot;&quot;&quot;Get the full noun phrase starting from a token&quot;&quot;&quot; # This is a simplified approach - spaCy has more sophisticated methods phrase = [] # Add left-side modifiers (adjectives, determiners) for left_token in token.lefts: if left_token.pos_ in [&quot;ADJ&quot;, &quot;DET&quot;]: phrase.append(left_token.text) # Add the main token phrase.append(token.text) # Add right-side modifiers if any for right_token in token.rights: if right_token.pos_ in [&quot;NOUN&quot;, &quot;PROPN&quot;] and right_token.dep_ == &quot;compound&quot;: phrase.append(right_token.text) return &quot; &quot;.join(phrase) def _parse_with_rules(self, text: str) -&gt; Optional[RobotCommand]: &quot;&quot;&quot;Parse command using rule-based patterns (fallback)&quot;&quot;&quot; for action, patterns in self.action_patterns.items(): for pattern in patterns: match = re.search(pattern, text) if match: groups = match.groups() if action in [Action.PLACE] and len(groups) &gt;= 2: # Pattern: &quot;put X on Y&quot; - groups are [object, location] return RobotCommand( action=action.value, target_object=groups[0], target_location=groups[1] ) elif len(groups) &gt;= 1: # Pattern with one main argument return RobotCommand( action=action.value, target_object=groups[0] if len(groups) &gt; 0 else None, target_location=groups[1] if len(groups) &gt; 1 else None ) else: # Pattern without arguments return RobotCommand(action=action.value) # If no pattern matches, return None return None class ContextualCommandResolver: def __init__(self, vision_system, robot_state): &quot;&quot;&quot; Initialize resolver with access to vision and robot state Args: vision_system: Interface to robot's vision system robot_state: Interface to robot's current state &quot;&quot;&quot; self.vision_system = vision_system self.robot_state = robot_state self.command_parser = CommandParser() self.conversation_context = [] def resolve_command(self, text: str, world_state: Dict = None) -&gt; Optional[RobotCommand]: &quot;&quot;&quot; Resolve an ambiguous command using context and perception Args: text: Natural language command world_state: Current world state including objects and locations Returns: Resolved RobotCommand with specific references &quot;&quot;&quot; # Parse the command first parsed_command = self.command_parser.parse_command(text) if not parsed_command: return None # Resolve ambiguous references using context resolved_command = self._resolve_ambiguous_references(parsed_command, world_state) # Add to conversation context self.conversation_context.append({ 'command': text, 'parsed': parsed_command, 'resolved': resolved_command, 'timestamp': time.time() }) # Keep only recent context (last 10 interactions) if len(self.conversation_context) &gt; 10: self.conversation_context = self.conversation_context[-10:] return resolved_command def _resolve_ambiguous_references(self, command: RobotCommand, world_state: Dict) -&gt; RobotCommand: &quot;&quot;&quot;Resolve ambiguous references like 'it', 'there', etc. using context&quot;&quot;&quot; resolved = RobotCommand( action=command.action, target_object=command.target_object, target_location=command.target_location, attributes=command.attributes or {} ) # Resolve pronouns and demonstratives if resolved.target_object and resolved.target_object.lower() in [&quot;it&quot;, &quot;that&quot;, &quot;this&quot;]: # Use vision system to identify the most likely referent resolved.target_object = self._resolve_pronoun_reference( resolved.target_object, world_state ) if resolved.target_location and resolved.target_location.lower() in [&quot;there&quot;, &quot;here&quot;, &quot;that place&quot;]: # Use robot state and world state to resolve location resolved.target_location = self._resolve_location_reference( resolved.target_location, world_state ) return resolved def _resolve_pronoun_reference(self, pronoun: str, world_state: Dict) -&gt; Optional[str]: &quot;&quot;&quot;Resolve pronoun references using visual context&quot;&quot;&quot; if not world_state or 'visible_objects' not in world_state: return None visible_objects = world_state['visible_objects'] if pronoun.lower() == &quot;it&quot;: # Use the most recently mentioned or most salient object if len(visible_objects) == 1: return visible_objects[0]['name'] else: # For now, return the first object - in practice, use more sophisticated resolution return visible_objects[0]['name'] if visible_objects else None elif pronoun.lower() in [&quot;this&quot;, &quot;that&quot;]: # Use spatial context or the closest object closest_object = self._find_closest_object(visible_objects) return closest_object['name'] if closest_object else None return None def _resolve_location_reference(self, location_ref: str, world_state: Dict) -&gt; Optional[str]: &quot;&quot;&quot;Resolve location references like 'here', 'there'&quot;&quot;&quot; if not world_state: return None current_location = world_state.get('robot_location', 'unknown') if location_ref.lower() == &quot;here&quot;: return current_location elif location_ref.lower() == &quot;there&quot;: # Use the most recently mentioned location from context for entry in reversed(self.conversation_context): if entry['resolved'].target_location: return entry['resolved'].target_location return None def _find_closest_object(self, objects: List[Dict]) -&gt; Optional[Dict]: &quot;&quot;&quot;Find the closest object based on spatial information&quot;&quot;&quot; # This would use actual spatial relationships # For now, return the first object return objects[0] if objects else None   ","version":"Next","tagName":"h3"},{"title":"Intent Recognition and Dialogue Management​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#intent-recognition-and-dialogue-management","content":" import json import time from typing import List, Dict, Any from collections import defaultdict class IntentRecognizer: def __init__(self): &quot;&quot;&quot;Initialize intent recognition system&quot;&quot;&quot; self.intents = self._load_intent_definitions() self.context_manager = ContextManager() def _load_intent_definitions(self) -&gt; Dict: &quot;&quot;&quot;Load intent definitions with patterns and required entities&quot;&quot;&quot; return { &quot;navigation&quot;: { &quot;patterns&quot;: [ r&quot;go to (.+)&quot;, r&quot;move to (.+)&quot;, r&quot;walk to (.+)&quot;, r&quot;navigate to (.+)&quot;, r&quot;approach (.+)&quot; ], &quot;required_entities&quot;: [&quot;location&quot;], &quot;optional_entities&quot;: [&quot;speed&quot;, &quot;caution_level&quot;] }, &quot;object_manipulation&quot;: { &quot;patterns&quot;: [ r&quot;pick up (.+)&quot;, r&quot;grab (.+)&quot;, r&quot;take (.+)&quot;, r&quot;get (.+) and put it (?:on|in) (.+)&quot;, r&quot;move (.+) from (.+) to (.+)&quot; ], &quot;required_entities&quot;: [&quot;object&quot;], &quot;optional_entities&quot;: [&quot;source_location&quot;, &quot;target_location&quot;] }, &quot;social_interaction&quot;: { &quot;patterns&quot;: [ r&quot;hello&quot;, r&quot;hi&quot;, r&quot;greet (.+)&quot;, r&quot;introduce yourself&quot;, r&quot;what's your name&quot; ], &quot;required_entities&quot;: [], &quot;optional_entities&quot;: [&quot;person&quot;] }, &quot;information_request&quot;: { &quot;patterns&quot;: [ r&quot;what can you do&quot;, r&quot;help&quot;, r&quot;what's possible&quot;, r&quot;capabilities&quot; ], &quot;required_entities&quot;: [], &quot;optional_entities&quot;: [] }, &quot;status_inquiry&quot;: { &quot;patterns&quot;: [ r&quot;what do you see&quot;, r&quot;describe (.+)&quot;, r&quot;report (.+)&quot;, r&quot;tell me about (.+)&quot; ], &quot;required_entities&quot;: [], &quot;optional_entities&quot;: [&quot;target&quot;] } } def recognize_intent(self, text: str) -&gt; Dict[str, Any]: &quot;&quot;&quot; Recognize intent and extract entities from text Args: text: Input text to analyze Returns: Dictionary with intent, confidence, and extracted entities &quot;&quot;&quot; text_lower = text.lower().strip() for intent_name, intent_def in self.intents.items(): for pattern in intent_def[&quot;patterns&quot;]: match = re.search(pattern, text_lower) if match: entities = { f&quot;entity_{i}&quot;: entity for i, entity in enumerate(match.groups()) } # Validate required entities are present required_count = len(intent_def[&quot;required_entities&quot;]) if len(entities) &gt;= required_count: return { &quot;intent&quot;: intent_name, &quot;confidence&quot;: 0.9, # High confidence for pattern matches &quot;entities&quot;: entities, &quot;original_text&quot;: text } # If no pattern matches, use default intent return { &quot;intent&quot;: &quot;unknown&quot;, &quot;confidence&quot;: 0.0, &quot;entities&quot;: {}, &quot;original_text&quot;: text } def process_command_with_context(self, text: str, context: Dict = None) -&gt; Dict[str, Any]: &quot;&quot;&quot; Process command considering conversation context Args: text: Input command text context: Current conversation context Returns: Processed command with context resolution &quot;&quot;&quot; # Recognize intent and entities result = self.recognize_intent(text) # Resolve context-dependent references if context: result[&quot;entities&quot;] = self._resolve_context_references( result[&quot;entities&quot;], context ) # Update context if context is not None: context[&quot;last_command&quot;] = result context[&quot;command_history&quot;].append(result) return result def _resolve_context_references(self, entities: Dict, context: Dict) -&gt; Dict: &quot;&quot;&quot;Resolve entities that depend on conversation context&quot;&quot;&quot; resolved_entities = entities.copy() # Resolve pronouns and demonstratives based on context for key, value in resolved_entities.items(): if value.lower() in [&quot;it&quot;, &quot;this&quot;, &quot;that&quot;]: # Resolve based on previous context previous_entities = self._get_recent_entities(context) if previous_entities: # For &quot;it&quot;, use the most recent object if value.lower() == &quot;it&quot;: for prev_entity_key, prev_value in previous_entities.items(): if &quot;object&quot; in prev_entity_key or &quot;location&quot; in prev_entity_key: resolved_entities[key] = prev_value break return resolved_entities def _get_recent_entities(self, context: Dict) -&gt; Dict: &quot;&quot;&quot;Get entities from recent conversation&quot;&quot;&quot; if &quot;command_history&quot; not in context: return {} recent_commands = context[&quot;command_history&quot;][-3:] # Last 3 commands recent_entities = {} for cmd in recent_commands: if &quot;entities&quot; in cmd: recent_entities.update(cmd[&quot;entities&quot;]) return recent_entities class ContextManager: def __init__(self): &quot;&quot;&quot;Initialize conversation context manager&quot;&quot;&quot; self.conversation_history = [] self.object_references = {} self.location_references = {} self.user_preferences = defaultdict(lambda: None) def update_context(self, command_result: Dict): &quot;&quot;&quot;Update context with new command result&quot;&quot;&quot; self.conversation_history.append(command_result) # Update object and location references if present entities = command_result.get(&quot;entities&quot;, {}) for key, value in entities.items(): if &quot;object&quot; in key.lower(): self.object_references[value] = len(self.conversation_history) - 1 elif &quot;location&quot; in key.lower(): self.location_references[value] = len(self.conversation_history) - 1 # Keep only recent history if len(self.conversation_history) &gt; 20: self.conversation_history = self.conversation_history[-20:] def get_context(self) -&gt; Dict: &quot;&quot;&quot;Get current context for command processing&quot;&quot;&quot; return { &quot;conversation_history&quot;: self.conversation_history[-10:], # Last 10 exchanges &quot;object_references&quot;: dict(self.object_references), &quot;location_references&quot;: dict(self.location_references), &quot;user_preferences&quot;: dict(self.user_preferences), &quot;current_time&quot;: time.time() } class DialogueManager: def __init__(self): &quot;&quot;&quot;Initialize dialogue management system&quot;&quot;&quot; self.intent_recognizer = IntentRecognizer() self.response_generator = ResponseGenerator() self.context_manager = ContextManager() def process_input(self, user_input: str, world_state: Dict = None) -&gt; Dict: &quot;&quot;&quot; Process user input and generate appropriate response Args: user_input: Natural language input from user world_state: Current state of the world (objects, locations, etc.) Returns: Dictionary with action to take and response to give &quot;&quot;&quot; # Get current context context = self.context_manager.get_context() # Recognize intent with context intent_result = self.intent_recognizer.process_command_with_context( user_input, context ) # Generate appropriate response based on intent response = self.response_generator.generate_response( intent_result, world_state, context ) # Update context self.context_manager.update_context({ &quot;input&quot;: user_input, &quot;intent_result&quot;: intent_result, &quot;response&quot;: response }) return { &quot;intent&quot;: intent_result[&quot;intent&quot;], &quot;entities&quot;: intent_result[&quot;entities&quot;], &quot;action&quot;: response.get(&quot;action&quot;), &quot;response_text&quot;: response.get(&quot;response_text&quot;), &quot;confidence&quot;: intent_result[&quot;confidence&quot;] } class ResponseGenerator: def __init__(self): &quot;&quot;&quot;Initialize response generation system&quot;&quot;&quot; self.response_templates = self._load_response_templates() def _load_response_templates(self) -&gt; Dict: &quot;&quot;&quot;Load response templates for different intents&quot;&quot;&quot; return { &quot;navigation&quot;: [ &quot;I will navigate to {location}.&quot;, &quot;Moving toward {location} now.&quot;, &quot;Heading to {location}.&quot; ], &quot;object_manipulation&quot;: [ &quot;I will {action} the {object}.&quot;, &quot;Attempting to {action} the {object}.&quot;, &quot;Processing manipulation of {object}.&quot; ], &quot;social_interaction&quot;: [ &quot;Hello! How can I assist you?&quot;, &quot;Greetings! What would you like me to do?&quot;, &quot;Hi there! How may I help you?&quot; ], &quot;information_request&quot;: [ &quot;I can help with navigation, object manipulation, and social interaction.&quot;, &quot;My capabilities include moving around, picking up objects, and talking with you.&quot;, &quot;I can navigate spaces, manipulate objects, and engage in conversation.&quot; ], &quot;status_inquiry&quot;: [ &quot;I see {objects_count} objects in view.&quot;, &quot;The room contains {objects_list}.&quot;, &quot;I can see: {objects_list}.&quot; ], &quot;unknown&quot;: [ &quot;I'm not sure I understand. Could you rephrase that?&quot;, &quot;I didn't catch that. Can you say it differently?&quot;, &quot;I'm not sure what you mean. Could you be more specific?&quot; ] } def generate_response(self, intent_result: Dict, world_state: Dict, context: Dict) -&gt; Dict: &quot;&quot;&quot;Generate appropriate response based on intent and world state&quot;&quot;&quot; intent = intent_result[&quot;intent&quot;] entities = intent_result[&quot;entities&quot;] # Select template based on intent templates = self.response_templates.get(intent, self.response_templates[&quot;unknown&quot;]) template = templates[0] if templates else &quot;I understand.&quot; # Fill in template with entities response_text = template for entity_key, entity_value in entities.items(): placeholder = &quot;{&quot; + entity_key.replace(&quot;entity_&quot;, &quot;&quot;) + &quot;}&quot; response_text = response_text.replace(placeholder, str(entity_value)) # Special handling for status inquiries if intent == &quot;status_inquiry&quot; and world_state: if &quot;visible_objects&quot; in world_state: objects = [obj.get(&quot;name&quot;, &quot;unknown object&quot;) for obj in world_state[&quot;visible_objects&quot;]] response_text = response_text.replace(&quot;{objects_count}&quot;, str(len(objects))) response_text = response_text.replace(&quot;{objects_list}&quot;, &quot;, &quot;.join(objects)) # Determine action based on intent action = self._map_intent_to_action(intent, entities, world_state) return { &quot;response_text&quot;: response_text, &quot;action&quot;: action, &quot;requires_clarification&quot;: self._needs_clarification(intent, entities, world_state) } def _map_intent_to_action(self, intent: str, entities: Dict, world_state: Dict) -&gt; Dict: &quot;&quot;&quot;Map intent to specific robot action&quot;&quot;&quot; if intent == &quot;navigation&quot;: return { &quot;type&quot;: &quot;navigate&quot;, &quot;target_location&quot;: entities.get(&quot;entity_0&quot;, &quot;unknown&quot;) } elif intent == &quot;object_manipulation&quot;: action = entities.get(&quot;entity_0&quot;, &quot;unknown&quot;) target_object = entities.get(&quot;entity_1&quot;, &quot;unknown&quot;) return { &quot;type&quot;: &quot;manipulate&quot;, &quot;action&quot;: action, &quot;target_object&quot;: target_object } elif intent == &quot;social_interaction&quot;: return { &quot;type&quot;: &quot;social&quot;, &quot;action&quot;: &quot;greet&quot; } elif intent == &quot;information_request&quot;: return { &quot;type&quot;: &quot;report&quot;, &quot;action&quot;: &quot;capabilities&quot; } elif intent == &quot;status_inquiry&quot;: return { &quot;type&quot;: &quot;report&quot;, &quot;action&quot;: &quot;environment_status&quot; } else: return { &quot;type&quot;: &quot;unknown&quot;, &quot;action&quot;: &quot;awaiting_clarification&quot; } def _needs_clarification(self, intent: str, entities: Dict, world_state: Dict) -&gt; bool: &quot;&quot;&quot;Determine if the command needs clarification&quot;&quot;&quot; if intent == &quot;unknown&quot;: return True # Check if required entities are missing if intent == &quot;object_manipulation&quot; and not entities: return True # Check if references are ambiguous if world_state and &quot;visible_objects&quot; in world_state: visible_objects = world_state[&quot;visible_objects&quot;] for entity_key, entity_value in entities.items(): if entity_value.lower() in [&quot;it&quot;, &quot;that&quot;, &quot;this&quot;] and len(visible_objects) &gt; 1: return True return False   ","version":"Next","tagName":"h3"},{"title":"Integration with Vision and Action Systems​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#integration-with-vision-and-action-systems","content":" ","version":"Next","tagName":"h2"},{"title":"Vision-Language Integration​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#vision-language-integration","content":" import numpy as np from typing import List, Dict, Tuple, Optional from dataclasses import dataclass @dataclass class VisualObject: &quot;&quot;&quot;Represents a detected object with visual and linguistic properties&quot;&quot;&quot; id: str name: str bbox: Tuple[int, int, int, int] # (x, y, width, height) confidence: float position_3d: Optional[Tuple[float, float, float]] = None properties: Dict[str, Any] = None class VisionLanguageIntegrator: def __init__(self, vision_system, language_system): &quot;&quot;&quot; Integrate vision and language systems Args: vision_system: Interface to computer vision system language_system: Interface to language understanding system &quot;&quot;&quot; self.vision_system = vision_system self.language_system = language_system self.object_grounding = ObjectGroundingSystem() def process_vision_language_task(self, command_text: str) -&gt; Dict: &quot;&quot;&quot; Process a task that requires both vision and language understanding Args: command_text: Natural language command Returns: Dictionary with processed results &quot;&quot;&quot; # Parse the language command parsed_command = self.language_system.resolve_command( command_text, world_state=self.get_current_world_state() ) if not parsed_command: return {&quot;success&quot;: False, &quot;error&quot;: &quot;Could not parse command&quot;} # Use vision to identify relevant objects vision_results = self.vision_system.get_current_scene_description() # Ground linguistic references to visual objects grounded_command = self.object_grounding.ground_command_to_objects( parsed_command, vision_results ) # Plan the action based on grounded command action_plan = self._create_action_plan(grounded_command, vision_results) return { &quot;success&quot;: True, &quot;parsed_command&quot;: parsed_command, &quot;vision_results&quot;: vision_results, &quot;grounded_command&quot;: grounded_command, &quot;action_plan&quot;: action_plan } def get_current_world_state(self) -&gt; Dict: &quot;&quot;&quot;Get current world state including visible objects and locations&quot;&quot;&quot; # Get results from vision system vision_results = self.vision_system.get_current_scene_description() world_state = { &quot;timestamp&quot;: time.time(), &quot;visible_objects&quot;: [], &quot;robot_location&quot;: self.vision_system.get_robot_position(), &quot;traversable_areas&quot;: self.vision_system.get_traversable_map() } # Convert vision results to standardized format for obj in vision_results.get(&quot;objects&quot;, []): visual_obj = VisualObject( id=obj.get(&quot;id&quot;, f&quot;obj_{len(world_state['visible_objects'])}&quot;), name=obj.get(&quot;class&quot;, &quot;unknown&quot;), bbox=obj.get(&quot;bbox&quot;, (0, 0, 0, 0)), confidence=obj.get(&quot;confidence&quot;, 0.0), position_3d=obj.get(&quot;position_3d&quot;), properties=obj.get(&quot;properties&quot;, {}) ) world_state[&quot;visible_objects&quot;].append(visual_obj) return world_state def _create_action_plan(self, grounded_command: RobotCommand, vision_results: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Create an action plan based on grounded command and vision results&quot;&quot;&quot; action_plan = [] if grounded_command.action == Action.GRASP.value: # Find the target object target_obj = self._find_object_by_name( grounded_command.target_object, vision_results.get(&quot;objects&quot;, []) ) if target_obj: action_plan.extend([ { &quot;action&quot;: &quot;navigate&quot;, &quot;target&quot;: target_obj[&quot;position_3d&quot;], &quot;description&quot;: f&quot;Navigate to {target_obj['name']}&quot; }, { &quot;action&quot;: &quot;grasp&quot;, &quot;target&quot;: target_obj, &quot;description&quot;: f&quot;Grasp {target_obj['name']}&quot; } ]) else: action_plan.append({ &quot;action&quot;: &quot;report&quot;, &quot;target&quot;: f&quot;Could not find {grounded_command.target_object}&quot;, &quot;description&quot;: &quot;Report failure to find object&quot; }) elif grounded_command.action == Action.MOVE.value: # Navigate to target location action_plan.append({ &quot;action&quot;: &quot;navigate&quot;, &quot;target&quot;: grounded_command.target_location, &quot;description&quot;: f&quot;Navigate to {grounded_command.target_location}&quot; }) elif grounded_command.action == Action.PLACE.value: # This would involve both object and location target_obj = self._find_object_by_name( grounded_command.target_object, vision_results.get(&quot;objects&quot;, []) ) target_location = self._find_location_by_name( grounded_command.target_location, vision_results.get(&quot;locations&quot;, []) ) if target_obj and target_location: action_plan.extend([ { &quot;action&quot;: &quot;grasp&quot;, &quot;target&quot;: target_obj, &quot;description&quot;: f&quot;Grasp {target_obj['name']}&quot; }, { &quot;action&quot;: &quot;navigate&quot;, &quot;target&quot;: target_location[&quot;position&quot;], &quot;description&quot;: f&quot;Navigate to {target_location['name']}&quot; }, { &quot;action&quot;: &quot;place&quot;, &quot;target&quot;: target_location, &quot;description&quot;: f&quot;Place object at {target_location['name']}&quot; } ]) return action_plan def _find_object_by_name(self, name: str, objects: List[Dict]) -&gt; Optional[Dict]: &quot;&quot;&quot;Find an object by name from a list of detected objects&quot;&quot;&quot; for obj in objects: if obj.get(&quot;name&quot;, &quot;&quot;).lower() == name.lower(): return obj # Also check class names and aliases if obj.get(&quot;class&quot;, &quot;&quot;).lower() == name.lower(): return obj if obj.get(&quot;id&quot;, &quot;&quot;).lower() == name.lower(): return obj # If exact match not found, try partial matching for obj in objects: if name.lower() in obj.get(&quot;name&quot;, &quot;&quot;).lower(): return obj if name.lower() in obj.get(&quot;class&quot;, &quot;&quot;).lower(): return obj return None def _find_location_by_name(self, name: str, locations: List[Dict]) -&gt; Optional[Dict]: &quot;&quot;&quot;Find a location by name from a list of known locations&quot;&quot;&quot; for loc in locations: if loc.get(&quot;name&quot;, &quot;&quot;).lower() == name.lower(): return loc return None class ObjectGroundingSystem: def __init__(self): &quot;&quot;&quot;Initialize system for grounding linguistic references to visual objects&quot;&quot;&quot; self.similarity_threshold = 0.7 def ground_command_to_objects(self, command: RobotCommand, vision_results: Dict) -&gt; RobotCommand: &quot;&quot;&quot; Ground linguistic object references to actual visual objects Args: command: Parsed command with potentially ambiguous object references vision_results: Current visual scene description Returns: Command with grounded object references &quot;&quot;&quot; grounded_command = RobotCommand( action=command.action, target_object=command.target_object, target_location=command.target_location, attributes=command.attributes, confidence=command.confidence ) # Ground target object if specified if command.target_object: grounded_obj = self._ground_object_reference( command.target_object, vision_results.get(&quot;objects&quot;, []) ) if grounded_obj: grounded_command.target_object = grounded_obj # Ground target location if specified if command.target_location: grounded_loc = self._ground_location_reference( command.target_location, vision_results.get(&quot;locations&quot;, []) ) if grounded_loc: grounded_command.target_location = grounded_loc return grounded_command def _ground_object_reference(self, reference: str, objects: List[Dict]) -&gt; Optional[str]: &quot;&quot;&quot;Ground a linguistic object reference to a visual object&quot;&quot;&quot; # Try exact name matching first for obj in objects: if obj.get(&quot;name&quot;, &quot;&quot;).lower() == reference.lower(): return obj.get(&quot;name&quot;) # Try class matching for obj in objects: if obj.get(&quot;class&quot;, &quot;&quot;).lower() == reference.lower(): return obj.get(&quot;name&quot;) # Try attribute-based matching for obj in objects: obj_attributes = obj.get(&quot;attributes&quot;, {}) if self._match_attributes(reference, obj_attributes): return obj.get(&quot;name&quot;) # If no exact match, try similarity-based matching best_match = self._find_best_similarity_match(reference, objects) if best_match and best_match[1] &gt; self.similarity_threshold: return best_match[0] return None def _match_attributes(self, reference: str, obj_attributes: Dict) -&gt; bool: &quot;&quot;&quot;Check if object attributes match the reference&quot;&quot;&quot; # This would implement more sophisticated attribute matching # For now, a simple approach reference_lower = reference.lower() for attr_key, attr_value in obj_attributes.items(): if reference_lower in str(attr_value).lower(): return True if reference_lower in attr_key.lower(): return True return False def _find_best_similarity_match(self, reference: str, objects: List[Dict]) -&gt; Optional[Tuple[str, float]]: &quot;&quot;&quot;Find the object with the best similarity to the reference&quot;&quot;&quot; best_match = (None, 0.0) for obj in objects: name = obj.get(&quot;name&quot;, &quot;&quot;) obj_class = obj.get(&quot;class&quot;, &quot;&quot;) # Calculate similarity scores name_similarity = self._calculate_string_similarity(reference, name) class_similarity = self._calculate_string_similarity(reference, obj_class) max_similarity = max(name_similarity, class_similarity) if max_similarity &gt; best_match[1]: best_match = (obj.get(&quot;name&quot;, obj.get(&quot;id&quot;, &quot;unknown&quot;)), max_similarity) return best_match if best_match[1] &gt; 0 else None def _calculate_string_similarity(self, str1: str, str2: str) -&gt; float: &quot;&quot;&quot;Calculate similarity between two strings (simplified)&quot;&quot;&quot; if not str1 or not str2: return 0.0 # Convert to lowercase for comparison str1, str2 = str1.lower(), str2.lower() # Simple token-based similarity tokens1 = set(str1.split()) tokens2 = set(str2.split()) if not tokens1 and not tokens2: return 1.0 if not tokens1 or not tokens2: return 0.0 # Calculate Jaccard similarity intersection = tokens1.intersection(tokens2) union = tokens1.union(tokens2) jaccard = len(intersection) / len(union) return jaccard def _ground_location_reference(self, reference: str, locations: List[Dict]) -&gt; Optional[str]: &quot;&quot;&quot;Ground a linguistic location reference to a visual location&quot;&quot;&quot; for loc in locations: if loc.get(&quot;name&quot;, &quot;&quot;).lower() == reference.lower(): return loc.get(&quot;name&quot;) # Try partial matching for loc in locations: if reference.lower() in loc.get(&quot;name&quot;, &quot;&quot;).lower(): return loc.get(&quot;name&quot;) return None   ","version":"Next","tagName":"h3"},{"title":"Speech Processing and Natural Interaction​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#speech-processing-and-natural-interaction","content":" ","version":"Next","tagName":"h2"},{"title":"Speech-to-Text Integration​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#speech-to-text-integration","content":" import speech_recognition as sr import pyaudio from typing import Optional, Callable import threading class SpeechInterface: def __init__(self, language: str = &quot;en-US&quot;): &quot;&quot;&quot; Initialize speech interface for humanoid robot Args: language: Language code for speech recognition &quot;&quot;&quot; self.recognizer = sr.Recognizer() self.microphone = sr.Microphone() self.language = language # Adjust for ambient noise with self.microphone as source: self.recognizer.adjust_for_ambient_noise(source) # Callbacks self.command_callback: Optional[Callable] = None self.listening_callback: Optional[Callable] = None def listen_once(self) -&gt; Optional[str]: &quot;&quot;&quot; Listen for a single command and return the recognized text Returns: Recognized text or None if recognition failed &quot;&quot;&quot; try: with self.microphone as source: if self.listening_callback: self.listening_callback(True) print(&quot;Listening...&quot;) audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=10) # Use Google's speech recognition text = self.recognizer.recognize_google(audio, language=self.language) print(f&quot;Recognized: {text}&quot;) return text except sr.WaitTimeoutError: print(&quot;Timeout: No speech detected&quot;) return None except sr.UnknownValueError: print(&quot;Could not understand audio&quot;) return None except sr.RequestError as e: print(f&quot;Could not request results; {e}&quot;) return None finally: if self.listening_callback: self.listening_callback(False) def start_continuous_listening(self): &quot;&quot;&quot;Start continuous listening in a separate thread&quot;&quot;&quot; self.listening_thread = threading.Thread(target=self._continuous_listening_loop) self.listening_thread.daemon = True self.listening_thread.start() def _continuous_listening_loop(self): &quot;&quot;&quot;Main loop for continuous speech recognition&quot;&quot;&quot; with self.microphone as source: while True: try: print(&quot;Waiting for command...&quot;) if self.listening_callback: self.listening_callback(True) # Listen for audio audio = self.recognizer.listen(source, timeout=10, phrase_time_limit=10) # Recognize speech text = self.recognizer.recognize_google(audio, language=self.language) print(f&quot;Recognized: {text}&quot;) # Call command callback if available if self.command_callback: self.command_callback(text) except sr.WaitTimeoutError: # Continue waiting for speech continue except sr.UnknownValueError: # Could not understand, continue listening continue except sr.RequestError as e: print(f&quot;Recognition error: {e}&quot;) continue finally: if self.listening_callback: self.listening_callback(False) def set_command_callback(self, callback: Callable[[str], None]): &quot;&quot;&quot;Set callback for when a command is recognized&quot;&quot;&quot; self.command_callback = callback def set_listening_callback(self, callback: Callable[[bool], None]): &quot;&quot;&quot;Set callback for when listening state changes&quot;&quot;&quot; self.listening_callback = callback class TextToSpeech: def __init__(self): &quot;&quot;&quot;Initialize text-to-speech system&quot;&quot;&quot; try: from gtts import gTTS import pygame self.gTTS = gTTS self.pygame = pygame self.pygame.mixer.init() self.use_gtts = True except ImportError: # Fallback to espeak if gtts not available self.use_gtts = False print(&quot;gTTS not available, using fallback TTS&quot;) def speak(self, text: str, language: str = &quot;en&quot;): &quot;&quot;&quot; Convert text to speech and play it Args: text: Text to convert to speech language: Language code &quot;&quot;&quot; if self.use_gtts: try: # Create speech tts = self.gTTS(text=text, lang=language, slow=False) # Save to temporary file and play import io fp = io.BytesIO() tts.write_to_fp(fp) fp.seek(0) # Play the audio self.pygame.mixer.music.load(fp) self.pygame.mixer.music.play() # Wait for playback to finish while self.pygame.mixer.music.get_busy(): continue except Exception as e: print(f&quot;TTS error: {e}&quot;) else: # Fallback: print text to console print(f&quot;[TTS]: {text}&quot;) class HumanoidInteractionManager: def __init__(self, vision_system, language_system, action_system): &quot;&quot;&quot; Manage the complete interaction pipeline for humanoid robot Args: vision_system: Vision processing system language_system: Language understanding system action_system: Action execution system &quot;&quot;&quot; self.vision_system = vision_system self.language_system = language_system self.action_system = action_system # Initialize speech interface self.speech_interface = SpeechInterface() self.text_to_speech = TextToSpeech() # Initialize dialogue manager self.dialogue_manager = DialogueManager() # Initialize vision-language integrator self.vision_language_integrator = VisionLanguageIntegrator( vision_system, language_system ) def setup_interaction_callbacks(self): &quot;&quot;&quot;Set up callbacks for continuous interaction&quot;&quot;&quot; def command_callback(text: str): self.process_spoken_command(text) def listening_callback(listening: bool): # Update robot's listening state for visual feedback self.action_system.set_listening_state(listening) self.speech_interface.set_command_callback(command_callback) self.speech_interface.set_listening_callback(listening_callback) def process_spoken_command(self, command_text: str): &quot;&quot;&quot; Process a command received through speech Args: command_text: Natural language command from speech recognition &quot;&quot;&quot; try: # Get current world state world_state = self.vision_language_integrator.get_current_world_state() # Process through dialogue manager response = self.dialogue_manager.process_input(command_text, world_state) # Generate verbal response response_text = response.get(&quot;response_text&quot;, &quot;I understand.&quot;) self.text_to_speech.speak(response_text) # Execute action if specified action = response.get(&quot;action&quot;) if action: self._execute_action(action, world_state) except Exception as e: error_response = &quot;I encountered an error processing your command.&quot; self.text_to_speech.speak(error_response) print(f&quot;Error processing command: {e}&quot;) def _execute_action(self, action: Dict, world_state: Dict): &quot;&quot;&quot;Execute the specified action&quot;&quot;&quot; action_type = action.get(&quot;type&quot;) target = action.get(&quot;target&quot;) if action_type == &quot;navigate&quot;: self.action_system.navigate_to(target) elif action_type == &quot;manipulate&quot;: self.action_system.manipulate_object(target) elif action_type == &quot;social&quot;: self.action_system.perform_social_action(target) elif action_type == &quot;report&quot;: info = self._generate_report(action.get(&quot;action&quot;), world_state) self.text_to_speech.speak(info) else: self.text_to_speech.speak(&quot;I'm not sure how to perform that action.&quot;) def _generate_report(self, report_type: str, world_state: Dict) -&gt; str: &quot;&quot;&quot;Generate a verbal report based on world state&quot;&quot;&quot; if report_type == &quot;environment_status&quot;: objects = world_state.get(&quot;visible_objects&quot;, []) count = len(objects) if count == 0: return &quot;I don't see any objects around me.&quot; elif count == 1: return f&quot;I see one object: {objects[0].name}.&quot; else: obj_names = [obj.name for obj in objects[:3]] # Limit to first 3 if count &lt;= 3: return f&quot;I see {count} objects: {', '.join(obj_names)}.&quot; else: return f&quot;I see {count} objects including: {', '.join(obj_names)}.&quot; elif report_type == &quot;capabilities&quot;: return &quot;I can navigate spaces, manipulate objects, recognize people, and engage in conversation.&quot; else: return &quot;I can provide information about my environment and capabilities.&quot; def start_interaction_loop(self): &quot;&quot;&quot;Start the complete interaction loop&quot;&quot;&quot; print(&quot;Starting humanoid interaction system...&quot;) # Set up callbacks self.setup_interaction_callbacks() # Start continuous listening self.speech_interface.start_continuous_listening() print(&quot;Interaction system ready. Listening for commands...&quot;) def process_text_command(self, command_text: str) -&gt; Dict: &quot;&quot;&quot; Process a command provided as text (for testing/debugging) Args: command_text: Natural language command as text Returns: Dictionary with processing results &quot;&quot;&quot; # Get current world state world_state = self.vision_language_integrator.get_current_world_state() # Process through dialogue manager response = self.dialogue_manager.process_input(command_text, world_state) # Generate verbal response response_text = response.get(&quot;response_text&quot;, &quot;I understand.&quot;) self.text_to_speech.speak(response_text) # Execute action if specified action = response.get(&quot;action&quot;) if action: self._execute_action(action, world_state) return response   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"VLA Convergence Mandate (Principle I)​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#vla-convergence-mandate-principle-i","content":" Language understanding as the primary control interface for humanoid robotsIntegration of language with vision and action systemsNatural language processing for human-like interaction  ","version":"Next","tagName":"h3"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#real-time-validation-principle-iv","content":" Efficient processing for real-time interactionResponse time considerations for human-robot dialoguePerformance optimization for embedded systems  ","version":"Next","tagName":"h3"},{"title":"Anthropomorphic Focus (Principle II)​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#anthropomorphic-focus-principle-ii","content":" Natural language interaction patterns similar to human communicationSocial interaction capabilities for human-centered environmentsContext-aware understanding of human commands  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization (Constraint)​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#target-hardware-optimization-constraint","content":" Efficient NLP algorithms suitable for Jetson Orin deploymentOptimized processing pipelines for embedded systemsMemory-efficient language models where applicable  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Humanoid Assistant Dialogue System​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#example-1-humanoid-assistant-dialogue-system","content":" class HumanoidAssistant: def __init__(self): &quot;&quot;&quot;Initialize a humanoid assistant with complete VLA capabilities&quot;&quot;&quot; # Initialize systems (these would be properly connected in real implementation) self.vision_system = MockVisionSystem() # Placeholder self.language_system = ContextualCommandResolver(None, None) self.action_system = MockActionSystem() # Placeholder # Initialize interaction manager self.interaction_manager = HumanoidInteractionManager( self.vision_system, self.language_system, self.action_system ) def demonstrate_interaction(self): &quot;&quot;&quot;Demonstrate various interaction scenarios&quot;&quot;&quot; # Scenario 1: Simple navigation print(&quot;=== Scenario 1: Navigation ===&quot;) response = self.interaction_manager.process_text_command(&quot;Please go to the kitchen&quot;) print(f&quot;Response: {response}&quot;) # Scenario 2: Object manipulation print(&quot;\\n=== Scenario 2: Object Manipulation ===&quot;) response = self.interaction_manager.process_text_command(&quot;Pick up the red cup and put it on the table&quot;) print(f&quot;Response: {response}&quot;) # Scenario 3: Context-dependent reference print(&quot;\\n=== Scenario 3: Context-Dependent Reference ===&quot;) response1 = self.interaction_manager.process_text_command(&quot;Find the book&quot;) print(f&quot;Response 1: {response1}&quot;) response2 = self.interaction_manager.process_text_command(&quot;Now put it on the shelf&quot;) print(f&quot;Response 2: {response2}&quot;) # Scenario 4: Social interaction print(&quot;\\n=== Scenario 4: Social Interaction ===&quot;) response = self.interaction_manager.process_text_command(&quot;Hello, how are you?&quot;) print(f&quot;Response: {response}&quot;) class MockVisionSystem: &quot;&quot;&quot;Mock vision system for demonstration purposes&quot;&quot;&quot; def get_current_scene_description(self): return { &quot;objects&quot;: [ {&quot;id&quot;: &quot;obj1&quot;, &quot;name&quot;: &quot;red cup&quot;, &quot;class&quot;: &quot;cup&quot;, &quot;bbox&quot;: [100, 100, 50, 50]}, {&quot;id&quot;: &quot;obj2&quot;, &quot;name&quot;: &quot;book&quot;, &quot;class&quot;: &quot;book&quot;, &quot;bbox&quot;: [200, 150, 80, 100]}, {&quot;id&quot;: &quot;obj3&quot;, &quot;name&quot;: &quot;table&quot;, &quot;class&quot;: &quot;table&quot;, &quot;bbox&quot;: [0, 300, 400, 200]} ], &quot;locations&quot;: [ {&quot;name&quot;: &quot;kitchen&quot;, &quot;position&quot;: [2.0, 1.0, 0.0]}, {&quot;name&quot;: &quot;living room&quot;, &quot;position&quot;: [0.0, 0.0, 0.0]}, {&quot;name&quot;: &quot;shelf&quot;, &quot;position&quot;: [1.5, 0.5, 0.0]} ] } def get_robot_position(self): return [0.0, 0.0, 0.0] def get_traversable_map(self): return np.ones((10, 10)) class MockActionSystem: &quot;&quot;&quot;Mock action system for demonstration purposes&quot;&quot;&quot; def navigate_to(self, target): print(f&quot;Navigating to: {target}&quot;) def manipulate_object(self, target): print(f&quot;Manipulating: {target}&quot;) def perform_social_action(self, target): print(f&quot;Performing social action: {target}&quot;) def set_listening_state(self, listening): print(f&quot;Listening state: {listening}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Example 2: Multi-Turn Dialogue Management​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#example-2-multi-turn-dialogue-management","content":" class MultiTurnDialogueManager: def __init__(self): self.context = { &quot;current_task&quot;: None, &quot;task_objects&quot;: [], &quot;task_locations&quot;: [], &quot;conversation_history&quot;: [], &quot;user_preferences&quot;: {} } self.language_understanding = IntentRecognizer() self.response_generator = ResponseGenerator() def process_multi_turn_dialogue(self, user_input: str, current_world_state: Dict) -&gt; Dict: &quot;&quot;&quot; Process multi-turn dialogue with context maintenance &quot;&quot;&quot; # Update conversation history self.context[&quot;conversation_history&quot;].append({ &quot;speaker&quot;: &quot;user&quot;, &quot;text&quot;: user_input, &quot;timestamp&quot;: time.time() }) # Process the input intent_result = self.language_understanding.process_command_with_context( user_input, self.context ) # Handle task-specific contexts task_response = self._handle_task_context(intent_result, current_world_state) # Generate natural response response = self.response_generator.generate_response( intent_result, current_world_state, self.context ) # Update context self.context[&quot;conversation_history&quot;].append({ &quot;speaker&quot;: &quot;robot&quot;, &quot;text&quot;: response[&quot;response_text&quot;], &quot;timestamp&quot;: time.time(), &quot;intent&quot;: intent_result[&quot;intent&quot;] }) return { &quot;response&quot;: response[&quot;response_text&quot;], &quot;intent&quot;: intent_result[&quot;intent&quot;], &quot;task_continuation&quot;: task_response, &quot;context_updated&quot;: True } def _handle_task_context(self, intent_result: Dict, world_state: Dict) -&gt; Optional[Dict]: &quot;&quot;&quot;Handle context for ongoing tasks&quot;&quot;&quot; current_task = self.context.get(&quot;current_task&quot;) if current_task: # If we're in the middle of a task, check if the user input is related if intent_result[&quot;intent&quot;] == &quot;confirmation&quot;: return self._handle_confirmation(current_task, intent_result) elif intent_result[&quot;intent&quot;] == &quot;correction&quot;: return self._handle_correction(current_task, intent_result) elif intent_result[&quot;intent&quot;] == &quot;abandon_task&quot;: self.context[&quot;current_task&quot;] = None return {&quot;action&quot;: &quot;task_abandoned&quot;, &quot;message&quot;: &quot;Task abandoned as requested.&quot;} # If this is a new task, set it as current if intent_result[&quot;intent&quot;] in [&quot;object_manipulation&quot;, &quot;navigation&quot;]: self.context[&quot;current_task&quot;] = { &quot;intent&quot;: intent_result[&quot;intent&quot;], &quot;entities&quot;: intent_result[&quot;entities&quot;], &quot;start_time&quot;: time.time() } return None def _handle_confirmation(self, current_task: Dict, intent_result: Dict) -&gt; Dict: &quot;&quot;&quot;Handle user confirmation of robot's interpretation&quot;&quot;&quot; if current_task: return { &quot;action&quot;: &quot;task_confirmed&quot;, &quot;message&quot;: f&quot;Proceeding with {current_task['intent']} task.&quot;, &quot;task&quot;: current_task } return {&quot;action&quot;: &quot;no_task_to_confirm&quot;, &quot;message&quot;: &quot;No current task to confirm.&quot;} def _handle_correction(self, current_task: Dict, intent_result: Dict) -&gt; Dict: &quot;&quot;&quot;Handle user corrections to robot's interpretation&quot;&quot;&quot; # Extract correction information correction_entities = intent_result.get(&quot;entities&quot;, {}) # Update current task with corrections if &quot;target_object&quot; in correction_entities: current_task[&quot;entities&quot;][&quot;target_object&quot;] = correction_entities[&quot;target_object&quot;] if &quot;target_location&quot; in correction_entities: current_task[&quot;entities&quot;][&quot;target_location&quot;] = correction_entities[&quot;target_location&quot;] return { &quot;action&quot;: &quot;task_corrected&quot;, &quot;message&quot;: f&quot;Task updated based on your correction.&quot;, &quot;task&quot;: current_task }   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Language Understanding Pipeline​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#exercise-1-language-understanding-pipeline","content":" Implement a complete language understanding pipeline that:  Parses natural language commands into structured representationsResolves ambiguous references using visual contextIntegrates with vision and action systemsHandles multi-turn dialogue with context maintenance  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Context-Aware Resolution​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#exercise-2-context-aware-resolution","content":" Create a system that resolves linguistic references by:  Identifying pronouns and demonstratives in commandsUsing visual input to ground ambiguous referencesMaintaining conversation context across turnsHandling corrections and clarifications naturally  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Real-Time Interaction​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#exercise-3-real-time-interaction","content":" Develop a real-time interaction system that:  Processes speech input continuouslyResponds to commands within 2-3 secondsProvides appropriate verbal feedbackIntegrates seamlessly with vision and action systems  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#summary","content":" Language understanding in robotics, particularly for humanoid robots, requires specialized approaches that go beyond traditional NLP. The Vision-Language-Action pipeline demands that language be grounded in the physical environment, with real-time processing capabilities and natural interaction patterns. For humanoid robots operating in human-centered environments, language systems must handle ambiguity, maintain context across conversations, and integrate seamlessly with perception and action systems. The primary control interface through natural language, as mandated by our constitution, requires sophisticated understanding of human communication patterns and the ability to translate high-level linguistic goals into concrete robotic behaviors.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 15 - Language Understanding in Robotics","url":"/docs/chapters/module-4-vla/chapter-15-language-understanding#further-reading","content":" &quot;Language and Robots&quot; by Tellex and Roy&quot;Grounded Language Learning and Processing&quot; by Gorniak&quot;Human-Robot Interaction: A Survey&quot; by Goodrich and Schultz&quot;Natural Language Processing for Human-Robot Interaction&quot; - Recent research papers&quot;Situated Language Understanding for Robotics&quot; - ACL and ICRA proceedings ","version":"Next","tagName":"h2"},{"title":"Tutorial Intro","type":0,"sectionRef":"#","url":"/docs/intro","content":"","keywords":"","version":"Next"},{"title":"Getting Started​","type":1,"pageTitle":"Tutorial Intro","url":"/docs/intro#getting-started","content":" Get started by creating a new site.  Or try Docusaurus immediately with docusaurus.new.  ","version":"Next","tagName":"h2"},{"title":"What you'll need​","type":1,"pageTitle":"Tutorial Intro","url":"/docs/intro#what-youll-need","content":" Node.js version 20.0 or above: When installing Node.js, you are recommended to check all checkboxes related to dependencies.  ","version":"Next","tagName":"h3"},{"title":"Generate a new site​","type":1,"pageTitle":"Tutorial Intro","url":"/docs/intro#generate-a-new-site","content":" Generate a new Docusaurus site using the classic template.  The classic template will automatically be added to your project after you run the command:  npm init docusaurus@latest my-website classic   You can type this command into Command Prompt, Powershell, Terminal, or any other integrated terminal of your code editor.  The command also installs all necessary dependencies you need to run Docusaurus.  ","version":"Next","tagName":"h2"},{"title":"Start your site​","type":1,"pageTitle":"Tutorial Intro","url":"/docs/intro#start-your-site","content":" Run the development server:  cd my-website npm run start   The cd command changes the directory you're working with. In order to work with your newly created Docusaurus site, you'll need to navigate the terminal there.  The npm run start command builds your website locally and serves it through a development server, ready for you to view at http://localhost:3000/.  Open docs/intro.md (this page) and edit some lines: the site reloads automatically and displays your changes. ","version":"Next","tagName":"h2"},{"title":"Congratulations!","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/congratulations","content":"","keywords":"","version":"Next"},{"title":"What's next?​","type":1,"pageTitle":"Congratulations!","url":"/docs/tutorial-basics/congratulations#whats-next","content":" Read the official documentationModify your site configuration with docusaurus.config.jsAdd navbar and footer items with themeConfigAdd a custom Design and LayoutAdd a search barFind inspirations in the Docusaurus showcaseGet involved in the Docusaurus Community ","version":"Next","tagName":"h2"},{"title":"Create a Blog Post","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/create-a-blog-post","content":"","keywords":"","version":"Next"},{"title":"Create your first Post​","type":1,"pageTitle":"Create a Blog Post","url":"/docs/tutorial-basics/create-a-blog-post#create-your-first-post","content":" Create a file at blog/2021-02-28-greetings.md:  blog/2021-02-28-greetings.md --- slug: greetings title: Greetings! authors: - name: Joel Marcey title: Co-creator of Docusaurus 1 url: https://github.com/JoelMarcey image_url: https://github.com/JoelMarcey.png - name: Sébastien Lorber title: Docusaurus maintainer url: https://sebastienlorber.com image_url: https://github.com/slorber.png tags: [greetings] --- Congratulations, you have made your first post! Feel free to play around and edit this post as much as you like.   A new blog post is now available at http://localhost:3000/blog/greetings. ","version":"Next","tagName":"h2"},{"title":"Create a Document","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/create-a-document","content":"","keywords":"","version":"Next"},{"title":"Create your first Doc​","type":1,"pageTitle":"Create a Document","url":"/docs/tutorial-basics/create-a-document#create-your-first-doc","content":" Create a Markdown file at docs/hello.md:  docs/hello.md # Hello This is my **first Docusaurus document**!   A new document is now available at http://localhost:3000/docs/hello.  ","version":"Next","tagName":"h2"},{"title":"Configure the Sidebar​","type":1,"pageTitle":"Create a Document","url":"/docs/tutorial-basics/create-a-document#configure-the-sidebar","content":" Docusaurus automatically creates a sidebar from the docs folder.  Add metadata to customize the sidebar label and position:  docs/hello.md --- sidebar_label: 'Hi!' sidebar_position: 3 --- # Hello This is my **first Docusaurus document**!   It is also possible to create your sidebar explicitly in sidebars.js:  sidebars.js export default { tutorialSidebar: [ 'intro', 'hello', { type: 'category', label: 'Tutorial', items: ['tutorial-basics/create-a-document'], }, ], };  ","version":"Next","tagName":"h2"},{"title":"Create a Page","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/create-a-page","content":"","keywords":"","version":"Next"},{"title":"Create your first React Page​","type":1,"pageTitle":"Create a Page","url":"/docs/tutorial-basics/create-a-page#create-your-first-react-page","content":" Create a file at src/pages/my-react-page.js:  src/pages/my-react-page.js import React from 'react'; import Layout from '@theme/Layout'; export default function MyReactPage() { return ( &lt;Layout&gt; &lt;h1&gt;My React page&lt;/h1&gt; &lt;p&gt;This is a React page&lt;/p&gt; &lt;/Layout&gt; ); }   A new page is now available at http://localhost:3000/my-react-page.  ","version":"Next","tagName":"h2"},{"title":"Create your first Markdown Page​","type":1,"pageTitle":"Create a Page","url":"/docs/tutorial-basics/create-a-page#create-your-first-markdown-page","content":" Create a file at src/pages/my-markdown-page.md:  src/pages/my-markdown-page.md # My Markdown page This is a Markdown page   A new page is now available at http://localhost:3000/my-markdown-page. ","version":"Next","tagName":"h2"},{"title":"Deploy your site","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/deploy-your-site","content":"","keywords":"","version":"Next"},{"title":"Build your site​","type":1,"pageTitle":"Deploy your site","url":"/docs/tutorial-basics/deploy-your-site#build-your-site","content":" Build your site for production:  npm run build   The static files are generated in the build folder.  ","version":"Next","tagName":"h2"},{"title":"Deploy your site​","type":1,"pageTitle":"Deploy your site","url":"/docs/tutorial-basics/deploy-your-site#deploy-your-site-1","content":" Test your production build locally:  npm run serve   The build folder is now served at http://localhost:3000/.  You can now deploy the build folder almost anywhere easily, for free or very small cost (read the Deployment Guide). ","version":"Next","tagName":"h2"},{"title":"Chapter 16 - Action Planning & Control Systems","type":0,"sectionRef":"#","url":"/docs/chapters/module-4-vla/chapter-16-action-planning","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#learning-objectives","content":" Implement action planning for humanoid robotsDesign control systems for humanoid locomotion and manipulationIntegrate action planning with vision and language systems  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#introduction","content":" Action planning and control systems form the Action component of the Vision-Language-Action (VLA) pipeline, completing the cycle that begins with language understanding and is informed by visual perception. For humanoid robots, action planning must address the unique challenges of bipedal locomotion, dexterous manipulation, and real-time control with strict timing constraints as mandated by our Real-Time Validation principle. This chapter explores action planning and control systems specifically designed for humanoid robots, with emphasis on the integration with vision and language systems to create a cohesive VLA pipeline that enables natural human-robot interaction in human-centered environments.  ","version":"Next","tagName":"h2"},{"title":"Action Planning Fundamentals​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#action-planning-fundamentals","content":" ","version":"Next","tagName":"h2"},{"title":"Hierarchical Action Planning Architecture​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#hierarchical-action-planning-architecture","content":" Humanoid robots require multi-level action planning to handle the complexity of their tasks:  graph TD A[High-Level Goals] --&gt; B[Task Planning] B --&gt; C[Motion Planning] C --&gt; D[Control Planning] D --&gt; E[Low-Level Control] E --&gt; F[Physical Execution] G[Vision Input] --&gt; B G --&gt; C G --&gt; D H[Language Input] --&gt; B I[Current State] --&gt; B I --&gt; C I --&gt; D   ","version":"Next","tagName":"h3"},{"title":"Action Representation and Planning​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#action-representation-and-planning","content":" from typing import List, Dict, Optional, Tuple, Any from dataclasses import dataclass from enum import Enum import numpy as np import time class ActionType(Enum): &quot;&quot;&quot;Enumeration of different action types for humanoid robots&quot;&quot;&quot; NAVIGATION = &quot;navigation&quot; MANIPULATION = &quot;manipulation&quot; LOCOMOTION = &quot;locomotion&quot; BALANCE = &quot;balance&quot; INTERACTION = &quot;interaction&quot; PERCEPTION = &quot;perception&quot; WAIT = &quot;wait&quot; @dataclass class ActionStep: &quot;&quot;&quot;Represents a single step in an action plan&quot;&quot;&quot; action_type: ActionType parameters: Dict[str, Any] duration: float # Expected duration in seconds preconditions: List[str] # Conditions that must be true before execution effects: List[str] # Effects that will be true after execution priority: int = 1 # Higher number means higher priority @dataclass class ActionPlan: &quot;&quot;&quot;Represents a complete action plan&quot;&quot;&quot; steps: List[ActionStep] start_time: float estimated_duration: float success_conditions: List[str] failure_conditions: List[str] class ActionPlanner: def __init__(self, robot_config: Dict): &quot;&quot;&quot; Initialize action planner for humanoid robot Args: robot_config: Configuration parameters for the specific robot &quot;&quot;&quot; self.robot_config = robot_config self.kinematic_model = self._load_kinematic_model() self.collision_checker = CollisionChecker() self.trajectory_generator = TrajectoryGenerator() def plan_navigation_action(self, start_pose: np.ndarray, goal_pose: np.ndarray, world_map: np.ndarray) -&gt; ActionPlan: &quot;&quot;&quot; Plan navigation action from start to goal Args: start_pose: Starting pose [x, y, theta] goal_pose: Goal pose [x, y, theta] world_map: 2D occupancy grid map Returns: Action plan for navigation &quot;&quot;&quot; # Plan path using A* or RRT path = self._plan_path(start_pose, goal_pose, world_map) # Generate trajectory for the path trajectory = self.trajectory_generator.generate_path_trajectory( path, self.robot_config['max_velocity'], self.robot_config['max_acceleration'] ) # Create action steps steps = [] for waypoint in trajectory: step = ActionStep( action_type=ActionType.LOCOMOTION, parameters={ 'target_pose': waypoint, 'walking_speed': self.robot_config.get('cruising_speed', 0.5) }, duration=0.1, # 100ms per waypoint preconditions=['robot_is_standing', 'path_is_clear'], effects=['robot_position_updated'], priority=2 ) steps.append(step) # Add final balance adjustment balance_step = ActionStep( action_type=ActionType.BALANCE, parameters={'target_pose': goal_pose}, duration=1.0, preconditions=['robot_has_reached_goal'], effects=['robot_is_balanced'], priority=3 ) steps.append(balance_step) return ActionPlan( steps=steps, start_time=time.time(), estimated_duration=len(trajectory) * 0.1 + 1.0, success_conditions=['robot_at_goal_pose', 'robot_balanced'], failure_conditions=['collision_detected', 'time_limit_exceeded'] ) def plan_manipulation_action(self, target_object: Dict, target_pose: np.ndarray, current_state: Dict) -&gt; ActionPlan: &quot;&quot;&quot; Plan manipulation action for grasping or placing an object Args: target_object: Dictionary describing the target object target_pose: Target pose for manipulation [x, y, z, roll, pitch, yaw] current_state: Current robot state Returns: Action plan for manipulation &quot;&quot;&quot; steps = [] # First, navigate to the object if needed if self._needs_navigation_to_object(target_object, current_state): navigate_plan = self._create_navigation_to_object_plan(target_object, current_state) steps.extend(navigate_plan.steps) # Plan reaching motion reach_steps = self._plan_reaching_motion(target_object, target_pose, current_state) steps.extend(reach_steps) # Plan grasp or place action grasp_steps = self._plan_grasp_or_place(target_object, current_state) steps.extend(grasp_steps) # Return to neutral position return_steps = self._plan_return_to_neutral(current_state) steps.extend(return_steps) return ActionPlan( steps=steps, start_time=time.time(), estimated_duration=sum(step.duration for step in steps), success_conditions=['object_manipulated', 'robot_safe_position'], failure_conditions=['collision_detected', 'grasp_failed', 'time_limit_exceeded'] ) def _plan_path(self, start_pose: np.ndarray, goal_pose: np.ndarray, world_map: np.ndarray) -&gt; List[np.ndarray]: &quot;&quot;&quot;Plan a collision-free path using A* algorithm&quot;&quot;&quot; # Implementation of A* path planning # This is a simplified version - in practice, would use more sophisticated algorithms path = [start_pose] # Calculate straight-line path first dx = goal_pose[0] - start_pose[0] dy = goal_pose[1] - start_pose[1] distance = np.sqrt(dx**2 + dy**2) # Generate intermediate waypoints steps = max(10, int(distance / 0.1)) # 10cm resolution for i in range(1, steps + 1): t = i / steps intermediate_pose = start_pose + t * (goal_pose - start_pose) intermediate_pose[2] = start_pose[2] + t * (goal_pose[2] - start_pose[2]) # Interpolate orientation # Check for collisions if not self.collision_checker.is_collision_at(intermediate_pose, world_map): path.append(intermediate_pose) else: # Implement path replanning in case of collision # For now, just skip collision points continue # Ensure we reach the goal if not np.allclose(path[-1][:2], goal_pose[:2], atol=0.05): # 5cm tolerance path.append(goal_pose) return path def _needs_navigation_to_object(self, target_object: Dict, current_state: Dict) -&gt; bool: &quot;&quot;&quot;Determine if navigation is needed to reach the target object&quot;&quot;&quot; current_pos = np.array(current_state['position']) object_pos = np.array(target_object['position']) distance = np.linalg.norm(current_pos[:2] - object_pos[:2]) reach_distance = self.robot_config.get('arm_reach', 1.0) return distance &gt; reach_distance def _create_navigation_to_object_plan(self, target_object: Dict, current_state: Dict) -&gt; ActionPlan: &quot;&quot;&quot;Create navigation plan to approach target object&quot;&quot;&quot; object_pos = np.array(target_object['position']) approach_pos = object_pos.copy() # Calculate approach position (slightly away from object for safety) current_pos = np.array(current_state['position']) approach_direction = object_pos[:2] - current_pos[:2] approach_direction = approach_direction / np.linalg.norm(approach_direction) # 30cm from object approach_pos[:2] = object_pos[:2] - approach_direction * 0.3 approach_pos[2] = current_pos[2] # Keep same height # Use current orientation approach_pos = np.append(approach_pos, current_state['orientation'][-1]) return self.plan_navigation_action( current_state['position'], approach_pos, current_state.get('world_map', np.ones((100, 100))) # Default map ) def _plan_reaching_motion(self, target_object: Dict, target_pose: np.ndarray, current_state: Dict) -&gt; List[ActionStep]: &quot;&quot;&quot;Plan the reaching motion for manipulation&quot;&quot;&quot; steps = [] # Calculate inverse kinematics for target pose joint_angles = self.kinematic_model.inverse_kinematics(target_pose) # Generate smooth trajectory to target joint angles current_joints = current_state['joint_positions'] trajectory = self.trajectory_generator.generate_joint_trajectory( current_joints, joint_angles, self.robot_config['max_joint_velocity'] ) for joint_config in trajectory: step = ActionStep( action_type=ActionType.MANIPULATION, parameters={ 'joint_positions': joint_config, 'control_mode': 'position' }, duration=0.05, # 50ms per step preconditions=['arm_is_moving_to_target'], effects=['joint_positions_updated'], priority=2 ) steps.append(step) return steps def _plan_grasp_or_place(self, target_object: Dict, current_state: Dict) -&gt; List[ActionStep]: &quot;&quot;&quot;Plan the actual grasp or place action&quot;&quot;&quot; steps = [] # Determine if this is a grasp or place based on current state is_grasping = target_object.get('state', 'placed') == 'placed' if is_grasping: # Grasp action grasp_step = ActionStep( action_type=ActionType.MANIPULATION, parameters={ 'gripper_command': 'close', 'grasp_force': target_object.get('weight', 1.0) * 2 # 2x weight for safety }, duration=1.0, preconditions=['end_effector_at_object', 'gripper_open'], effects=['object_grasped', 'gripper_closed'], priority=3 ) else: # Place action place_step = ActionStep( action_type=ActionType.MANIPULATION, parameters={ 'gripper_command': 'open', 'release_force': 0.0 }, duration=0.5, preconditions=['object_grasped', 'at_place_location'], effects=['object_placed', 'gripper_open'], priority=3 ) grasp_step = place_step steps.append(grasp_step) # Verify grasp/place success verification_step = ActionStep( action_type=ActionType.PERCEPTION, parameters={'sensor_check': 'force_torque'}, duration=0.1, preconditions=['grasp_action_completed'], effects=['grasp_success_verified'], priority=4 ) steps.append(verification_step) return steps def _plan_return_to_neutral(self, current_state: Dict) -&gt; List[ActionStep]: &quot;&quot;&quot;Plan return to neutral/safe position&quot;&quot;&quot; steps = [] neutral_joints = self.robot_config.get('neutral_joints', current_state['joint_positions']) current_joints = current_state['joint_positions'] trajectory = self.trajectory_generator.generate_joint_trajectory( current_joints, neutral_joints, self.robot_config['max_joint_velocity'] / 2 # Slower return ) for joint_config in trajectory: step = ActionStep( action_type=ActionType.MANIPULATION, parameters={ 'joint_positions': joint_config, 'control_mode': 'position' }, duration=0.05, preconditions=['manipulation_completed'], effects=['joint_positions_updated'], priority=1 ) steps.append(step) return steps def _load_kinematic_model(self): &quot;&quot;&quot;Load or create kinematic model for the robot&quot;&quot;&quot; # In practice, this would load from URDF or DH parameters return KinematicModel(self.robot_config) class KinematicModel: &quot;&quot;&quot;Simple kinematic model for humanoid robot&quot;&quot;&quot; def __init__(self, robot_config: Dict): self.config = robot_config def inverse_kinematics(self, target_pose: np.ndarray) -&gt; np.ndarray: &quot;&quot;&quot;Calculate joint angles for desired end-effector pose&quot;&quot;&quot; # Simplified inverse kinematics # In practice, would use more sophisticated methods like FABRIK, CCD, or analytical solutions # This is a placeholder implementation return np.zeros(self.config.get('num_joints', 6)) # Return zero joints as placeholder def forward_kinematics(self, joint_angles: np.ndarray) -&gt; np.ndarray: &quot;&quot;&quot;Calculate end-effector pose from joint angles&quot;&quot;&quot; # Simplified forward kinematics return np.zeros(6) # Return zero pose as placeholder class CollisionChecker: &quot;&quot;&quot;Simple collision checker for path planning&quot;&quot;&quot; def __init__(self): pass def is_collision_at(self, pose: np.ndarray, world_map: np.ndarray) -&gt; bool: &quot;&quot;&quot;Check if there's a collision at the given pose&quot;&quot;&quot; # Convert world coordinates to map coordinates map_x = int(pose[0] / 0.05) # Assuming 5cm resolution map_y = int(pose[1] / 0.05) if 0 &lt;= map_x &lt; world_map.shape[1] and 0 &lt;= map_y &lt; world_map.shape[0]: return world_map[map_y, map_x] &gt; 0.5 # Occupied if value &gt; 0.5 else: return True # Out of bounds is considered collision class TrajectoryGenerator: &quot;&quot;&quot;Generate smooth trajectories for motion planning&quot;&quot;&quot; def __init__(self): pass def generate_path_trajectory(self, path: List[np.ndarray], max_velocity: float, max_acceleration: float) -&gt; List[np.ndarray]: &quot;&quot;&quot;Generate smooth trajectory through waypoints&quot;&quot;&quot; # This would implement trajectory generation with velocity and acceleration limits # For now, return the path as is return path def generate_joint_trajectory(self, start_joints: np.ndarray, end_joints: np.ndarray, max_velocity: float) -&gt; List[np.ndarray]: &quot;&quot;&quot;Generate smooth joint trajectory&quot;&quot;&quot; # Linear interpolation between start and end joint positions steps = 20 # Number of intermediate steps trajectory = [] for i in range(steps + 1): t = i / steps joint_pos = start_joints + t * (end_joints - start_joints) trajectory.append(joint_pos) return trajectory   ","version":"Next","tagName":"h3"},{"title":"Control Systems for Humanoid Robots​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#control-systems-for-humanoid-robots","content":" ","version":"Next","tagName":"h2"},{"title":"Balance Control Systems​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#balance-control-systems","content":" Humanoid robots require sophisticated balance control systems due to the inherent instability of bipedal locomotion:  import control # Python Control Systems Library from scipy import signal class BalanceController: def __init__(self, robot_config: Dict): &quot;&quot;&quot; Initialize balance controller for humanoid robot Args: robot_config: Configuration parameters for the robot &quot;&quot;&quot; self.robot_config = robot_config self.mass = robot_config.get('total_mass', 70.0) # kg self.height = robot_config.get('height', 1.6) # meters self.com_height = robot_config.get('com_height', 0.8) # Center of mass height # Initialize control parameters self.kp = robot_config.get('balance_kp', 100.0) # Proportional gain self.kd = robot_config.get('balance_kd', 10.0) # Derivative gain self.ki = robot_config.get('balance_ki', 1.0) # Integral gain # Initialize PID controllers for different axes self.pitch_controller = self._create_pid_controller(self.kp, self.kd, self.ki) self.roll_controller = self._create_pid_controller(self.kp, self.kd, self.ki) self.z_controller = self._create_pid_controller(self.kp/10, self.kd/10, self.ki/10) # Less aggressive for height # State variables self.prev_pitch_error = 0.0 self.prev_roll_error = 0.0 self.integrated_pitch_error = 0.0 self.integrated_roll_error = 0.0 self.control_output_history = [] def _create_pid_controller(self, kp: float, kd: float, ki: float): &quot;&quot;&quot;Create a PID controller with given parameters&quot;&quot;&quot; # This is a simplified implementation # In practice, would use proper control library functions return { 'kp': kp, 'kd': kd, 'ki': ki, 'prev_error': 0.0, 'integrated_error': 0.0 } def update_balance_control(self, current_state: Dict, target_state: Dict) -&gt; Dict: &quot;&quot;&quot; Update balance control based on current and target states Args: current_state: Current robot state including IMU readings target_state: Target state for balance Returns: Control commands to maintain balance &quot;&quot;&quot; # Extract current state current_pitch = current_state.get('pitch', 0.0) current_roll = current_state.get('roll', 0.0) current_z = current_state.get('z_position', self.com_height) # Extract target state target_pitch = target_state.get('pitch', 0.0) target_roll = target_state.get('roll', 0.0) target_z = target_state.get('z_position', self.com_height) # Calculate errors pitch_error = target_pitch - current_pitch roll_error = target_roll - current_roll z_error = target_z - current_z # Update integrated errors (with anti-windup) self.integrated_pitch_error = self._limit_integration( self.integrated_pitch_error + pitch_error * 0.01, # Assuming 100Hz control rate -1.0, 1.0 ) self.integrated_roll_error = self._limit_integration( self.integrated_roll_error + roll_error * 0.01, -1.0, 1.0 ) # Calculate control outputs using PID pitch_control = (self.kp * pitch_error + self.kd * (pitch_error - self.prev_pitch_error) / 0.01 + self.ki * self.integrated_pitch_error) roll_control = (self.kp * roll_error + self.kd * (roll_error - self.prev_roll_error) / 0.01 + self.ki * self.integrated_roll_error) # Update previous errors self.prev_pitch_error = pitch_error self.prev_roll_error = roll_error # Limit control outputs to safe ranges pitch_control = self._limit_output(pitch_control, -100.0, 100.0) roll_control = self._limit_output(roll_control, -100.0, 100.0) # Calculate Z control separately (for height adjustment) z_control = self.kp * z_error * 0.1 # Reduced gain for height z_control = self._limit_output(z_control, -50.0, 50.0) # Package control outputs control_commands = { 'pitch_torque': pitch_control, 'roll_torque': roll_control, 'z_force': z_control, 'joint_commands': self._map_to_joint_commands(pitch_control, roll_control, z_control) } # Store for history and debugging self.control_output_history.append({ 'timestamp': time.time(), 'errors': {'pitch': pitch_error, 'roll': roll_error, 'z': z_error}, 'outputs': control_commands }) return control_commands def _limit_integration(self, value: float, min_val: float, max_val: float) -&gt; float: &quot;&quot;&quot;Limit the integrated error to prevent windup&quot;&quot;&quot; return max(min_val, min(max_val, value)) def _limit_output(self, value: float, min_val: float, max_val: float) -&gt; float: &quot;&quot;&quot;Limit control output to safe range&quot;&quot;&quot; return max(min_val, min(max_val, value)) def _map_to_joint_commands(self, pitch_control: float, roll_control: float, z_control: float) -&gt; Dict[str, float]: &quot;&quot;&quot;Map balance control outputs to individual joint commands&quot;&quot;&quot; # This mapping depends on the specific robot configuration # Simplified mapping for demonstration joint_commands = {} # Map pitch control to hip and ankle pitch joints hip_pitch_cmd = pitch_control * 0.4 ankle_pitch_cmd = pitch_control * 0.6 joint_commands['left_hip_pitch'] = hip_pitch_cmd joint_commands['right_hip_pitch'] = hip_pitch_cmd joint_commands['left_ankle_pitch'] = ankle_pitch_cmd joint_commands['right_ankle_pitch'] = ankle_pitch_cmd # Map roll control to hip and ankle roll joints hip_roll_cmd = roll_control * 0.3 ankle_roll_cmd = roll_control * 0.5 joint_commands['left_hip_roll'] = hip_roll_cmd joint_commands['right_hip_roll'] = -hip_roll_cmd # Opposite for stability joint_commands['left_ankle_roll'] = ankle_roll_cmd joint_commands['right_ankle_roll'] = -ankle_roll_cmd # Map Z control to hip height adjustment joint_commands['left_hip_z'] = z_control * 0.1 joint_commands['right_hip_z'] = z_control * 0.1 return joint_commands def get_stability_metrics(self) -&gt; Dict[str, float]: &quot;&quot;&quot;Get stability metrics for monitoring&quot;&quot;&quot; if not self.control_output_history: return {'stability_index': 1.0, 'max_control_effort': 0.0} recent_outputs = self.control_output_history[-10:] # Last 10 control cycles avg_pitch_control = np.mean([out['outputs']['pitch_torque'] for out in recent_outputs]) avg_roll_control = np.mean([out['outputs']['roll_torque'] for out in recent_outputs]) max_control_effort = max( abs(out['outputs']['pitch_torque']) for out in recent_outputs ) # Calculate stability index (lower is more stable) stability_index = (abs(avg_pitch_control) + abs(avg_roll_control)) / 2.0 stability_index = min(1.0, stability_index / 50.0) # Normalize return { 'stability_index': 1.0 - stability_index, # Higher is more stable 'max_control_effort': max_control_effort, 'average_pitch_control': abs(avg_pitch_control), 'average_roll_control': abs(avg_roll_control) }   ","version":"Next","tagName":"h3"},{"title":"Locomotion Control Systems​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#locomotion-control-systems","content":" class LocomotionController: def __init__(self, robot_config: Dict): &quot;&quot;&quot; Initialize locomotion controller for humanoid walking Args: robot_config: Configuration parameters for the robot &quot;&quot;&quot; self.robot_config = robot_config self.step_height = robot_config.get('step_height', 0.05) # meters self.step_length = robot_config.get('step_length', 0.3) # meters self.step_duration = robot_config.get('step_duration', 0.8) # seconds self.zmp_margin = robot_config.get('zmp_margin', 0.05) # safety margin # Walking pattern parameters self.stride_length = robot_config.get('stride_length', 0.6) self.step_width = robot_config.get('step_width', 0.2) # distance between feet self.walking_speed = 0.0 # Current walking speed # Initialize walking state self.current_support_foot = 'left' # 'left' or 'right' self.step_phase = 0.0 # 0.0 to 1.0, representing step cycle self.step_count = 0 # ZMP (Zero Moment Point) controller self.zmp_controller = ZMPController(robot_config) def generate_walking_pattern(self, target_velocity: np.ndarray, current_state: Dict) -&gt; Dict: &quot;&quot;&quot; Generate walking pattern based on target velocity Args: target_velocity: Target velocity [vx, vy, omega] (linear x, linear y, angular) current_state: Current robot state Returns: Walking pattern with foot positions and timing &quot;&quot;&quot; vx, vy, omega = target_velocity # Calculate step parameters based on target velocity walking_speed = np.sqrt(vx**2 + vy**2) step_frequency = self._calculate_step_frequency(walking_speed) # Generate footstep plan footsteps = self._generate_footsteps(vx, vy, omega, current_state) # Generate ZMP reference trajectory zmp_reference = self.zmp_controller.generate_zmp_trajectory(footsteps, step_frequency) # Package walking pattern walking_pattern = { 'footsteps': footsteps, 'zmp_reference': zmp_reference, 'step_frequency': step_frequency, 'walking_speed': walking_speed, 'step_timing': self._calculate_step_timing(step_frequency) } return walking_pattern def _calculate_step_frequency(self, walking_speed: float) -&gt; float: &quot;&quot;&quot;Calculate step frequency based on walking speed&quot;&quot;&quot; # Simplified relationship: frequency increases with speed # In practice, this would use more sophisticated models base_frequency = 0.8 # steps per second at very slow walking max_frequency = 2.0 # maximum steps per second if walking_speed &lt; 0.1: return base_frequency else: # Linear relationship between speed and frequency up to a point frequency = base_frequency + (walking_speed * 1.0) # Adjust multiplier as needed return min(frequency, max_frequency) def _generate_footsteps(self, vx: float, vy: float, omega: float, current_state: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Generate sequence of footsteps for walking&quot;&quot;&quot; footsteps = [] # Get current robot pose current_pose = current_state.get('pose', np.array([0.0, 0.0, 0.0])) # [x, y, theta] current_x, current_y, current_theta = current_pose # Calculate number of steps to plan ahead planning_horizon = 10 # Plan 10 steps ahead # Current support foot position support_foot_x = current_x support_foot_y = current_y + (self.step_width/2 if self.current_support_foot == 'left' else -self.step_width/2) for i in range(planning_horizon): # Calculate swing foot position # For simplicity, assume alternating feet swing_foot = 'right' if (i + self.step_count) % 2 == 0 else 'left' # Calculate step displacement based on target velocity dt = self.step_duration dx = vx * dt dy = vy * dt dtheta = omega * dt # Calculate new foot position if swing_foot == 'left': new_x = support_foot_x + dx + self.step_width/2 * np.sin(current_theta + dtheta) new_y = support_foot_y + dy - self.step_width/2 * np.cos(current_theta + dtheta) else: new_x = support_foot_x + dx - self.step_width/2 * np.sin(current_theta + dtheta) new_y = support_foot_y + dy + self.step_width/2 * np.cos(current_theta + dtheta) # Update support foot for next step support_foot_x = new_x support_foot_y = new_y + (self.step_width if swing_foot == 'left' else -self.step_width) footstep = { 'foot': swing_foot, 'position': [new_x, new_y, 0.0], # z=0 for ground contact 'orientation': current_theta + dtheta, 'timing': i * self.step_duration, 'swing_height': self.step_height, 'step_length': np.sqrt(dx**2 + dy**2) } footsteps.append(footstep) return footsteps def _calculate_step_timing(self, step_frequency: float) -&gt; Dict: &quot;&quot;&quot;Calculate detailed step timing for smooth walking&quot;&quot;&quot; step_period = 1.0 / step_frequency # Define phases of step cycle (double support, single support, etc.) timing = { 'double_support_start': 0.0, 'single_support_swing': 0.2, # 20% of cycle 'double_support_end': 0.9, # 90% of cycle 'cycle_period': step_period } return timing def update_walking_control(self, walking_pattern: Dict, current_state: Dict) -&gt; Dict: &quot;&quot;&quot; Update walking control based on planned walking pattern Args: walking_pattern: Planned walking pattern current_state: Current robot state Returns: Control commands for walking &quot;&quot;&quot; # Calculate current step phase self.step_phase = (time.time() % self.step_duration) / self.step_duration # Get current target foot position from pattern current_target = self._get_current_foot_target(walking_pattern, self.step_phase) # Calculate control commands to track the target control_commands = self._calculate_tracking_control(current_target, current_state) # Update ZMP control zmp_commands = self.zmp_controller.update_zmp_control( current_state, walking_pattern['zmp_reference'] ) # Combine all control commands combined_commands = { **control_commands, 'zmp_control': zmp_commands, 'walking_state': { 'step_phase': self.step_phase, 'support_foot': self.current_support_foot, 'step_count': self.step_count } } return combined_commands def _get_current_foot_target(self, walking_pattern: Dict, phase: float) -&gt; Dict: &quot;&quot;&quot;Get the current target position for feet based on walking phase&quot;&quot;&quot; # This would interpolate between footsteps based on current phase # For simplicity, return the next planned footstep footsteps = walking_pattern['footsteps'] if footsteps: return footsteps[0] # Return first (next) footstep else: return {'position': [0, 0, 0], 'orientation': 0} def _calculate_tracking_control(self, target: Dict, current_state: Dict) -&gt; Dict: &quot;&quot;&quot;Calculate control commands to track foot targets&quot;&quot;&quot; # Simplified tracking control # In practice, this would use inverse kinematics and more sophisticated control target_pos = np.array(target['position']) current_pos = np.array(current_state.get('foot_position', [0, 0, 0])) # Calculate position error pos_error = target_pos - current_pos # Simple proportional control kp_pos = 10.0 control_output = kp_pos * pos_error return { 'hip_commands': control_output[:2], # Simplified hip control 'ankle_commands': control_output[:2], # Simplified ankle control 'position_error': pos_error.tolist() } class ZMPController: def __init__(self, robot_config: Dict): &quot;&quot;&quot;Initialize ZMP (Zero Moment Point) controller&quot;&quot;&quot; self.robot_config = robot_config self.total_mass = robot_config.get('total_mass', 70.0) self.gravity = 9.81 self.com_height = robot_config.get('com_height', 0.8) # ZMP tracking controller gains self.zmp_kp = robot_config.get('zmp_kp', 50.0) self.zmp_kd = robot_config.get('zmp_kd', 10.0) def generate_zmp_trajectory(self, footsteps: List[Dict], step_frequency: float) -&gt; List[Dict]: &quot;&quot;&quot;Generate ZMP reference trajectory based on footsteps&quot;&quot;&quot; zmp_trajectory = [] step_period = 1.0 / step_frequency for i, footstep in enumerate(footsteps): # Calculate ZMP reference based on foot position and timing zmp_ref = { 'time': i * step_period, 'x': footstep['position'][0], 'y': footstep['position'][1], 'valid': True } zmp_trajectory.append(zmp_ref) return zmp_trajectory def update_zmp_control(self, current_state: Dict, zmp_reference: List[Dict]) -&gt; Dict: &quot;&quot;&quot;Update ZMP-based balance control&quot;&quot;&quot; # Get current ZMP estimate from force sensors current_zmp = self._estimate_current_zmp(current_state) # Find closest reference ZMP target_zmp = self._find_closest_zmp_reference(zmp_reference) if target_zmp and current_zmp: # Calculate ZMP error zmp_error = { 'x': target_zmp['x'] - current_zmp['x'], 'y': target_zmp['y'] - current_zmp['y'] } # Calculate control correction zmp_correction = { 'x': self.zmp_kp * zmp_error['x'], 'y': self.zmp_kp * zmp_error['y'] } return { 'zmp_error': zmp_error, 'zmp_correction': zmp_correction, 'current_zmp': current_zmp, 'target_zmp': target_zmp } return {'zmp_error': {'x': 0, 'y': 0}, 'zmp_correction': {'x': 0, 'y': 0}} def _estimate_current_zmp(self, current_state: Dict) -&gt; Optional[Dict]: &quot;&quot;&quot;Estimate current ZMP from force/torque sensors&quot;&quot;&quot; # This would use data from force/torque sensors in feet # For now, return a placeholder left_ft = current_state.get('left_foot_force_torque', [0, 0, 0, 0, 0, 0]) right_ft = current_state.get('right_foot_force_torque', [0, 0, 0, 0, 0, 0]) # Simplified ZMP calculation (would need actual sensor positions) # ZMP = [sum(M_y)/sum(F_z), -sum(M_x)/sum(F_z)] # where M_x, M_y are moments and F_z is vertical force if sum(left_ft[2:3]) + sum(right_ft[2:3]) &gt; 1: # Avoid division by zero zmp_x = (left_ft[4] + right_ft[4]) / (left_ft[2] + right_ft[2] + 1e-6) zmp_y = -(left_ft[3] + right_ft[3]) / (left_ft[2] + right_ft[2] + 1e-6) return {'x': zmp_x, 'y': zmp_y} return None def _find_closest_zmp_reference(self, zmp_reference: List[Dict]) -&gt; Optional[Dict]: &quot;&quot;&quot;Find the closest ZMP reference point in time&quot;&quot;&quot; if not zmp_reference: return None # For now, return the first reference point # In practice, would interpolate based on current time return zmp_reference[0]   ","version":"Next","tagName":"h3"},{"title":"VLA Pipeline Integration​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#vla-pipeline-integration","content":" ","version":"Next","tagName":"h2"},{"title":"Action Planning with Vision and Language Integration​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#action-planning-with-vision-and-language-integration","content":" class VLAPipelineIntegrator: def __init__(self, vision_system, language_system, action_system): &quot;&quot;&quot; Integrate Vision-Language-Action pipeline components Args: vision_system: Vision processing system language_system: Language understanding system action_system: Action planning and control system &quot;&quot;&quot; self.vision_system = vision_system self.language_system = language_system self.action_system = action_system # Initialize the complete VLA pipeline self.pipeline_state = { 'vision_buffer': [], 'language_buffer': [], 'action_buffer': [], 'world_state': {}, 'execution_state': 'idle' } def process_vla_command(self, natural_language_command: str) -&gt; Dict: &quot;&quot;&quot; Process a complete VLA command from language to action Args: natural_language_command: Natural language command from user Returns: Dictionary with processing results and action plan &quot;&quot;&quot; start_time = time.time() # Step 1: Language Understanding print(&quot;Step 1: Processing natural language command...&quot;) language_result = self.language_system.process_command(natural_language_command) if not language_result.get('success'): return { 'success': False, 'error': 'Language understanding failed', 'message': language_result.get('error', 'Unknown language error') } # Step 2: Vision Processing print(&quot;Step 2: Processing visual information...&quot;) vision_result = self.vision_system.get_current_scene_description() # Step 3: World State Integration print(&quot;Step 3: Integrating world state...&quot;) self.pipeline_state['world_state'] = self._integrate_world_state( vision_result, language_result ) # Step 4: Action Planning print(&quot;Step 4: Generating action plan...&quot;) action_plan = self._generate_action_plan(language_result, self.pipeline_state['world_state']) # Step 5: Plan Validation print(&quot;Step 5: Validating action plan...&quot;) validation_result = self._validate_action_plan(action_plan, self.pipeline_state['world_state']) if not validation_result['valid']: return { 'success': False, 'error': 'Action plan validation failed', 'message': validation_result['error'], 'plan': action_plan } # Step 6: Execute or Return Plan print(&quot;Step 6: Preparing for execution...&quot;) execution_ready = self.action_system.prepare_for_execution(action_plan) total_time = time.time() - start_time return { 'success': True, 'language_result': language_result, 'vision_result': vision_result, 'action_plan': action_plan, 'validation_result': validation_result, 'execution_ready': execution_ready, 'processing_time': total_time, 'world_state': self.pipeline_state['world_state'] } def _integrate_world_state(self, vision_result: Dict, language_result: Dict) -&gt; Dict: &quot;&quot;&quot;Integrate vision and language information into a coherent world state&quot;&quot;&quot; world_state = { 'timestamp': time.time(), 'objects': vision_result.get('objects', []), 'locations': vision_result.get('locations', []), 'robot_state': vision_result.get('robot_state', {}), 'command': language_result.get('parsed_command'), 'resolved_entities': language_result.get('resolved_entities', {}), 'spatial_context': self._create_spatial_context(vision_result, language_result) } return world_state def _create_spatial_context(self, vision_result: Dict, language_result: Dict) -&gt; Dict: &quot;&quot;&quot;Create spatial context by grounding language references to visual objects&quot;&quot;&quot; spatial_context = { 'object_poses': {}, 'relative_positions': {}, 'reachable_objects': [], 'navigation_targets': [] } # Map object names to their visual detections for obj in vision_result.get('objects', []): obj_name = obj.get('name', obj.get('class', 'unknown')) spatial_context['object_poses'][obj_name] = obj.get('position_3d', [0, 0, 0]) # Determine which objects are relevant based on language command command_entities = language_result.get('resolved_entities', {}) for entity_type, entity_value in command_entities.items(): if 'object' in entity_type.lower(): if entity_value in spatial_context['object_poses']: # Check if object is reachable obj_pos = spatial_context['object_poses'][entity_value] robot_pos = vision_result.get('robot_state', {}).get('position', [0, 0, 0]) distance = np.linalg.norm(np.array(obj_pos[:2]) - np.array(robot_pos[:2])) if distance &lt; 2.0: # Within 2 meters is reachable spatial_context['reachable_objects'].append({ 'name': entity_value, 'position': obj_pos, 'distance': distance }) return spatial_context def _generate_action_plan(self, language_result: Dict, world_state: Dict) -&gt; ActionPlan: &quot;&quot;&quot;Generate action plan based on language command and world state&quot;&quot;&quot; command = language_result['parsed_command'] if command.action == 'navigate': # Navigation action target_location = command.target_location or command.target_object if target_location: # Find the actual location in world state actual_location = self._find_location_in_world(target_location, world_state) if actual_location: return self.action_system.plan_navigation_to_location(actual_location) elif command.action == 'grasp': # Manipulation action target_object = command.target_object if target_object: actual_object = self._find_object_in_world(target_object, world_state) if actual_object: return self.action_system.plan_manipulation_of_object(actual_object) elif command.action == 'place': # Place action target_object = command.target_object target_location = command.target_location if target_object and target_location: obj = self._find_object_in_world(target_object, world_state) loc = self._find_location_in_world(target_location, world_state) if obj and loc: return self.action_system.plan_placement_action(obj, loc) # Default: return empty plan return ActionPlan( steps=[], start_time=time.time(), estimated_duration=0.0, success_conditions=[], failure_conditions=[] ) def _find_location_in_world(self, location_name: str, world_state: Dict) -&gt; Optional[Dict]: &quot;&quot;&quot;Find a location in the world state&quot;&quot;&quot; for loc in world_state['locations']: if loc.get('name', '').lower() == location_name.lower(): return loc return None def _find_object_in_world(self, object_name: str, world_state: Dict) -&gt; Optional[Dict]: &quot;&quot;&quot;Find an object in the world state&quot;&quot;&quot; for obj in world_state['objects']: if obj.get('name', '').lower() == object_name.lower(): return obj return None def _validate_action_plan(self, action_plan: ActionPlan, world_state: Dict) -&gt; Dict: &quot;&quot;&quot;Validate that the action plan is feasible given the world state&quot;&quot;&quot; validation_result = { 'valid': True, 'warnings': [], 'errors': [] } if not action_plan.steps: validation_result['valid'] = False validation_result['errors'].append('No action steps in plan') return validation_result # Check if all required objects are visible for step in action_plan.steps: if step.action_type == ActionType.MANIPULATION: target_obj = step.parameters.get('target_object') if target_obj and not self._object_is_accessible(target_obj, world_state): validation_result['valid'] = False validation_result['errors'].append(f'Target object {target_obj} is not accessible') # Check for potential collisions for step in action_plan.steps: if step.action_type == ActionType.LOCOMOTION: path = step.parameters.get('path') if path and self._path_has_collisions(path, world_state): validation_result['warnings'].append('Potential collision detected in path') return validation_result def _object_is_accessible(self, object_name: str, world_state: Dict) -&gt; bool: &quot;&quot;&quot;Check if an object is accessible to the robot&quot;&quot;&quot; for obj in world_state['reachable_objects']: if obj['name'].lower() == object_name.lower(): return True return False def _path_has_collisions(self, path: List, world_state: Dict) -&gt; bool: &quot;&quot;&quot;Check if a path has collisions&quot;&quot;&quot; # Simplified collision checking # In practice, would use the actual map and collision checking return False def execute_action_plan(self, action_plan: ActionPlan) -&gt; Dict: &quot;&quot;&quot;Execute the action plan and monitor progress&quot;&quot;&quot; print(f&quot;Executing action plan with {len(action_plan.steps)} steps...&quot;) execution_results = [] start_time = time.time() for i, step in enumerate(action_plan.steps): print(f&quot;Executing step {i+1}/{len(action_plan.steps)}: {step.action_type.value}&quot;) # Execute the step step_result = self.action_system.execute_action_step(step) # Monitor success if not step_result.get('success', False): return { 'success': False, 'completed_steps': i, 'failed_step': i, 'error': step_result.get('error', 'Unknown execution error'), 'execution_time': time.time() - start_time } execution_results.append(step_result) # Check for plan adaptation needs if self._should_adapt_plan_during_execution(action_plan, i): print(&quot;Adapting plan based on execution feedback...&quot;) action_plan = self._adapt_plan(action_plan, i, execution_results) total_time = time.time() - start_time return { 'success': True, 'completed_steps': len(action_plan.steps), 'execution_results': execution_results, 'execution_time': total_time, 'plan_completed': True } def _should_adapt_plan_during_execution(self, action_plan: ActionPlan, current_step: int) -&gt; bool: &quot;&quot;&quot;Determine if the plan should be adapted during execution&quot;&quot;&quot; # For now, adapt if we're in manipulation and environment changed if current_step &gt; 0 and action_plan.steps[current_step].action_type == ActionType.MANIPULATION: # Check if environment changed significantly # This would involve re-sensing the environment return False # Simplified - no adaptation for now return False def _adapt_plan(self, original_plan: ActionPlan, current_step: int, execution_results: List[Dict]) -&gt; ActionPlan: &quot;&quot;&quot;Adapt the plan based on execution feedback&quot;&quot;&quot; # Simplified adaptation - in practice, this would be more sophisticated return original_plan # Return original plan for now   ","version":"Next","tagName":"h3"},{"title":"Real-Time Control and Performance​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#real-time-control-and-performance","content":" ","version":"Next","tagName":"h2"},{"title":"Efficient Control Loop Implementation​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#efficient-control-loop-implementation","content":" import threading import queue from collections import deque import time class RealTimeController: def __init__(self, control_frequency: int = 100): &quot;&quot;&quot; Initialize real-time controller with specified frequency Args: control_frequency: Control frequency in Hz (typically 100Hz for humanoid balance) &quot;&quot;&quot; self.control_frequency = control_frequency self.control_period = 1.0 / control_frequency self.is_running = False # Control loop components self.state_estimator = StateEstimator() self.balance_controller = None # Will be set externally self.locomotion_controller = None # Will be set externally self.manipulation_controller = None # Will be set externally # Data queues for multi-threading self.sensor_queue = queue.Queue(maxsize=10) self.command_queue = queue.Queue(maxsize=10) self.state_queue = queue.Queue(maxsize=10) # Performance monitoring self.control_times = deque(maxlen=100) self.loop_overruns = 0 # Control threads self.control_thread = None def start_control_loop(self): &quot;&quot;&quot;Start the real-time control loop in a separate thread&quot;&quot;&quot; self.is_running = True self.control_thread = threading.Thread(target=self._control_loop) self.control_thread.daemon = True self.control_thread.start() def stop_control_loop(self): &quot;&quot;&quot;Stop the real-time control loop&quot;&quot;&quot; self.is_running = False if self.control_thread: self.control_thread.join() def _control_loop(self): &quot;&quot;&quot;Main real-time control loop&quot;&quot;&quot; while self.is_running: loop_start = time.time() try: # Get current sensor data sensor_data = self._get_sensor_data() # Estimate current state current_state = self.state_estimator.estimate_state(sensor_data) # Update controllers balance_commands = self.balance_controller.update_balance_control( current_state, {'pitch': 0, 'roll': 0, 'z_position': 0.8} ) if self.balance_controller else {} locomotion_commands = self.locomotion_controller.update_walking_control( {'footsteps': [], 'zmp_reference': []}, current_state ) if self.locomotion_controller else {} # Combine commands combined_commands = self._combine_commands( balance_commands, locomotion_commands ) # Send commands to robot self._send_commands_to_robot(combined_commands) # Calculate control time control_time = time.time() - loop_start self.control_times.append(control_time) # Check for loop overrun if control_time &gt; self.control_period: self.loop_overruns += 1 # Sleep to maintain control frequency sleep_time = self.control_period - control_time if sleep_time &gt; 0: time.sleep(sleep_time) else: # Loop took too long - warn but continue print(f&quot;Control loop overrun by {-sleep_time:.4f}s&quot;) except Exception as e: print(f&quot;Error in control loop: {e}&quot;) time.sleep(0.01) # Brief pause to avoid spinning on error def _get_sensor_data(self) -&gt; Dict: &quot;&quot;&quot;Get current sensor data from robot&quot;&quot;&quot; # This would interface with actual robot sensors # For simulation, return mock data return { 'imu': {'orientation': [0, 0, 0, 1], 'angular_velocity': [0, 0, 0], 'linear_acceleration': [0, 0, -9.81]}, 'joint_positions': [0] * 20, # Example: 20 joints 'joint_velocities': [0] * 20, 'force_torque': {'left_foot': [0, 0, 0, 0, 0, 0], 'right_foot': [0, 0, 0, 0, 0, 0]}, 'encoders': [0] * 20, 'timestamp': time.time() } def _combine_commands(self, balance_commands: Dict, locomotion_commands: Dict) -&gt; Dict: &quot;&quot;&quot;Combine commands from different controllers&quot;&quot;&quot; # Priority-based command combination # Balance commands typically have highest priority combined = balance_commands.copy() combined.update(locomotion_commands) # Add any manipulation commands if available # combined.update(manipulation_commands) return combined def _send_commands_to_robot(self, commands: Dict): &quot;&quot;&quot;Send control commands to robot hardware&quot;&quot;&quot; # This would interface with robot's control interface # For now, just print the commands pass def get_performance_metrics(self) -&gt; Dict: &quot;&quot;&quot;Get real-time control performance metrics&quot;&quot;&quot; if not self.control_times: return { 'average_loop_time': 0, 'control_frequency': 0, 'loop_overruns': 0, 'min_loop_time': 0, 'max_loop_time': 0 } avg_time = sum(self.control_times) / len(self.control_times) actual_frequency = 1.0 / avg_time if avg_time &gt; 0 else 0 return { 'average_loop_time': avg_time, 'control_frequency': actual_frequency, 'loop_overruns': self.loop_overruns, 'min_loop_time': min(self.control_times), 'max_loop_time': max(self.control_times), 'target_frequency': self.control_frequency } class StateEstimator: def __init__(self): &quot;&quot;&quot;Initialize state estimator for humanoid robot&quot;&quot;&quot; self.state_history = deque(maxlen=10) self.com_estimator = CenterOfMassEstimator() def estimate_state(self, sensor_data: Dict) -&gt; Dict: &quot;&quot;&quot;Estimate current robot state from sensor data&quot;&quot;&quot; # Extract sensor readings imu_data = sensor_data.get('imu', {}) joint_positions = sensor_data.get('joint_positions', []) joint_velocities = sensor_data.get('joint_velocities', []) # Estimate orientation (pitch, roll, yaw) orientation = self._extract_orientation(imu_data.get('orientation', [0, 0, 0, 1])) # Estimate center of mass position and velocity com_state = self.com_estimator.estimate_com_state(joint_positions, joint_velocities) # Estimate ZMP (Zero Moment Point) zmp = self._estimate_zmp(sensor_data) # Package estimated state estimated_state = { 'timestamp': sensor_data.get('timestamp', time.time()), 'position': com_state['position'], 'velocity': com_state['velocity'], 'orientation': orientation, 'angular_velocity': imu_data.get('angular_velocity', [0, 0, 0]), 'linear_acceleration': imu_data.get('linear_acceleration', [0, 0, -9.81]), 'center_of_mass': com_state, 'zmp': zmp, 'joint_positions': joint_positions, 'joint_velocities': joint_velocities, 'balance_metrics': self._calculate_balance_metrics(com_state, zmp) } # Store in history self.state_history.append(estimated_state) return estimated_state def _extract_orientation(self, quaternion: List[float]) -&gt; Dict: &quot;&quot;&quot;Extract pitch, roll, yaw from quaternion&quot;&quot;&quot; # Convert quaternion to roll, pitch, yaw w, x, y, z = quaternion # Roll (x-axis rotation) sinr_cosp = 2 * (w * x + y * z) cosr_cosp = 1 - 2 * (x * x + y * y) roll = np.arctan2(sinr_cosp, cosr_cosp) # Pitch (y-axis rotation) sinp = 2 * (w * y - z * x) if abs(sinp) &gt;= 1: pitch = np.copysign(np.pi / 2, sinp) # Use 90 degrees if out of range else: pitch = np.arcsin(sinp) # Yaw (z-axis rotation) siny_cosp = 2 * (w * z + x * y) cosy_cosp = 1 - 2 * (y * y + z * z) yaw = np.arctan2(siny_cosp, cosy_cosp) return {'roll': roll, 'pitch': pitch, 'yaw': yaw} def _estimate_zmp(self, sensor_data: Dict) -&gt; List[float]: &quot;&quot;&quot;Estimate Zero Moment Point from force/torque sensors&quot;&quot;&quot; # Simplified ZMP estimation # In practice, would use actual force/torque sensor data left_ft = sensor_data.get('force_torque', {}).get('left_foot', [0, 0, 0, 0, 0, 0]) right_ft = sensor_data.get('force_torque', {}).get('right_foot', [0, 0, 0, 0, 0, 0]) # ZMP = [sum(M_y)/sum(F_z), -sum(M_x)/sum(F_z)] # where M_x, M_y are moments and F_z is vertical force total_fz = left_ft[2] + right_ft[2] total_mx = left_ft[3] + right_ft[3] total_my = left_ft[4] + right_ft[4] if abs(total_fz) &gt; 1: # Avoid division by zero zmp_x = total_my / total_fz zmp_y = -total_mx / total_fz return [zmp_x, zmp_y, 0] # ZMP in world coordinates else: return [0, 0, 0] # Default to origin if no force def _calculate_balance_metrics(self, com_state: Dict, zmp: List[float]) -&gt; Dict: &quot;&quot;&quot;Calculate balance-related metrics&quot;&quot;&quot; com_pos = np.array(com_state['position']) zmp_pos = np.array(zmp) # Distance between COM and ZMP (smaller is more stable) com_zmp_distance = np.linalg.norm(com_pos[:2] - zmp_pos[:2]) # Calculate support polygon (simplified as a rectangle between feet) # This would be more complex in reality support_margin = 0.1 # Simplified margin return { 'com_zmp_distance': float(com_zmp_distance), 'stability_margin': float(support_margin - com_zmp_distance), 'is_balanced': (support_margin - com_zmp_distance) &gt; 0 } class CenterOfMassEstimator: def __init__(self): &quot;&quot;&quot;Initialize center of mass estimator&quot;&quot;&quot; # This would typically load robot's kinematic and mass parameters self.mass_properties = self._load_mass_properties() def _load_mass_properties(self) -&gt; Dict: &quot;&quot;&quot;Load mass properties for COM estimation&quot;&quot;&quot; # Simplified mass properties - in practice, would load from robot description return { 'total_mass': 70.0, # kg 'segment_masses': [5.0, 5.0, 10.0, 5.0, 5.0, 10.0, 10.0], # Example: head, arms, torso, legs 'segment_positions': [] # Would be calculated from joint positions } def estimate_com_state(self, joint_positions: List[float], joint_velocities: List[float]) -&gt; Dict: &quot;&quot;&quot;Estimate center of mass position and velocity&quot;&quot;&quot; # Simplified COM estimation # In practice, would use full kinematic model and mass distribution # For demonstration, return a fixed position relative to base # This would use forward kinematics and mass-weighted averaging com_position = [0.0, 0.0, 0.8] # Approximate COM height com_velocity = [0.0, 0.0, 0.0] # Simplified return { 'position': com_position, 'velocity': com_velocity, 'acceleration': [0.0, 0.0, 0.0] }   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#real-time-validation-principle-iv","content":" High-frequency control loops (100Hz) for balance feedbackReal-time performance optimization for embedded systemsTiming constraints for bipedal balance controlPerformance monitoring for control systems  ","version":"Next","tagName":"h3"},{"title":"VLA Convergence Mandate (Principle I)​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#vla-convergence-mandate-principle-i","content":" Complete integration of Vision-Language-Action pipelineAction planning informed by vision and language understandingReal-time control systems for VLA execution  ","version":"Next","tagName":"h3"},{"title":"Anthropomorphic Focus (Principle II)​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#anthropomorphic-focus-principle-ii","content":" Bipedal locomotion control systemsDexterous manipulation controlHuman-like movement patterns  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization (Constraint)​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#target-hardware-optimization-constraint","content":" Efficient algorithms suitable for Jetson Orin deploymentReal-time performance on embedded systemsMemory and computation optimization  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Humanoid Walking Controller​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#example-1-humanoid-walking-controller","content":" class HumanoidWalkingController: def __init__(self): &quot;&quot;&quot;Initialize complete humanoid walking system&quot;&quot;&quot; self.robot_config = { 'total_mass': 70.0, 'height': 1.6, 'com_height': 0.8, 'step_height': 0.05, 'step_length': 0.3, 'step_duration': 0.8, 'step_width': 0.2, 'balance_kp': 100.0, 'balance_kd': 10.0, 'balance_ki': 1.0 } # Initialize controllers self.balance_controller = BalanceController(self.robot_config) self.locomotion_controller = LocomotionController(self.robot_config) self.state_estimator = StateEstimator() self.real_time_controller = RealTimeController(control_frequency=100) # Set up the real-time controller with our specific controllers self.real_time_controller.balance_controller = self.balance_controller self.real_time_controller.locomotion_controller = self.locomotion_controller def demonstrate_walking(self): &quot;&quot;&quot;Demonstrate walking control with various gaits&quot;&quot;&quot; print(&quot;=== Humanoid Walking Demonstration ===&quot;) # Example 1: Standing balance print(&quot;\\n1. Testing balance control...&quot;) current_state = { 'pitch': 0.01, # Small disturbance 'roll': -0.02, 'z_position': 0.8, 'pose': [0, 0, 0] } target_state = {'pitch': 0, 'roll': 0, 'z_position': 0.8} balance_commands = self.balance_controller.update_balance_control(current_state, target_state) stability_metrics = self.balance_controller.get_stability_metrics() print(f&quot;Balance commands: {balance_commands}&quot;) print(f&quot;Stability metrics: {stability_metrics}&quot;) # Example 2: Walking pattern generation print(&quot;\\n2. Generating walking pattern...&quot;) target_velocity = [0.3, 0.0, 0.0] # Move forward at 0.3 m/s current_state['pose'] = [0, 0, 0] walking_pattern = self.locomotion_controller.generate_walking_pattern( target_velocity, current_state ) print(f&quot;Generated {len(walking_pattern['footsteps'])} footsteps&quot;) print(f&quot;Step frequency: {walking_pattern['step_frequency']:.2f} Hz&quot;) # Example 3: Complete walking control print(&quot;\\n3. Simulating walking control...&quot;) walking_control = self.locomotion_controller.update_walking_control( walking_pattern, current_state ) print(f&quot;Walking control state: {walking_control['walking_state']}&quot;) print(f&quot;ZMP control: {walking_control['zmp_control']}&quot;) # Example 4: Real-time control performance print(&quot;\\n4. Real-time control metrics...&quot;) # This would show actual performance if the controller was running # For demonstration, we'll create mock metrics mock_metrics = { 'average_loop_time': 0.008, # 8ms average 'control_frequency': 100.0, # 100Hz 'loop_overruns': 0, 'min_loop_time': 0.005, 'max_loop_time': 0.012 } print(f&quot;Control performance: {mock_metrics}&quot;) def start_autonomous_walking(self): &quot;&quot;&quot;Start autonomous walking with real-time control&quot;&quot;&quot; print(&quot;\\nStarting autonomous walking control...&quot;) self.real_time_controller.start_control_loop() # Monitor performance import time as time_module start_time = time_module.time() try: while time_module.time() - start_time &lt; 10: # Run for 10 seconds time_module.sleep(1) metrics = self.real_time_controller.get_performance_metrics() print(f&quot;Control frequency: {metrics['control_frequency']:.1f}Hz, &quot; f&quot;Overruns: {metrics['loop_overruns']}&quot;) except KeyboardInterrupt: print(&quot;\\nStopping autonomous walking...&quot;) finally: self.real_time_controller.stop_control_loop() # Example usage if __name__ == &quot;__main__&quot;: walker = HumanoidWalkingController() walker.demonstrate_walking()   ","version":"Next","tagName":"h3"},{"title":"Example 2: VLA Integration for Task Execution​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#example-2-vla-integration-for-task-execution","content":" class VLATaskExecutor: def __init__(self): &quot;&quot;&quot;Initialize complete VLA task execution system&quot;&quot;&quot; # Initialize mock systems (in practice, these would be real systems) self.vision_system = MockVisionSystem() self.language_system = MockLanguageSystem() self.action_system = MockActionSystem() # Initialize the VLA integrator self.vla_integrator = VLAPipelineIntegrator( self.vision_system, self.language_system, self.action_system ) def execute_task(self, task_description: str): &quot;&quot;&quot;Execute a complete task using the VLA pipeline&quot;&quot;&quot; print(f&quot;\\n=== Executing Task: '{task_description}' ===&quot;) # Process the complete VLA pipeline result = self.vla_integrator.process_vla_command(task_description) if result['success']: print(&quot;✓ VLA pipeline processing successful&quot;) print(f&quot;• Language understanding: {result['language_result']['intent']}&quot;) print(f&quot;• Vision analysis: {len(result['vision_result']['objects'])} objects detected&quot;) print(f&quot;• Action plan: {len(result['action_plan'].steps)} steps&quot;) print(f&quot;• Processing time: {result['processing_time']:.3f}s&quot;) # Execute the action plan execution_result = self.vla_integrator.execute_action_plan(result['action_plan']) if execution_result['success']: print(f&quot;✓ Task execution completed in {execution_result['execution_time']:.3f}s&quot;) else: print(f&quot;✗ Task execution failed: {execution_result.get('error', 'Unknown error')}&quot;) else: print(f&quot;✗ VLA pipeline failed: {result.get('error', 'Unknown error')}&quot;) print(f&quot; Message: {result.get('message', 'No additional information')}&quot;) def demonstrate_vla_integration(self): &quot;&quot;&quot;Demonstrate various VLA integration scenarios&quot;&quot;&quot; tasks = [ &quot;Walk to the kitchen and find the red cup&quot;, &quot;Pick up the book from the table&quot;, &quot;Go to the living room and sit on the couch&quot;, &quot;Find the person and greet them&quot; ] for task in tasks: self.execute_task(task) class MockVisionSystem: &quot;&quot;&quot;Mock vision system for demonstration&quot;&quot;&quot; def get_current_scene_description(self): return { 'objects': [ {'name': 'red cup', 'class': 'cup', 'position_3d': [1.5, 0.5, 0.8]}, {'name': 'book', 'class': 'book', 'position_3d': [0.8, 0.2, 0.9]}, {'name': 'table', 'class': 'table', 'position_3d': [1.0, 0.0, 0.0]}, {'name': 'person', 'class': 'person', 'position_3d': [2.0, 1.0, 0.0]} ], 'locations': [ {'name': 'kitchen', 'position': [3.0, 0.0, 0.0]}, {'name': 'living room', 'position': [0.0, 2.0, 0.0]}, {'name': 'couch', 'position': [0.5, 1.5, 0.0]} ], 'robot_state': {'position': [0.0, 0.0, 0.0], 'orientation': [0, 0, 0, 1]} } class MockLanguageSystem: &quot;&quot;&quot;Mock language system for demonstration&quot;&quot;&quot; def process_command(self, command): # Simple command parsing for demonstration command_lower = command.lower() if 'walk to' in command_lower or 'go to' in command_lower: intent = 'navigation' elif 'pick up' in command_lower or 'take' in command_lower: intent = 'manipulation' elif 'greet' in command_lower or 'hello' in command_lower: intent = 'social' else: intent = 'unknown' return { 'success': True, 'intent': intent, 'parsed_command': { 'action': intent, 'target_object': self._extract_object(command), 'target_location': self._extract_location(command) }, 'resolved_entities': {} } def _extract_object(self, command): objects = ['red cup', 'book', 'person'] for obj in objects: if obj in command.lower(): return obj return None def _extract_location(self, command): locations = ['kitchen', 'living room', 'couch'] for loc in locations: if loc in command.lower(): return loc return None class MockActionSystem: &quot;&quot;&quot;Mock action system for demonstration&quot;&quot;&quot; def plan_navigation_to_location(self, location): # Create a simple navigation plan steps = [ ActionStep( action_type=ActionType.LOCOMOTION, parameters={'target_location': location['position']}, duration=2.0, preconditions=['robot_is_standing'], effects=['robot_moved'], priority=2 ), ActionStep( action_type=ActionType.BALANCE, parameters={}, duration=0.5, preconditions=['navigation_completed'], effects=['robot_balanced'], priority=3 ) ] return ActionPlan( steps=steps, start_time=time.time(), estimated_duration=2.5, success_conditions=['robot_at_location'], failure_conditions=['obstacle_detected'] ) def plan_manipulation_of_object(self, obj): steps = [ ActionStep( action_type=ActionType.MANIPULATION, parameters={'target_object': obj['name']}, duration=3.0, preconditions=['robot_at_object_location'], effects=['object_grasped'], priority=3 ) ] return ActionPlan( steps=steps, start_time=time.time(), estimated_duration=3.0, success_conditions=['object_manipulated'], failure_conditions=['grasp_failed'] ) def prepare_for_execution(self, action_plan): return True def execute_action_step(self, step): # Simulate step execution time.sleep(step.duration * 0.1) # Simulate execution time return {'success': True, 'step_completed': True} # Run demonstration if __name__ == &quot;__main__&quot;: executor = VLATaskExecutor() executor.demonstrate_vla_integration()   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Action Planning Implementation​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#exercise-1-action-planning-implementation","content":" Implement a complete action planning system that:  Plans navigation paths with obstacle avoidanceGenerates manipulation plans for object interactionValidates plans against current world stateHandles plan adaptation during execution  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Balance Control System​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#exercise-2-balance-control-system","content":" Create a robust balance control system that:  Maintains stability during walking and standingUses ZMP control for dynamic balanceHandles external disturbances gracefullyMonitors stability metrics in real-time  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: VLA Pipeline Integration​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#exercise-3-vla-pipeline-integration","content":" Develop a complete VLA pipeline that:  Integrates vision, language, and action systemsProcesses natural language commands in real-timeExecutes complex multi-step tasksHandles failures and uncertainties gracefully  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#summary","content":" Action planning and control systems form the crucial Action component of the Vision-Language-Action pipeline, enabling humanoid robots to execute the commands understood through language and informed by vision. The complexity of humanoid control, with its requirements for bipedal balance, dexterous manipulation, and real-time performance, demands sophisticated planning and control algorithms. The integration of these systems into the complete VLA pipeline enables natural human-robot interaction where users can issue commands in natural language and have the robot execute complex tasks in physical environments. The real-time constraints and safety requirements for humanoid robots necessitate efficient algorithms optimized for embedded systems while maintaining the stability and performance required for safe operation in human-centered environments.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 16 - Action Planning & Control Systems","url":"/docs/chapters/module-4-vla/chapter-16-action-planning#further-reading","content":" &quot;Humanoid Robotics: A Reference&quot; by Goswami and Vadakkepat&quot;Robotics: Control, Sensing, Vision, and Intelligence&quot; by Fu, Gonzalez, and Lee&quot;Feedback Control of Dynamic Bipedal Robot Locomotion&quot; by Gregg and Spong&quot;Whole-Body Dynamic Control of Humanoid Robots&quot; - Recent research papers&quot;Real-Time Systems and Robotics&quot; - Control systems for robotics applications ","version":"Next","tagName":"h2"},{"title":"Markdown Features","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/markdown-features","content":"","keywords":"","version":"Next"},{"title":"Front Matter​","type":1,"pageTitle":"Markdown Features","url":"/docs/tutorial-basics/markdown-features#front-matter","content":" Markdown documents have metadata at the top called Front Matter:  my-doc.md --- id: my-doc-id title: My document title description: My document description slug: /my-custom-url --- ## Markdown heading Markdown text with [links](./hello.md)   ","version":"Next","tagName":"h2"},{"title":"Links​","type":1,"pageTitle":"Markdown Features","url":"/docs/tutorial-basics/markdown-features#links","content":" Regular Markdown links are supported, using url paths or relative file paths.  Let's see how to [Create a page](/create-a-page).   Let's see how to [Create a page](./create-a-page.md).   Result: Let's see how to Create a page.  ","version":"Next","tagName":"h2"},{"title":"Images​","type":1,"pageTitle":"Markdown Features","url":"/docs/tutorial-basics/markdown-features#images","content":" Regular Markdown images are supported.  You can use absolute paths to reference images in the static directory (static/img/docusaurus.png):  ![Docusaurus logo](/img/docusaurus.png)     You can reference images relative to the current file as well. This is particularly useful to colocate images close to the Markdown files using them:  ![Docusaurus logo](./img/docusaurus.png)   ","version":"Next","tagName":"h2"},{"title":"Code Blocks​","type":1,"pageTitle":"Markdown Features","url":"/docs/tutorial-basics/markdown-features#code-blocks","content":" Markdown code blocks are supported with Syntax highlighting.  ```jsx title=&quot;src/components/HelloDocusaurus.js&quot; function HelloDocusaurus() { return &lt;h1&gt;Hello, Docusaurus!&lt;/h1&gt;; } ```   src/components/HelloDocusaurus.js function HelloDocusaurus() { return &lt;h1&gt;Hello, Docusaurus!&lt;/h1&gt;; }   ","version":"Next","tagName":"h2"},{"title":"Admonitions​","type":1,"pageTitle":"Markdown Features","url":"/docs/tutorial-basics/markdown-features#admonitions","content":" Docusaurus has a special syntax to create admonitions and callouts:  :::tip[My tip] Use this awesome feature option ::: :::danger[Take care] This action is dangerous :::   My tip Use this awesome feature option  Take care This action is dangerous  ","version":"Next","tagName":"h2"},{"title":"MDX and React Components​","type":1,"pageTitle":"Markdown Features","url":"/docs/tutorial-basics/markdown-features#mdx-and-react-components","content":" MDX can make your documentation more interactive and allows using any React components inside Markdown:  export const Highlight = ({children, color}) =&gt; ( &lt;span style={{ backgroundColor: color, borderRadius: '20px', color: '#fff', padding: '10px', cursor: 'pointer', }} onClick={() =&gt; { alert(`You clicked the color ${color} with label ${children}`) }}&gt; {children} &lt;/span&gt; ); This is &lt;Highlight color=&quot;#25c2a0&quot;&gt;Docusaurus green&lt;/Highlight&gt; ! This is &lt;Highlight color=&quot;#1877F2&quot;&gt;Facebook blue&lt;/Highlight&gt; !     This is Docusaurus green !  This is Facebook blue ! ","version":"Next","tagName":"h2"},{"title":"Chapter 17 - Integration: Vision-Language-Action Systems","type":0,"sectionRef":"#","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#learning-objectives","content":" Integrate vision, language, and action systems into complete pipelineImplement real-time VLA coordination for humanoid robotsValidate integrated system performance and safety  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#introduction","content":" The Vision-Language-Action (VLA) pipeline represents the complete cognitive architecture for humanoid robots operating in human-centered environments, embodying our project's core principle of VLA Convergence Mandate. This chapter focuses on the critical challenge of integrating vision, language, and action systems into a cohesive, real-time operational framework that enables natural human-robot interaction. The integration must be robust, responsive, and safe, meeting the real-time validation requirements for humanoid stability and the anthropomorphic focus of our design philosophy. This chapter covers the complete integration of the three systems, with special attention to the timing constraints, data flow, and coordination mechanisms necessary for effective humanoid robot operation.  ","version":"Next","tagName":"h2"},{"title":"Complete VLA System Architecture​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#complete-vla-system-architecture","content":" ","version":"Next","tagName":"h2"},{"title":"System Architecture Overview​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#system-architecture-overview","content":" graph TB A[Natural Language Input] --&gt; B[Language Understanding] B --&gt; C[Vision System] C --&gt; D[Action Planning] D --&gt; E[Control Systems] E --&gt; F[Physical Execution] F --&gt; G[Feedback Loop] G --&gt; B G --&gt; C G --&gt; D H[External Sensors] --&gt; C H --&gt; E I[User Interaction] --&gt; A J[Robot Status] --&gt; D J --&gt; E   ","version":"Next","tagName":"h3"},{"title":"Real-Time Coordination Architecture​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#real-time-coordination-architecture","content":" For humanoid robots, the VLA pipeline must operate in real-time with strict timing constraints:  import asyncio import threading from concurrent.futures import ThreadPoolExecutor import time from typing import Dict, List, Optional, Callable from dataclasses import dataclass from enum import Enum class VLAModule(Enum): &quot;&quot;&quot;Enumeration of VLA pipeline modules&quot;&quot;&quot; LANGUAGE = &quot;language&quot; VISION = &quot;vision&quot; ACTION_PLANNING = &quot;action_planning&quot; CONTROL = &quot;control&quot; EXECUTION = &quot;execution&quot; class VLAState(Enum): &quot;&quot;&quot;Enumeration of VLA system states&quot;&quot;&quot; IDLE = &quot;idle&quot; PROCESSING = &quot;processing&quot; EXECUTING = &quot;executing&quot; ADAPTING = &quot;adapting&quot; ERROR = &quot;error&quot; SAFETY_STOP = &quot;safety_stop&quot; @dataclass class VLADataPacket: &quot;&quot;&quot;Data packet for VLA pipeline communication&quot;&quot;&quot; timestamp: float module_origin: VLAModule module_destination: VLAModule data: Dict priority: int = 1 correlation_id: Optional[str] = None timeout: float = 5.0 class VLACommunicationBus: &quot;&quot;&quot;Communication bus for VLA pipeline modules&quot;&quot;&quot; def __init__(self): self.modules = {} self.data_queues = {} self.callbacks = {} self.lock = threading.Lock() # Initialize queues for each module communication for source in VLAModule: for dest in VLAModule: if source != dest: queue_key = f&quot;{source.value}_to_{dest.value}&quot; self.data_queues[queue_key] = asyncio.Queue() def register_module(self, module: VLAModule, handler: Callable): &quot;&quot;&quot;Register a module with its processing handler&quot;&quot;&quot; self.modules[module.value] = handler def send_data(self, packet: VLADataPacket) -&gt; bool: &quot;&quot;&quot;Send data packet to destination module&quot;&quot;&quot; queue_key = f&quot;{packet.module_origin.value}_to_{packet.module_destination.value}&quot; if queue_key in self.data_queues: try: self.data_queues[queue_key].put_nowait(packet) return True except asyncio.QueueFull: return False return False async def receive_data(self, source_module: VLAModule, dest_module: VLAModule) -&gt; Optional[VLADataPacket]: &quot;&quot;&quot;Receive data from source module&quot;&quot;&quot; queue_key = f&quot;{source_module.value}_to_{dest_module.value}&quot; if queue_key in self.data_queues: try: return await asyncio.wait_for( self.data_queues[queue_key].get(), timeout=0.1 ) except asyncio.TimeoutError: return None return None class VLAIntegrationCoordinator: def __init__(self): &quot;&quot;&quot;Initialize VLA integration coordinator&quot;&quot;&quot; self.communication_bus = VLACommunicationBus() self.state = VLAState.IDLE self.system_health = { 'language_system_healthy': True, 'vision_system_healthy': True, 'action_system_healthy': True, 'control_system_healthy': True } self.active_tasks = [] self.execution_context = {} # Initialize VLA modules self.language_module = LanguageModule(self.communication_bus) self.vision_module = VisionModule(self.communication_bus) self.action_module = ActionModule(self.communication_bus) self.control_module = ControlModule(self.communication_bus) # Register modules with communication bus self.communication_bus.register_module(VLAModule.LANGUAGE, self.language_module.process) self.communication_bus.register_module(VLAModule.VISION, self.vision_module.process) self.communication_bus.register_module(VLAModule.ACTION_PLANNING, self.action_module.process) self.communication_bus.register_module(VLAModule.CONTROL, self.control_module.process) async def start_system(self): &quot;&quot;&quot;Start the complete VLA system&quot;&quot;&quot; print(&quot;Starting VLA Integration System...&quot;) # Start individual modules await self.language_module.start() await self.vision_module.start() await self.action_module.start() await self.control_module.start() # Start the main coordination loop self.coordination_task = asyncio.create_task(self._coordination_loop()) print(&quot;VLA System started successfully&quot;) async def stop_system(self): &quot;&quot;&quot;Stop the complete VLA system&quot;&quot;&quot; print(&quot;Stopping VLA Integration System...&quot;) # Cancel coordination task if hasattr(self, 'coordination_task'): self.coordination_task.cancel() # Stop individual modules await self.language_module.stop() await self.vision_module.stop() await self.action_module.stop() await self.control_module.stop() print(&quot;VLA System stopped&quot;) async def _coordination_loop(self): &quot;&quot;&quot;Main coordination loop for VLA system&quot;&quot;&quot; while True: try: # Monitor system health await self._monitor_system_health() # Process incoming data from all modules await self._process_module_data() # Update system state based on module statuses await self._update_system_state() # Handle any required adaptations or interventions await self._handle_system_adaptations() # Sleep briefly to prevent busy waiting await asyncio.sleep(0.01) # 10ms loop except asyncio.CancelledError: print(&quot;Coordination loop cancelled&quot;) break except Exception as e: print(f&quot;Error in coordination loop: {e}&quot;) await asyncio.sleep(0.1) # Longer sleep on error async def _monitor_system_health(self): &quot;&quot;&quot;Monitor health of all VLA modules&quot;&quot;&quot; # Check if modules are responding self.system_health['language_system_healthy'] = await self.language_module.is_healthy() self.system_health['vision_system_healthy'] = await self.vision_module.is_healthy() self.system_health['action_system_healthy'] = await self.action_module.is_healthy() self.system_health['control_system_healthy'] = await self.control_module.is_healthy() # Check for system-wide issues if not any(self.system_health.values()): self.state = VLAState.ERROR elif self.state != VLAState.SAFETY_STOP: # Determine appropriate state based on module activities if any(module.is_processing() for module in [ self.language_module, self.vision_module, self.action_module, self.control_module ]): self.state = VLAState.PROCESSING else: self.state = VLAState.IDLE async def _process_module_data(self): &quot;&quot;&quot;Process data flowing between modules&quot;&quot;&quot; # Process language to vision data lang_to_vision = await self.communication_bus.receive_data( VLAModule.LANGUAGE, VLAModule.VISION ) if lang_to_vision: await self.vision_module.handle_language_request(lang_to_vision) # Process vision to action data vision_to_action = await self.communication_bus.receive_data( VLAModule.VISION, VLAModule.ACTION_PLANNING ) if vision_to_action: await self.action_module.handle_vision_data(vision_to_action) # Process action to control data action_to_control = await self.communication_bus.receive_data( VLAModule.ACTION_PLANNING, VLAModule.CONTROL ) if action_to_control: await self.control_module.handle_action_plan(action_to_control) # Process control to execution data control_to_exec = await self.communication_bus.receive_data( VLAModule.CONTROL, VLAModule.EXECUTION ) if control_to_exec: # In simulation, execution is handled by control module pass async def _update_system_state(self): &quot;&quot;&quot;Update system state based on current activities&quot;&quot;&quot; # Update based on active tasks and module states if self.state == VLAState.ERROR: # Handle error state await self._handle_error_state() elif self.state == VLAState.SAFETY_STOP: # Ensure safety stop is active await self._ensure_safety_stop() else: # Normal operation pass async def _handle_system_adaptations(self): &quot;&quot;&quot;Handle system adaptations and interventions&quot;&quot;&quot; # Check for adaptation needs if await self._needs_adaptation(): await self._perform_adaptation() async def _needs_adaptation(self) -&gt; bool: &quot;&quot;&quot;Check if system adaptation is needed&quot;&quot;&quot; # Check for various conditions requiring adaptation if self.state == VLAState.ERROR: return True # Check for performance degradation if await self._is_performance_degrading(): return True # Check for environmental changes if await self._is_environment_changing(): return True return False async def _perform_adaptation(self): &quot;&quot;&quot;Perform system adaptation&quot;&quot;&quot; print(&quot;Performing system adaptation...&quot;) # Implementation would include: # - Re-planning actions based on new information # - Adjusting control parameters # - Requesting additional sensing # - Modifying execution plans async def _handle_error_state(self): &quot;&quot;&quot;Handle error state by ensuring safety&quot;&quot;&quot; print(&quot;Handling error state - activating safety protocols&quot;) # Stop all modules safely await self.control_module.emergency_stop() self.state = VLAState.SAFETY_STOP async def _ensure_safety_stop(self): &quot;&quot;&quot;Ensure safety stop is active&quot;&quot;&quot; await self.control_module.ensure_safety_stop() async def _is_performance_degrading(self) -&gt; bool: &quot;&quot;&quot;Check if system performance is degrading&quot;&quot;&quot; # This would monitor various performance metrics return False async def _is_environment_changing(self) -&gt; bool: &quot;&quot;&quot;Check if environment is changing significantly&quot;&quot;&quot; # This would analyze vision data for changes return False def submit_language_command(self, command: str, context: Dict = None) -&gt; str: &quot;&quot;&quot;Submit a language command to the VLA system&quot;&quot;&quot; correlation_id = f&quot;cmd_{int(time.time() * 1000)}&quot; packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.LANGUAGE, module_destination=VLAModule.LANGUAGE, data={ 'command': command, 'context': context or {}, 'correlation_id': correlation_id }, priority=2, correlation_id=correlation_id ) # Send to language module to start processing self.communication_bus.send_data(packet) return correlation_id async def get_system_status(self) -&gt; Dict: &quot;&quot;&quot;Get comprehensive system status&quot;&quot;&quot; return { 'state': self.state.value, 'health': self.system_health, 'modules': { 'language': await self.language_module.get_status(), 'vision': await self.vision_module.get_status(), 'action': await self.action_module.get_status(), 'control': await self.control_module.get_status() }, 'timestamp': time.time() }   ","version":"Next","tagName":"h3"},{"title":"Language Module Integration​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#language-module-integration","content":" class LanguageModule: def __init__(self, communication_bus: VLACommunicationBus): &quot;&quot;&quot;Initialize language processing module&quot;&quot;&quot; self.communication_bus = communication_bus self.processor = self._initialize_language_processor() self.is_running = False self.processing_queue = asyncio.Queue() self.active_processes = {} def _initialize_language_processor(self): &quot;&quot;&quot;Initialize the language processing system&quot;&quot;&quot; # In practice, this would initialize NLP models, parsers, etc. return { 'parser': None, # Would be actual parser 'intent_recognizer': None, # Would be actual intent recognizer 'context_manager': {}, # Context tracking 'command_validator': None # Command validation system } async def start(self): &quot;&quot;&quot;Start the language processing module&quot;&quot;&quot; self.is_running = True self.processing_task = asyncio.create_task(self._processing_loop()) print(&quot;Language module started&quot;) async def stop(self): &quot;&quot;&quot;Stop the language processing module&quot;&quot;&quot; self.is_running = False if hasattr(self, 'processing_task'): self.processing_task.cancel() print(&quot;Language module stopped&quot;) async def _processing_loop(self): &quot;&quot;&quot;Main processing loop for language module&quot;&quot;&quot; while self.is_running: try: # Process incoming commands if not self.processing_queue.empty(): command_data = await self.processing_queue.get() await self._process_command(command_data) # Process any queued tasks await self._process_queued_tasks() # Brief sleep to prevent busy waiting await asyncio.sleep(0.01) except asyncio.CancelledError: break except Exception as e: print(f&quot;Error in language processing loop: {e}&quot;) await asyncio.sleep(0.1) async def _process_command(self, command_data: Dict): &quot;&quot;&quot;Process a single language command&quot;&quot;&quot; command = command_data['command'] context = command_data.get('context', {}) correlation_id = command_data.get('correlation_id') try: # Parse the command parsed_result = await self._parse_command(command) # Validate the command if not await self._validate_command(parsed_result): raise ValueError(f&quot;Invalid command: {command}&quot;) # Generate vision request if needed vision_request = await self._generate_vision_request(parsed_result, context) if vision_request: vision_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.LANGUAGE, module_destination=VLAModule.VISION, data={ 'request': vision_request, 'context': context, 'correlation_id': correlation_id }, priority=2 ) self.communication_bus.send_data(vision_packet) # If no vision needed, proceed to action planning if not vision_request: action_request = await self._generate_action_request(parsed_result) action_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.LANGUAGE, module_destination=VLAModule.ACTION_PLANNING, data={ 'request': action_request, 'context': context, 'correlation_id': correlation_id }, priority=2 ) self.communication_bus.send_data(action_packet) except Exception as e: print(f&quot;Error processing command '{command}': {e}&quot;) # Send error notification error_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.LANGUAGE, module_destination=VLAModule.CONTROL, data={ 'error': str(e), 'correlation_id': correlation_id }, priority=3 ) self.communication_bus.send_data(error_packet) async def _parse_command(self, command: str) -&gt; Dict: &quot;&quot;&quot;Parse natural language command into structured format&quot;&quot;&quot; # This would use actual NLP processing # For demonstration, we'll do simple parsing command_lower = command.lower() if 'go to' in command_lower or 'move to' in command_lower: action = 'navigate' target = self._extract_location(command_lower) elif 'pick up' in command_lower or 'grasp' in command_lower: action = 'grasp' target = self._extract_object(command_lower) elif 'place' in command_lower or 'put' in command_lower: action = 'place' target = self._extract_object(command_lower) else: action = 'unknown' target = '' return { 'action': action, 'target': target, 'original_command': command, 'confidence': 0.9 # High confidence for demo } def _extract_location(self, command: str) -&gt; str: &quot;&quot;&quot;Extract location from command&quot;&quot;&quot; # Simple location extraction for demo locations = ['kitchen', 'living room', 'bedroom', 'office', 'hallway'] for loc in locations: if loc in command: return loc return 'unknown_location' def _extract_object(self, command: str) -&gt; str: &quot;&quot;&quot;Extract object from command&quot;&quot;&quot; # Simple object extraction for demo objects = ['cup', 'book', 'phone', 'bottle', 'box', 'chair'] for obj in objects: if obj in command: return obj return 'unknown_object' async def _validate_command(self, parsed_result: Dict) -&gt; bool: &quot;&quot;&quot;Validate parsed command&quot;&quot;&quot; # Check if action is supported supported_actions = ['navigate', 'grasp', 'place', 'greet', 'follow'] return parsed_result['action'] in supported_actions async def _generate_vision_request(self, parsed_result: Dict, context: Dict) -&gt; Optional[Dict]: &quot;&quot;&quot;Generate vision request based on parsed command&quot;&quot;&quot; action = parsed_result['action'] target = parsed_result['target'] # Determine if vision is needed if action in ['grasp', 'navigate'] and target != 'unknown': # Vision needed to locate target object/location return { 'task': 'locate_target', 'target': target, 'task_priority': 2 } return None async def _generate_action_request(self, parsed_result: Dict) -&gt; Dict: &quot;&quot;&quot;Generate action planning request&quot;&quot;&quot; return { 'task': 'execute_command', 'parsed_command': parsed_result, 'task_priority': 2 } async def process(self, data: Dict): &quot;&quot;&quot;Process incoming data for this module&quot;&quot;&quot; # This method would be called by the coordinator # For now, just add to processing queue await self.processing_queue.put(data) async def handle_vision_feedback(self, vision_data: Dict): &quot;&quot;&quot;Handle feedback from vision system&quot;&quot;&quot; # Process vision results and generate appropriate action plan correlation_id = vision_data.get('correlation_id') # Generate action plan based on vision results action_request = await self._generate_action_from_vision(vision_data) action_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.LANGUAGE, module_destination=VLAModule.ACTION_PLANNING, data={ 'request': action_request, 'vision_results': vision_data, 'correlation_id': correlation_id }, priority=2 ) self.communication_bus.send_data(action_packet) async def _generate_action_from_vision(self, vision_data: Dict) -&gt; Dict: &quot;&quot;&quot;Generate action request based on vision results&quot;&quot;&quot; # Create action plan using vision results return { 'task': 'execute_with_vision_guidance', 'vision_data': vision_data, 'task_priority': 2 } async def is_healthy(self) -&gt; bool: &quot;&quot;&quot;Check if language module is healthy&quot;&quot;&quot; return self.is_running def is_processing(self) -&gt; bool: &quot;&quot;&quot;Check if language module is actively processing&quot;&quot;&quot; return not self.processing_queue.empty() async def get_status(self) -&gt; Dict: &quot;&quot;&quot;Get language module status&quot;&quot;&quot; return { 'running': self.is_running, 'queue_size': self.processing_queue.qsize(), 'active_processes': len(self.active_processes) }   ","version":"Next","tagName":"h2"},{"title":"Vision Module Integration​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#vision-module-integration","content":" class VisionModule: def __init__(self, communication_bus: VLACommunicationBus): &quot;&quot;&quot;Initialize vision processing module&quot;&quot;&quot; self.communication_bus = communication_bus self.processor = self._initialize_vision_processor() self.is_running = False self.processing_queue = asyncio.Queue() self.active_processes = {} self.scene_cache = {} # Cache recent scene analyses def _initialize_vision_processor(self): &quot;&quot;&quot;Initialize vision processing system&quot;&quot;&quot; # In practice, this would initialize computer vision models, etc. return { 'object_detector': None, # Would be actual detector 'pose_estimator': None, # Would be actual pose estimator 'scene_analyzer': None, # Would be actual scene analyzer 'tracking_system': None, # Would be actual tracker 'depth_processor': None # Would be actual depth processor } async def start(self): &quot;&quot;&quot;Start the vision processing module&quot;&quot;&quot; self.is_running = True self.processing_task = asyncio.create_task(self._processing_loop()) self.sensing_task = asyncio.create_task(self._sensing_loop()) print(&quot;Vision module started&quot;) async def stop(self): &quot;&quot;&quot;Stop the vision processing module&quot;&quot;&quot; self.is_running = False if hasattr(self, 'processing_task'): self.processing_task.cancel() if hasattr(self, 'sensing_task'): self.sensing_task.cancel() print(&quot;Vision module stopped&quot;) async def _processing_loop(self): &quot;&quot;&quot;Main processing loop for vision module&quot;&quot;&quot; while self.is_running: try: # Process incoming requests if not self.processing_queue.empty(): request_data = await self.processing_queue.get() await self._process_vision_request(request_data) # Process any queued tasks await self._process_queued_tasks() # Brief sleep to prevent busy waiting await asyncio.sleep(0.01) except asyncio.CancelledError: break except Exception as e: print(f&quot;Error in vision processing loop: {e}&quot;) await asyncio.sleep(0.1) async def _sensing_loop(self): &quot;&quot;&quot;Continuous sensing loop for environment monitoring&quot;&quot;&quot; while self.is_running: try: # Perform continuous environment sensing current_scene = await self._sense_current_environment() # Cache the scene for quick access self.scene_cache[time.time()] = current_scene # Check if significant changes occurred if await self._has_significant_changes(current_scene): # Notify other modules of environment change change_notification = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.VISION, module_destination=VLAModule.ACTION_PLANNING, data={ 'notification': 'environment_change', 'new_scene_data': current_scene }, priority=1 ) self.communication_bus.send_data(change_notification) # Sleep for next sensing cycle (e.g., 30fps) await asyncio.sleep(1.0/30.0) # 30Hz sensing except asyncio.CancelledError: break except Exception as e: print(f&quot;Error in vision sensing loop: {e}&quot;) await asyncio.sleep(0.5) async def _sense_current_environment(self) -&gt; Dict: &quot;&quot;&quot;Sense current environment and return scene description&quot;&quot;&quot; # This would interface with actual cameras and sensors # For demonstration, return mock data return { 'timestamp': time.time(), 'objects': [ {'name': 'red cup', 'position': [1.5, 0.5, 0.0], 'confidence': 0.95}, {'name': 'book', 'position': [0.8, 0.2, 0.0], 'confidence': 0.92}, {'name': 'chair', 'position': [2.0, 1.0, 0.0], 'confidence': 0.88} ], 'locations': [ {'name': 'kitchen', 'position': [3.0, 0.0, 0.0]}, {'name': 'living room', 'position': [0.0, 2.0, 0.0]} ], 'robot_position': [0.0, 0.0, 0.0], 'traversable_map': [[1]*10 for _ in range(10)] # Mock map } async def _process_vision_request(self, request_data: Dict): &quot;&quot;&quot;Process a vision request&quot;&quot;&quot; request = request_data['request'] correlation_id = request_data.get('correlation_id') try: # Determine the type of vision task if request['task'] == 'locate_target': target = request['target'] vision_result = await self._locate_target(target) # Send result back to requesting module result_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.VISION, module_destination=VLAModule.LANGUAGE, # Send back to language data={ 'task_result': 'target_located', 'target': target, 'location': vision_result.get('position'), 'confidence': vision_result.get('confidence', 0.0), 'correlation_id': correlation_id }, priority=2 ) self.communication_bus.send_data(result_packet) elif request['task'] == 'analyze_scene': scene_analysis = await self._analyze_scene() result_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.VISION, module_destination=VLAModule.ACTION_PLANNING, data={ 'task_result': 'scene_analyzed', 'scene_data': scene_analysis, 'correlation_id': correlation_id }, priority=2 ) self.communication_bus.send_data(result_packet) except Exception as e: print(f&quot;Error processing vision request: {e}&quot;) # Send error notification error_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.VISION, module_destination=VLAModule.CONTROL, data={ 'error': str(e), 'correlation_id': correlation_id }, priority=3 ) self.communication_bus.send_data(error_packet) async def _locate_target(self, target_name: str) -&gt; Dict: &quot;&quot;&quot;Locate a specific target in the environment&quot;&quot;&quot; # Get current scene current_scene = await self._sense_current_environment() # Search for target in detected objects for obj in current_scene['objects']: if target_name.lower() in obj['name'].lower(): return { 'position': obj['position'], 'confidence': obj['confidence'], 'object_details': obj } # If not found, search in locations for loc in current_scene['locations']: if target_name.lower() in loc['name'].lower(): return { 'position': loc['position'], 'confidence': 0.8, # Lower confidence for location 'location_details': loc } # Target not found return { 'position': None, 'confidence': 0.0, 'found': False } async def _analyze_scene(self) -&gt; Dict: &quot;&quot;&quot;Perform comprehensive scene analysis&quot;&quot;&quot; current_scene = await self._sense_current_environment() # Perform additional analysis analysis = { 'object_count': len(current_scene['objects']), 'traversable_areas': self._identify_traversable_areas(current_scene), 'obstacles': self._identify_obstacles(current_scene), 'navigation_targets': self._identify_navigation_targets(current_scene), 'manipulation_targets': self._identify_manipulation_targets(current_scene) } return {**current_scene, **analysis} def _identify_traversable_areas(self, scene: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Identify traversable areas in the scene&quot;&quot;&quot; # This would use the traversable map and object positions # For demo, return mock data return [ {'center': [1.0, 1.0], 'radius': 0.5, 'traversable': True}, {'center': [2.0, 2.0], 'radius': 0.3, 'traversable': True} ] def _identify_obstacles(self, scene: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Identify obstacles in the scene&quot;&quot;&quot; obstacles = [] for obj in scene['objects']: if obj['confidence'] &gt; 0.7: # High confidence detections obstacles.append({ 'name': obj['name'], 'position': obj['position'], 'size_estimate': self._estimate_object_size(obj) }) return obstacles def _estimate_object_size(self, obj: Dict) -&gt; Dict: &quot;&quot;&quot;Estimate object size from detection&quot;&quot;&quot; # Simplified size estimation return { 'width': 0.1, # 10cm default 'height': 0.1, 'depth': 0.1 } def _identify_navigation_targets(self, scene: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Identify potential navigation targets&quot;&quot;&quot; targets = [] for loc in scene['locations']: targets.append({ 'name': loc['name'], 'position': loc['position'], 'type': 'location' }) return targets def _identify_manipulation_targets(self, scene: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Identify potential manipulation targets&quot;&quot;&quot; targets = [] for obj in scene['objects']: if obj['confidence'] &gt; 0.8: # High confidence for manipulation targets.append({ 'name': obj['name'], 'position': obj['position'], 'graspable': self._is_object_graspable(obj) }) return targets def _is_object_graspable(self, obj: Dict) -&gt; bool: &quot;&quot;&quot;Determine if an object is graspable&quot;&quot;&quot; # Simplified graspability check # In practice, would consider size, shape, weight, etc. return obj['confidence'] &gt; 0.8 async def _has_significant_changes(self, new_scene: Dict) -&gt; bool: &quot;&quot;&quot;Check if there are significant changes in the scene&quot;&quot;&quot; # Compare with cached scenes to detect changes if not self.scene_cache: return True # First scene is always a change # For demo, return True periodically return int(time.time()) % 10 == 0 # Change every 10 seconds async def _process_queued_tasks(self): &quot;&quot;&quot;Process any queued vision tasks&quot;&quot;&quot; # Implementation would process queued vision tasks pass async def process(self, data: Dict): &quot;&quot;&quot;Process incoming data for this module&quot;&quot;&quot; await self.processing_queue.put(data) async def handle_language_request(self, language_request: VLADataPacket): &quot;&quot;&quot;Handle vision request from language module&quot;&quot;&quot; # Process the request and generate appropriate response await self.processing_queue.put({ 'request_type': 'language_guided_vision', 'data': language_request.data, 'correlation_id': language_request.correlation_id }) async def is_healthy(self) -&gt; bool: &quot;&quot;&quot;Check if vision module is healthy&quot;&quot;&quot; return self.is_running def is_processing(self) -&gt; bool: &quot;&quot;&quot;Check if vision module is actively processing&quot;&quot;&quot; return not self.processing_queue.empty() async def get_status(self) -&gt; Dict: &quot;&quot;&quot;Get vision module status&quot;&quot;&quot; return { 'running': self.is_running, 'queue_size': self.processing_queue.qsize(), 'active_processes': len(self.active_processes), 'cached_scenes': len(self.scene_cache) }   ","version":"Next","tagName":"h2"},{"title":"Action Planning Module Integration​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#action-planning-module-integration","content":" class ActionModule: def __init__(self, communication_bus: VLACommunicationBus): &quot;&quot;&quot;Initialize action planning module&quot;&quot;&quot; self.communication_bus = communication_bus self.planner = self._initialize_action_planner() self.is_running = False self.processing_queue = asyncio.Queue() self.active_plans = {} self.world_state = {} def _initialize_action_planner(self): &quot;&quot;&quot;Initialize action planning system&quot;&quot;&quot; return { 'task_planner': None, # Would be actual task planner 'motion_planner': None, # Would be actual motion planner 'constraint_solver': None, # Would be actual constraint solver 'validator': None, # Would be actual plan validator 'optimizer': None # Would be actual plan optimizer } async def start(self): &quot;&quot;&quot;Start the action planning module&quot;&quot;&quot; self.is_running = True self.processing_task = asyncio.create_task(self._processing_loop()) print(&quot;Action planning module started&quot;) async def stop(self): &quot;&quot;&quot;Stop the action planning module&quot;&quot;&quot; self.is_running = False if hasattr(self, 'processing_task'): self.processing_task.cancel() print(&quot;Action planning module stopped&quot;) async def _processing_loop(self): &quot;&quot;&quot;Main processing loop for action module&quot;&quot;&quot; while self.is_running: try: # Process incoming requests if not self.processing_queue.empty(): request_data = await self.processing_queue.get() await self._process_action_request(request_data) # Process any queued tasks await self._process_queued_tasks() # Brief sleep to prevent busy waiting await asyncio.sleep(0.01) except asyncio.CancelledError: break except Exception as e: print(f&quot;Error in action processing loop: {e}&quot;) await asyncio.sleep(0.1) async def _process_action_request(self, request_data: Dict): &quot;&quot;&quot;Process an action planning request&quot;&quot;&quot; request = request_data['request'] correlation_id = request_data.get('correlation_id') try: # Determine request type if request['task'] == 'execute_command': parsed_command = request['parsed_command'] action_plan = await self._plan_command_execution(parsed_command) elif request['task'] == 'execute_with_vision_guidance': vision_data = request['vision_data'] action_plan = await self._plan_with_vision_guidance(vision_data) elif request['task'] == 'analyze_scene': scene_data = request['scene_data'] action_plan = await self._plan_based_on_scene(scene_data) else: raise ValueError(f&quot;Unknown action request type: {request['task']}&quot;) # Validate the plan if not await self._validate_plan(action_plan): raise ValueError(&quot;Generated plan failed validation&quot;) # Send plan to control module plan_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.ACTION_PLANNING, module_destination=VLAModule.CONTROL, data={ 'action_plan': action_plan, 'correlation_id': correlation_id }, priority=2 ) self.communication_bus.send_data(plan_packet) # Store plan for monitoring self.active_plans[correlation_id] = action_plan except Exception as e: print(f&quot;Error processing action request: {e}&quot;) # Send error notification error_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.ACTION_PLANNING, module_destination=VLAModule.CONTROL, data={ 'error': str(e), 'correlation_id': correlation_id }, priority=3 ) self.communication_bus.send_data(error_packet) async def _plan_command_execution(self, parsed_command: Dict) -&gt; Dict: &quot;&quot;&quot;Plan execution of a parsed command&quot;&quot;&quot; action = parsed_command['action'] target = parsed_command['target'] if action == 'navigate': # Plan navigation to target location return await self._plan_navigation(target) elif action == 'grasp': # Plan grasping of target object return await self._plan_grasping(target) elif action == 'place': # Plan placing of object at location return await self._plan_placement(target) elif action == 'greet': # Plan greeting action return await self._plan_greeting(target) else: # Unknown action - return empty plan return { 'actions': [], 'duration': 0.0, 'success_criteria': [], 'failure_criteria': [] } async def _plan_navigation(self, target_location: str) -&gt; Dict: &quot;&quot;&quot;Plan navigation action&quot;&quot;&quot; # In practice, this would use path planning algorithms # For demo, create a simple navigation plan return { 'actions': [ { 'type': 'navigate', 'target_location': target_location, 'estimated_duration': 5.0, # 5 seconds 'preconditions': ['robot_is_idle'], 'postconditions': ['robot_at_target'], 'priority': 2 }, { 'type': 'balance_adjust', 'target_pose': [0, 0, 0], 'estimated_duration': 1.0, 'preconditions': ['navigation_completed'], 'postconditions': ['robot_balanced'], 'priority': 3 } ], 'duration': 6.0, 'success_criteria': ['robot_at_target', 'robot_balanced'], 'failure_criteria': ['obstacle_detected', 'timeout'] } async def _plan_grasping(self, target_object: str) -&gt; Dict: &quot;&quot;&quot;Plan grasping action&quot;&quot;&quot; return { 'actions': [ { 'type': 'approach_object', 'target_object': target_object, 'estimated_duration': 3.0, 'preconditions': ['robot_at_approach_position'], 'postconditions': ['robot_at_grasp_position'], 'priority': 2 }, { 'type': 'grasp_object', 'target_object': target_object, 'estimated_duration': 2.0, 'preconditions': ['robot_at_grasp_position'], 'postconditions': ['object_grasped'], 'priority': 3 }, { 'type': 'lift_object', 'estimated_duration': 1.0, 'preconditions': ['object_grasped'], 'postconditions': ['object_lifted'], 'priority': 2 } ], 'duration': 6.0, 'success_criteria': ['object_grasped', 'object_lifted'], 'failure_criteria': ['grasp_failed', 'object_dropped', 'timeout'] } async def _plan_placement(self, target_location: str) -&gt; Dict: &quot;&quot;&quot;Plan placement action&quot;&quot;&quot; return { 'actions': [ { 'type': 'navigate', 'target_location': target_location, 'estimated_duration': 4.0, 'preconditions': ['object_grasped'], 'postconditions': ['robot_at_placement_position'], 'priority': 2 }, { 'type': 'place_object', 'target_location': target_location, 'estimated_duration': 2.0, 'preconditions': ['robot_at_placement_position'], 'postconditions': ['object_placed'], 'priority': 3 }, { 'type': 'release_gripper', 'estimated_duration': 0.5, 'preconditions': ['object_placed'], 'postconditions': ['gripper_released'], 'priority': 2 } ], 'duration': 6.5, 'success_criteria': ['object_placed', 'gripper_released'], 'failure_criteria': ['placement_failed', 'object_dropped', 'timeout'] } async def _plan_greeting(self, target_person: str) -&gt; Dict: &quot;&quot;&quot;Plan greeting action&quot;&quot;&quot; return { 'actions': [ { 'type': 'navigate', 'target_location': 'near_' + target_person, 'estimated_duration': 3.0, 'preconditions': ['person_detected'], 'postconditions': ['robot_near_person'], 'priority': 2 }, { 'type': 'turn_towards_person', 'estimated_duration': 1.0, 'preconditions': ['robot_near_person'], 'postconditions': ['facing_person'], 'priority': 3 }, { 'type': 'greet_person', 'estimated_duration': 2.0, 'preconditions': ['facing_person'], 'postconditions': ['greeting_delivered'], 'priority': 3 } ], 'duration': 6.0, 'success_criteria': ['greeting_delivered'], 'failure_criteria': ['person_not_found', 'timeout'] } async def _plan_with_vision_guidance(self, vision_data: Dict) -&gt; Dict: &quot;&quot;&quot;Plan action based on vision guidance&quot;&quot;&quot; # Use vision data to refine action plan target_location = vision_data.get('location') if target_location: return await self._plan_navigation(target_location) # For other vision-guided actions, use appropriate planning return await self._plan_command_execution({ 'action': 'unknown', 'target': 'vision_guided', 'original_command': 'vision_guided_action' }) async def _plan_based_on_scene(self, scene_data: Dict) -&gt; Dict: &quot;&quot;&quot;Plan actions based on scene analysis&quot;&quot;&quot; # Generate appropriate plans based on scene # For demo, create a simple exploration plan return { 'actions': [ { 'type': 'explore_scene', 'estimated_duration': 10.0, 'preconditions': ['robot_idle'], 'postconditions': ['scene_explored'], 'priority': 1 } ], 'duration': 10.0, 'success_criteria': ['scene_explored'], 'failure_criteria': ['exploration_timeout'] } async def _validate_plan(self, plan: Dict) -&gt; bool: &quot;&quot;&quot;Validate that the plan is feasible&quot;&quot;&quot; # Check if plan has required fields required_fields = ['actions', 'duration', 'success_criteria'] for field in required_fields: if field not in plan: return False # Check if actions have required fields for action in plan.get('actions', []): if 'type' not in action or 'estimated_duration' not in action: return False # Check for basic feasibility if plan['duration'] &lt;= 0: return False return True async def _process_queued_tasks(self): &quot;&quot;&quot;Process any queued action planning tasks&quot;&quot;&quot; pass async def process(self, data: Dict): &quot;&quot;&quot;Process incoming data for this module&quot;&quot;&quot; await self.processing_queue.put(data) async def handle_vision_data(self, vision_packet: VLADataPacket): &quot;&quot;&quot;Handle vision data from vision module&quot;&quot;&quot; vision_data = vision_packet.data correlation_id = vision_packet.correlation_id # Create action plan based on vision data action_plan = await self._plan_with_vision_guidance(vision_data) # Send to control module plan_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.ACTION_PLANNING, module_destination=VLAModule.CONTROL, data={ 'action_plan': action_plan, 'vision_context': vision_data, 'correlation_id': correlation_id }, priority=2 ) self.communication_bus.send_data(plan_packet) async def is_healthy(self) -&gt; bool: &quot;&quot;&quot;Check if action module is healthy&quot;&quot;&quot; return self.is_running def is_processing(self) -&gt; bool: &quot;&quot;&quot;Check if action module is actively processing&quot;&quot;&quot; return not self.processing_queue.empty() async def get_status(self) -&gt; Dict: &quot;&quot;&quot;Get action module status&quot;&quot;&quot; return { 'running': self.is_running, 'queue_size': self.processing_queue.qsize(), 'active_plans': len(self.active_plans), 'world_state_size': len(self.world_state) }   ","version":"Next","tagName":"h2"},{"title":"Control Module Integration​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#control-module-integration","content":" class ControlModule: def __init__(self, communication_bus: VLACommunicationBus): &quot;&quot;&quot;Initialize control module for real-time robot control&quot;&quot;&quot; self.communication_bus = communication_bus self.controller = self._initialize_controller() self.is_running = False self.execution_queue = asyncio.Queue() self.active_execution = {} self.safety_system = SafetySystem() self.real_time_loop = None def _initialize_controller(self): &quot;&quot;&quot;Initialize robot controller&quot;&quot;&quot; return { 'balance_controller': None, # Would be actual balance controller 'motion_controller': None, # Would be actual motion controller 'gripper_controller': None, # Would be actual gripper controller 'navigation_controller': None, # Would be actual nav controller 'scheduler': None # Would be actual task scheduler } async def start(self): &quot;&quot;&quot;Start the control module&quot;&quot;&quot; self.is_running = True self.execution_task = asyncio.create_task(self._execution_loop()) self.real_time_loop = asyncio.create_task(self._real_time_control_loop()) print(&quot;Control module started&quot;) async def stop(self): &quot;&quot;&quot;Stop the control module&quot;&quot;&quot; self.is_running = False if hasattr(self, 'execution_task'): self.execution_task.cancel() if hasattr(self, 'real_time_loop'): self.real_time_loop.cancel() print(&quot;Control module stopped&quot;) async def _execution_loop(self): &quot;&quot;&quot;Main execution loop for control module&quot;&quot;&quot; while self.is_running: try: # Process incoming action plans if not self.execution_queue.empty(): plan_data = await self.execution_queue.get() await self._execute_action_plan(plan_data) # Monitor active executions await self._monitor_active_executions() # Check safety systems await self.safety_system.monitor_safety() # Brief sleep to prevent busy waiting await asyncio.sleep(0.01) except asyncio.CancelledError: break except Exception as e: print(f&quot;Error in control execution loop: {e}&quot;) await asyncio.sleep(0.1) async def _real_time_control_loop(self): &quot;&quot;&quot;Real-time control loop (100Hz for humanoid balance)&quot;&quot;&quot; control_period = 1.0 / 100.0 # 100Hz while self.is_running: loop_start = time.time() try: # Perform real-time control tasks await self._perform_real_time_control() # Calculate control time control_time = time.time() - loop_start # Sleep to maintain control frequency sleep_time = control_period - control_time if sleep_time &gt; 0: await asyncio.sleep(sleep_time) else: # Loop overrun - log but continue print(f&quot;Real-time control loop overrun by {control_time - control_period:.4f}s&quot;) except asyncio.CancelledError: break except Exception as e: print(f&quot;Error in real-time control loop: {e}&quot;) await asyncio.sleep(0.01) async def _perform_real_time_control(self): &quot;&quot;&quot;Perform real-time control tasks&quot;&quot;&quot; # Update balance control await self._update_balance_control() # Update motion control await self._update_motion_control() # Update gripper control await self._update_gripper_control() # Update safety monitoring await self.safety_system.update_monitors() async def _update_balance_control(self): &quot;&quot;&quot;Update real-time balance control&quot;&quot;&quot; # This would interface with actual balance controller # For demo, just simulate balance control update pass async def _update_motion_control(self): &quot;&quot;&quot;Update real-time motion control&quot;&quot;&quot; # This would interface with actual motion controller # For demo, just simulate motion control update pass async def _update_gripper_control(self): &quot;&quot;&quot;Update real-time gripper control&quot;&quot;&quot; # This would interface with actual gripper controller # For demo, just simulate gripper control update pass async def _execute_action_plan(self, plan_data: Dict): &quot;&quot;&quot;Execute an action plan&quot;&quot;&quot; action_plan = plan_data['action_plan'] correlation_id = plan_data.get('correlation_id') try: # Start execution of the plan execution_id = f&quot;exec_{int(time.time() * 1000)}&quot; # Initialize execution state execution_state = { 'id': execution_id, 'plan': action_plan, 'current_step': 0, 'status': 'executing', 'start_time': time.time(), 'correlation_id': correlation_id } # Store active execution self.active_execution[execution_id] = execution_state # Execute each action in the plan for i, action in enumerate(action_plan['actions']): execution_state['current_step'] = i success = await self._execute_single_action(action) if not success: execution_state['status'] = 'failed' execution_state['failure_reason'] = f'Action {i} failed' break # Update final state execution_state['end_time'] = time.time() execution_state['status'] = 'completed' if execution_state['status'] != 'failed' else 'failed' # Send completion notification completion_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.CONTROL, module_destination=VLAModule.ACTION_PLANNING, data={ 'execution_id': execution_id, 'status': execution_state['status'], 'correlation_id': correlation_id }, priority=2 ) self.communication_bus.send_data(completion_packet) except Exception as e: print(f&quot;Error executing action plan: {e}&quot;) # Send error notification error_packet = VLADataPacket( timestamp=time.time(), module_origin=VLAModule.CONTROL, module_destination=VLAModule.ACTION_PLANNING, data={ 'error': str(e), 'correlation_id': correlation_id }, priority=3 ) self.communication_bus.send_data(error_packet) async def _execute_single_action(self, action: Dict) -&gt; bool: &quot;&quot;&quot;Execute a single action&quot;&quot;&quot; action_type = action['type'] estimated_duration = action['estimated_duration'] try: if action_type == 'navigate': success = await self._execute_navigation(action) elif action_type == 'grasp_object': success = await self._execute_grasping(action) elif action_type == 'place_object': success = await self._execute_placement(action) elif action_type == 'balance_adjust': success = await self._execute_balance_adjustment(action) else: # Unknown action type - treat as successful success = True await asyncio.sleep(estimated_duration) return success except Exception as e: print(f&quot;Error executing action {action_type}: {e}&quot;) return False async def _execute_navigation(self, action: Dict) -&gt; bool: &quot;&quot;&quot;Execute navigation action&quot;&quot;&quot; target_location = action.get('target_location', [0, 0, 0]) estimated_duration = action['estimated_duration'] # This would interface with actual navigation system # For demo, simulate navigation await asyncio.sleep(estimated_duration * 0.8) # Simulate 80% of estimated time # Check if navigation succeeded # In practice, would check actual robot position return True async def _execute_grasping(self, action: Dict) -&gt; bool: &quot;&quot;&quot;Execute grasping action&quot;&quot;&quot; target_object = action.get('target_object', 'unknown') estimated_duration = action['estimated_duration'] # This would interface with actual manipulation system # For demo, simulate grasping await asyncio.sleep(estimated_duration * 0.9) # Simulate 90% of estimated time # Check if grasp succeeded # In practice, would check force/torque sensors return True async def _execute_placement(self, action: Dict) -&gt; bool: &quot;&quot;&quot;Execute placement action&quot;&quot;&quot; target_location = action.get('target_location', 'unknown') estimated_duration = action['estimated_duration'] # This would interface with actual manipulation system # For demo, simulate placement await asyncio.sleep(estimated_duration * 0.85) # Simulate 85% of estimated time # Check if placement succeeded return True async def _execute_balance_adjustment(self, action: Dict) -&gt; bool: &quot;&quot;&quot;Execute balance adjustment action&quot;&quot;&quot; target_pose = action.get('target_pose', [0, 0, 0]) estimated_duration = action['estimated_duration'] # This would interface with actual balance controller # For demo, simulate balance adjustment await asyncio.sleep(estimated_duration * 0.7) # Simulate 70% of estimated time # Check if balance adjustment succeeded return True async def _monitor_active_executions(self): &quot;&quot;&quot;Monitor active executions for completion or issues&quot;&quot;&quot; current_time = time.time() completed_executions = [] for exec_id, exec_state in self.active_execution.items(): if exec_state['status'] in ['completed', 'failed']: completed_executions.append(exec_id) # Remove completed executions for exec_id in completed_executions: del self.active_execution[exec_id] async def emergency_stop(self): &quot;&quot;&quot;Emergency stop all active executions&quot;&quot;&quot; print(&quot;EMERGENCY STOP ACTIVATED&quot;) # Stop all active executions for exec_id, exec_state in self.active_execution.items(): exec_state['status'] = 'emergency_stopped' # Clear all active executions self.active_execution.clear() # Activate safety systems await self.safety_system.activate_emergency_stop() async def ensure_safety_stop(self): &quot;&quot;&quot;Ensure safety stop is active&quot;&quot;&quot; await self.safety_system.ensure_safety() async def process(self, data: Dict): &quot;&quot;&quot;Process incoming data for this module&quot;&quot;&quot; await self.execution_queue.put(data) async def handle_action_plan(self, plan_packet: VLADataPacket): &quot;&quot;&quot;Handle action plan from action planning module&quot;&quot;&quot; plan_data = { 'action_plan': plan_packet.data['action_plan'], 'correlation_id': plan_packet.correlation_id } await self.execution_queue.put(plan_data) async def is_healthy(self) -&gt; bool: &quot;&quot;&quot;Check if control module is healthy&quot;&quot;&quot; return self.is_running and await self.safety_system.is_system_safe() def is_processing(self) -&gt; bool: &quot;&quot;&quot;Check if control module is actively processing&quot;&quot;&quot; return not self.execution_queue.empty() or bool(self.active_execution) async def get_status(self) -&gt; Dict: &quot;&quot;&quot;Get control module status&quot;&quot;&quot; return { 'running': self.is_running, 'queue_size': self.execution_queue.qsize(), 'active_executions': len(self.active_execution), 'safety_status': await self.safety_system.get_safety_status() } class SafetySystem: &quot;&quot;&quot;Safety system for humanoid robot control&quot;&quot;&quot; def __init__(self): self.safety_enabled = True self.emergency_stop_active = False self.safety_limits = { 'max_torque': 100.0, # Nm 'max_velocity': 2.0, # rad/s 'max_acceleration': 5.0, # rad/s² 'max_force': 500.0, # N 'max_power': 1000.0 # W } self.monitoring_data = { 'joint_torques': [], 'joint_velocities': [], 'forces': [], 'powers': [], 'timestamps': [] } async def monitor_safety(self): &quot;&quot;&quot;Monitor safety parameters&quot;&quot;&quot; if not self.safety_enabled: return # Check for safety violations violations = await self._check_safety_violations() if violations: print(f&quot;Safety violations detected: {violations}&quot;) if self._should_emergency_stop(violations): await self.activate_emergency_stop() async def _check_safety_violations(self) -&gt; List[str]: &quot;&quot;&quot;Check for safety violations&quot;&quot;&quot; violations = [] # Check torque limits for torque in self.monitoring_data['joint_torques'][-10:]: # Check last 10 readings if torque &gt; self.safety_limits['max_torque']: violations.append(f&quot;Torque limit exceeded: {torque}&quot;) # Check velocity limits for vel in self.monitoring_data['joint_velocities'][-10:]: if vel &gt; self.safety_limits['max_velocity']: violations.append(f&quot;Velocity limit exceeded: {vel}&quot;) # Check force limits for force in self.monitoring_data['forces'][-10:]: if force &gt; self.safety_limits['max_force']: violations.append(f&quot;Force limit exceeded: {force}&quot;) return violations def _should_emergency_stop(self, violations: List[str]) -&gt; bool: &quot;&quot;&quot;Determine if emergency stop should be activated&quot;&quot;&quot; # For demo, emergency stop if any critical violation critical_violations = [ v for v in violations if 'torque' in v.lower() or 'force' in v.lower() ] return len(critical_violations) &gt; 0 async def activate_emergency_stop(self): &quot;&quot;&quot;Activate emergency stop&quot;&quot;&quot; if not self.emergency_stop_active: print(&quot;EMERGENCY STOP ACTIVATED&quot;) self.emergency_stop_active = True # In practice, would send emergency stop commands to robot async def ensure_safety(self): &quot;&quot;&quot;Ensure safety systems are active&quot;&quot;&quot; self.emergency_stop_active = True # In practice, would ensure all safety systems are engaged async def update_monitors(self): &quot;&quot;&quot;Update safety monitoring data&quot;&quot;&quot; # This would get real data from robot sensors # For demo, add dummy data import random self.monitoring_data['joint_torques'].append(random.uniform(0, 80)) self.monitoring_data['joint_velocities'].append(random.uniform(0, 1.5)) self.monitoring_data['forces'].append(random.uniform(0, 400)) self.monitoring_data['powers'].append(random.uniform(0, 800)) self.monitoring_data['timestamps'].append(time.time()) # Keep only recent data (last 100 readings) for key in self.monitoring_data: if len(self.monitoring_data[key]) &gt; 100: self.monitoring_data[key] = self.monitoring_data[key][-100:] async def is_system_safe(self) -&gt; bool: &quot;&quot;&quot;Check if system is currently safe&quot;&quot;&quot; return not self.emergency_stop_active async def get_safety_status(self) -&gt; Dict: &quot;&quot;&quot;Get safety system status&quot;&quot;&quot; return { 'safety_enabled': self.safety_enabled, 'emergency_stop_active': self.emergency_stop_active, 'safety_limits': self.safety_limits, 'recent_violations': await self._check_safety_violations() }   ","version":"Next","tagName":"h2"},{"title":"Performance Monitoring and Validation​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#performance-monitoring-and-validation","content":" ","version":"Next","tagName":"h2"},{"title":"System Performance Monitoring​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#system-performance-monitoring","content":" import psutil import GPUtil from collections import deque import matplotlib.pyplot as plt class VLAPerformanceMonitor: def __init__(self): &quot;&quot;&quot;Initialize performance monitoring for VLA system&quot;&quot;&quot; self.metrics_history = { 'cpu_usage': deque(maxlen=1000), 'memory_usage': deque(maxlen=1000), 'gpu_usage': deque(maxlen=1000), 'gpu_memory': deque(maxlen=1000), 'communication_latency': deque(maxlen=1000), 'processing_time': deque(maxlen=1000), 'throughput': deque(maxlen=1000), 'response_time': deque(maxlen=1000) } self.start_time = time.time() self.command_count = 0 self.error_count = 0 async def start_monitoring(self, coordinator: VLAIntegrationCoordinator): &quot;&quot;&quot;Start performance monitoring&quot;&quot;&quot; self.monitoring_task = asyncio.create_task( self._monitoring_loop(coordinator) ) async def stop_monitoring(self): &quot;&quot;&quot;Stop performance monitoring&quot;&quot;&quot; if hasattr(self, 'monitoring_task'): self.monitoring_task.cancel() async def _monitoring_loop(self, coordinator: VLAIntegrationCoordinator): &quot;&quot;&quot;Main monitoring loop&quot;&quot;&quot; while True: try: # Collect system metrics await self._collect_system_metrics() # Collect VLA-specific metrics await self._collect_vla_metrics(coordinator) # Check for performance issues await self._check_performance_issues() # Brief sleep to prevent busy waiting await asyncio.sleep(0.1) # 10Hz monitoring except asyncio.CancelledError: break except Exception as e: print(f&quot;Error in monitoring loop: {e}&quot;) await asyncio.sleep(1.0) async def _collect_system_metrics(self): &quot;&quot;&quot;Collect system-level performance metrics&quot;&quot;&quot; # CPU usage cpu_percent = psutil.cpu_percent(interval=0.1) self.metrics_history['cpu_usage'].append(cpu_percent) # Memory usage memory = psutil.virtual_memory() self.metrics_history['memory_usage'].append(memory.percent) # GPU usage (if available) try: gpus = GPUtil.getGPUs() if gpus: gpu = gpus[0] # Use first GPU self.metrics_history['gpu_usage'].append(gpu.load * 100) self.metrics_history['gpu_memory'].append(gpu.memoryUtil * 100) else: self.metrics_history['gpu_usage'].append(0) self.metrics_history['gpu_memory'].append(0) except: # GPU monitoring not available self.metrics_history['gpu_usage'].append(0) self.metrics_history['gpu_memory'].append(0) async def _collect_vla_metrics(self, coordinator: VLAIntegrationCoordinator): &quot;&quot;&quot;Collect VLA system-specific metrics&quot;&quot;&quot; # Get system status status = await coordinator.get_system_status() # Calculate processing time based on state changes current_time = time.time() processing_time = 0.01 # Placeholder self.metrics_history['processing_time'].append(processing_time) # Calculate throughput (commands processed per second) self.command_count += 1 uptime = current_time - self.start_time throughput = self.command_count / max(uptime, 1) self.metrics_history['throughput'].append(throughput) # Calculate response time response_time = 0.1 # Placeholder self.metrics_history['response_time'].append(response_time) # Calculate communication latency latency = 0.005 # Placeholder self.metrics_history['communication_latency'].append(latency) async def _check_performance_issues(self): &quot;&quot;&quot;Check for performance issues and alert if needed&quot;&quot;&quot; # Check CPU usage if len(self.metrics_history['cpu_usage']) &gt; 10: recent_cpu = list(self.metrics_history['cpu_usage'])[-10:] avg_cpu = sum(recent_cpu) / len(recent_cpu) if avg_cpu &gt; 90: print(f&quot;HIGH CPU USAGE ALERT: {avg_cpu:.1f}%&quot;) # Check memory usage if len(self.metrics_history['memory_usage']) &gt; 10: recent_mem = list(self.metrics_history['memory_usage'])[-10:] avg_mem = sum(recent_mem) / len(recent_mem) if avg_mem &gt; 90: print(f&quot;HIGH MEMORY USAGE ALERT: {avg_mem:.1f}%&quot;) # Check processing time if len(self.metrics_history['processing_time']) &gt; 10: recent_proc = list(self.metrics_history['processing_time'])[-10:] max_proc = max(recent_proc) if max_proc &gt; 0.1: # More than 100ms processing time print(f&quot;HIGH PROCESSING TIME ALERT: {max_proc:.3f}s&quot;) def get_performance_summary(self) -&gt; Dict: &quot;&quot;&quot;Get summary of performance metrics&quot;&quot;&quot; summary = {} for metric, values in self.metrics_history.items(): if values: summary[metric] = { 'current': values[-1] if values else 0, 'average': sum(values) / len(values), 'min': min(values), 'max': max(values), 'std_dev': self._calculate_std(values) } else: summary[metric] = { 'current': 0, 'average': 0, 'min': 0, 'max': 0, 'std_dev': 0 } summary['total_commands_processed'] = self.command_count summary['total_errors'] = self.error_count summary['uptime_seconds'] = time.time() - self.start_time return summary def _calculate_std(self, values: List[float]) -&gt; float: &quot;&quot;&quot;Calculate standard deviation of values&quot;&quot;&quot; if len(values) &lt; 2: return 0 mean = sum(values) / len(values) variance = sum((x - mean) ** 2 for x in values) / len(values) return variance ** 0.5 def plot_performance_metrics(self): &quot;&quot;&quot;Plot performance metrics&quot;&quot;&quot; fig, axes = plt.subplots(2, 3, figsize=(15, 10)) fig.suptitle('VLA System Performance Metrics') metrics_to_plot = [ ('cpu_usage', 'CPU Usage (%)'), ('memory_usage', 'Memory Usage (%)'), ('gpu_usage', 'GPU Usage (%)'), ('processing_time', 'Processing Time (s)'), ('throughput', 'Throughput (cmds/s)'), ('response_time', 'Response Time (s)') ] for idx, (metric, title) in enumerate(metrics_to_plot): row, col = divmod(idx, 3) ax = axes[row, col] values = list(self.metrics_history[metric]) if values: ax.plot(values) ax.set_title(title) ax.set_xlabel('Sample') ax.set_ylabel(title.split('(')[0].strip()) plt.tight_layout() plt.show() def save_performance_log(self, filename: str): &quot;&quot;&quot;Save performance log to file&quot;&quot;&quot; import json summary = self.get_performance_summary() with open(filename, 'w') as f: json.dump(summary, f, indent=2) print(f&quot;Performance log saved to {filename}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#constitution-alignment","content":" This chapter addresses several constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"VLA Convergence Mandate (Principle I)​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#vla-convergence-mandate-principle-i","content":" Complete integration of Vision-Language-Action systemsReal-time coordination between all three componentsUnified cognitive architecture for humanoid robots  ","version":"Next","tagName":"h3"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#real-time-validation-principle-iv","content":" 100Hz real-time control loops for balance feedbackStrict timing constraints for humanoid stabilityPerformance monitoring for real-time validation  ","version":"Next","tagName":"h3"},{"title":"Anthropomorphic Focus (Principle II)​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#anthropomorphic-focus-principle-ii","content":" Human-like interaction patterns through VLA integrationNatural language as primary control interfaceHuman-centered environment operation  ","version":"Next","tagName":"h3"},{"title":"Sim-to-Real Rigor (Principle III)​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#sim-to-real-rigor-principle-iii","content":" Robust integration that works in both simulation and realitySafety systems for real-world deploymentValidation procedures for sim-to-real transfer  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#target-hardware-optimization","content":" Efficient algorithms suitable for Jetson Orin deploymentReal-time performance on embedded systemsMemory and computation optimization  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Complete VLA Task Execution​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#example-1-complete-vla-task-execution","content":" async def demonstrate_complete_vla_task(): &quot;&quot;&quot;Demonstrate a complete VLA task execution&quot;&quot;&quot; print(&quot;=== Complete VLA Task Execution Demo ===&quot;) # Initialize the complete VLA system coordinator = VLAIntegrationCoordinator() monitor = VLAPerformanceMonitor() # Start the system await coordinator.start_system() await monitor.start_monitoring(coordinator) try: # Submit a complex command print(&quot;\\n1. Submitting complex command...&quot;) command = &quot;Go to the kitchen, find the red cup on the table, pick it up, and bring it to me&quot; correlation_id = coordinator.submit_language_command(command) print(f&quot;Command submitted with ID: {correlation_id}&quot;) # Monitor the execution print(&quot;\\n2. Monitoring execution...&quot;) for i in range(20): # Monitor for 20 iterations status = await coordinator.get_system_status() print(f&quot; Iteration {i+1}: State = {status['state']}&quot;) # Check if task is complete if status['state'] == 'idle': print(&quot; Task completed!&quot;) break await asyncio.sleep(0.5) # Get final status final_status = await coordinator.get_system_status() print(f&quot;\\n3. Final system status: {final_status['state']}&quot;) # Get performance summary perf_summary = monitor.get_performance_summary() print(f&quot;\\n4. Performance summary:&quot;) print(f&quot; - Commands processed: {perf_summary['total_commands_processed']}&quot;) print(f&quot; - Average CPU usage: {perf_summary['cpu_usage']['average']:.1f}%&quot;) print(f&quot; - Average processing time: {perf_summary['processing_time']['average']:.3f}s&quot;) except Exception as e: print(f&quot;Error during VLA task execution: {e}&quot;) finally: # Stop monitoring and system await monitor.stop_monitoring() await coordinator.stop_system() # Run the demonstration if __name__ == &quot;__main__&quot;: asyncio.run(demonstrate_complete_vla_task())   ","version":"Next","tagName":"h3"},{"title":"Example 2: Safety-First VLA Operation​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#example-2-safety-first-vla-operation","content":" class SafeVLAOperation: def __init__(self): &quot;&quot;&quot;Initialize safe VLA operation with comprehensive safety checks&quot;&quot;&quot; self.coordinator = VLAIntegrationCoordinator() self.safety_monitor = SafetySystem() self.operation_history = [] async def execute_safe_command(self, command: str) -&gt; Dict: &quot;&quot;&quot;Execute a command with comprehensive safety checks&quot;&quot;&quot; start_time = time.time() try: # Pre-execution safety checks if not await self._pre_execution_safety_check(): return { 'success': False, 'error': 'Pre-execution safety check failed', 'safety_violations': await self.safety_monitor.get_safety_status() } # Submit command to VLA system correlation_id = self.coordinator.submit_language_command(command) # Monitor execution with safety oversight execution_result = await self._monitor_execution_with_safety(correlation_id) # Post-execution safety checks post_safety_ok = await self._post_execution_safety_check() result = { 'success': execution_result.get('success', True) and post_safety_ok, 'correlation_id': correlation_id, 'execution_time': time.time() - start_time, 'safety_status': await self.safety_monitor.get_safety_status(), 'command': command } # Log operation self.operation_history.append(result) return result except Exception as e: error_result = { 'success': False, 'error': str(e), 'correlation_id': None, 'execution_time': time.time() - start_time, 'safety_status': await self.safety_monitor.get_safety_status() } self.operation_history.append(error_result) return error_result async def _pre_execution_safety_check(self) -&gt; bool: &quot;&quot;&quot;Perform pre-execution safety checks&quot;&quot;&quot; safety_status = await self.safety_monitor.get_safety_status() # Check if emergency stop is active if safety_status['emergency_stop_active']: return False # Check safety limits violations = safety_status.get('recent_violations', []) if violations: print(f&quot;Pre-execution safety violations: {violations}&quot;) return False return True async def _monitor_execution_with_safety(self, correlation_id: str) -&gt; Dict: &quot;&quot;&quot;Monitor execution with continuous safety oversight&quot;&quot;&quot; start_time = time.time() max_execution_time = 30.0 # 30 second timeout while time.time() - start_time &lt; max_execution_time: # Check safety status safety_ok = await self.safety_monitor.is_system_safe() if not safety_ok: return { 'success': False, 'error': 'Safety violation during execution', 'safety_status': await self.safety_monitor.get_safety_status() } # Check if execution is complete # This would involve checking with coordinator status = await self.coordinator.get_system_status() if status['state'] == 'idle': return {'success': True} await asyncio.sleep(0.1) return { 'success': False, 'error': 'Execution timeout', 'execution_time': time.time() - start_time } async def _post_execution_safety_check(self) -&gt; bool: &quot;&quot;&quot;Perform post-execution safety checks&quot;&quot;&quot; # Check for any safety violations during execution safety_status = await self.safety_monitor.get_safety_status() violations = safety_status.get('recent_violations', []) if violations: print(f&quot;Post-execution safety violations: {violations}&quot;) return False # Check that robot is in safe state status = await self.coordinator.get_system_status() if status['state'] == 'safety_stop': return False return True def get_safety_statistics(self) -&gt; Dict: &quot;&quot;&quot;Get safety statistics from operation history&quot;&quot;&quot; total_ops = len(self.operation_history) successful_ops = sum(1 for op in self.operation_history if op['success']) failed_ops = total_ops - successful_ops safety_violations = 0 for op in self.operation_history: if 'safety_status' in op: violations = op['safety_status'].get('recent_violations', []) safety_violations += len(violations) return { 'total_operations': total_ops, 'successful_operations': successful_ops, 'failed_operations': failed_ops, 'success_rate': successful_ops / total_ops if total_ops &gt; 0 else 0, 'safety_violations': safety_violations, 'violation_rate': safety_violations / total_ops if total_ops &gt; 0 else 0 } # Example usage async def demonstrate_safe_operation(): &quot;&quot;&quot;Demonstrate safe VLA operation&quot;&quot;&quot; safe_op = SafeVLAOperation() # Start the coordinator await safe_op.coordinator.start_system() try: # Execute several commands safely commands = [ &quot;Move forward slowly&quot;, &quot;Turn left&quot;, &quot;Stop and check surroundings&quot; ] for command in commands: print(f&quot;\\nExecuting safe command: {command}&quot;) result = await safe_op.execute_safe_command(command) print(f&quot;Result: {result['success']}&quot;) if not result['success']: print(f&quot;Error: {result.get('error', 'Unknown error')}&quot;) # Get safety statistics stats = safe_op.get_safety_statistics() print(f&quot;\\nSafety Statistics:&quot;) print(f&quot; Total operations: {stats['total_operations']}&quot;) print(f&quot; Success rate: {stats['success_rate']:.2%}&quot;) print(f&quot; Safety violations: {stats['safety_violations']}&quot;) print(f&quot; Violation rate: {stats['violation_rate']:.2%}&quot;) finally: await safe_op.coordinator.stop_system()   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: VLA System Integration​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#exercise-1-vla-system-integration","content":" Implement a complete VLA system integration that:  Coordinates vision, language, and action modules in real-timeHandles communication between modules with proper data flowImplements safety systems and error handlingMonitors performance and validates system health  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Real-Time Performance Optimization​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#exercise-2-real-time-performance-optimization","content":" Optimize the VLA system for real-time performance by:  Implementing efficient data structures for module communicationOptimizing processing pipelines for 100Hz control loopsAdding performance monitoring and adaptive controlEnsuring timing constraints are met for humanoid stability  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Safety-First Operation​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#exercise-3-safety-first-operation","content":" Create a safety-first VLA operation system that:  Performs comprehensive safety checks before, during, and after executionImplements emergency stop proceduresMonitors for safety violations continuouslyValidates safe operation in human-centered environments  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#summary","content":" The integration of Vision-Language-Action systems represents the culmination of our humanoid robot architecture, creating a unified cognitive system that enables natural human-robot interaction. The complete VLA pipeline must operate in real-time with strict safety and performance constraints, requiring sophisticated coordination mechanisms between the three core components. Successful integration involves careful attention to timing constraints, data flow, communication protocols, and safety systems. The real-time validation requirements for humanoid balance and the anthropomorphic focus of our design mandate a robust, efficient, and safe integrated system that can operate effectively in human-centered environments while maintaining the high performance standards required for stable humanoid operation.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 17 - Integration: Vision-Language-Action Systems","url":"/docs/chapters/module-4-vla/chapter-17-vla-integration#further-reading","content":" &quot;Humanoid Robotics: A Reference&quot; by Goswami and Vadakkepat (Integration chapter)&quot;Robotics: Control, Sensing, Vision, and Intelligence&quot; by Fu, Gonzalez, and Lee (System Integration)&quot;Handbook of Robotics&quot; edited by Siciliano and Khatib (Integration and Control)&quot;Real-Time Systems and Robotics&quot; - Integration for real-time robotics applications&quot;Safety in Robotics&quot; - Safety systems for humanoid robots ","version":"Next","tagName":"h2"},{"title":"Manage Docs Versions","type":0,"sectionRef":"#","url":"/docs/tutorial-extras/manage-docs-versions","content":"","keywords":"","version":"Next"},{"title":"Create a docs version​","type":1,"pageTitle":"Manage Docs Versions","url":"/docs/tutorial-extras/manage-docs-versions#create-a-docs-version","content":" Release a version 1.0 of your project:  npm run docusaurus docs:version 1.0   The docs folder is copied into versioned_docs/version-1.0 and versions.json is created.  Your docs now have 2 versions:  1.0 at http://localhost:3000/docs/ for the version 1.0 docscurrent at http://localhost:3000/docs/next/ for the upcoming, unreleased docs  ","version":"Next","tagName":"h2"},{"title":"Add a Version Dropdown​","type":1,"pageTitle":"Manage Docs Versions","url":"/docs/tutorial-extras/manage-docs-versions#add-a-version-dropdown","content":" To navigate seamlessly across versions, add a version dropdown.  Modify the docusaurus.config.js file:  docusaurus.config.js export default { themeConfig: { navbar: { items: [ { type: 'docsVersionDropdown', }, ], }, }, };   The docs version dropdown appears in your navbar:    ","version":"Next","tagName":"h2"},{"title":"Update an existing version​","type":1,"pageTitle":"Manage Docs Versions","url":"/docs/tutorial-extras/manage-docs-versions#update-an-existing-version","content":" It is possible to edit versioned docs in their respective folder:  versioned_docs/version-1.0/hello.md updates http://localhost:3000/docs/hellodocs/hello.md updates http://localhost:3000/docs/next/hello ","version":"Next","tagName":"h2"},{"title":"Translate your site","type":0,"sectionRef":"#","url":"/docs/tutorial-extras/translate-your-site","content":"","keywords":"","version":"Next"},{"title":"Configure i18n​","type":1,"pageTitle":"Translate your site","url":"/docs/tutorial-extras/translate-your-site#configure-i18n","content":" Modify docusaurus.config.js to add support for the fr locale:  docusaurus.config.js export default { i18n: { defaultLocale: 'en', locales: ['en', 'fr'], }, };   ","version":"Next","tagName":"h2"},{"title":"Translate a doc​","type":1,"pageTitle":"Translate your site","url":"/docs/tutorial-extras/translate-your-site#translate-a-doc","content":" Copy the docs/intro.md file to the i18n/fr folder:  mkdir -p i18n/fr/docusaurus-plugin-content-docs/current/ cp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md   Translate i18n/fr/docusaurus-plugin-content-docs/current/intro.md in French.  ","version":"Next","tagName":"h2"},{"title":"Start your localized site​","type":1,"pageTitle":"Translate your site","url":"/docs/tutorial-extras/translate-your-site#start-your-localized-site","content":" Start your site on the French locale:  npm run start -- --locale fr   Your localized site is accessible at http://localhost:3000/fr/ and the Getting Started page is translated.  caution In development, you can only use one locale at a time.  ","version":"Next","tagName":"h2"},{"title":"Add a Locale Dropdown​","type":1,"pageTitle":"Translate your site","url":"/docs/tutorial-extras/translate-your-site#add-a-locale-dropdown","content":" To navigate seamlessly across languages, add a locale dropdown.  Modify the docusaurus.config.js file:  docusaurus.config.js export default { themeConfig: { navbar: { items: [ { type: 'localeDropdown', }, ], }, }, };   The locale dropdown now appears in your navbar:    ","version":"Next","tagName":"h2"},{"title":"Build your localized site​","type":1,"pageTitle":"Translate your site","url":"/docs/tutorial-extras/translate-your-site#build-your-localized-site","content":" Build your site for a specific locale:  npm run build -- --locale fr   Or build your site to include all the locales at once:  npm run build  ","version":"Next","tagName":"h2"},{"title":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","type":0,"sectionRef":"#","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project","content":"","keywords":"","version":"Next"},{"title":"Learning Objectives​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#learning-objectives","content":" Implement complete autonomous humanoid robot systemIntegrate all previous modules into cohesive systemValidate system performance in end-to-end scenarios  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#introduction","content":" The capstone project represents the ultimate demonstration of our Physical AI &amp; Humanoid Robotics curriculum, bringing together all the knowledge and skills developed across the four modules to create an autonomous humanoid robot system capable of executing the complete Vision-Language-Action (VLA) cycle in human-centered environments. This project embodies our VLA Convergence Mandate principle, demonstrating how language serves as the primary control interface for humanoid robots, with vision and action systems working in harmony to enable natural human-robot interaction. The capstone project validates the Sim-to-Real Rigor principle through comprehensive testing in both simulated and real environments, while adhering to the Real-Time Validation requirements for safe humanoid operation. This chapter guides you through the implementation of the complete autonomous humanoid system that demonstrates the end-to-end VLA cycle.  ","version":"Next","tagName":"h2"},{"title":"Capstone Project Architecture​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#capstone-project-architecture","content":" ","version":"Next","tagName":"h2"},{"title":"Complete System Architecture​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#complete-system-architecture","content":" graph TB subgraph &quot;User Interface Layer&quot; A[Natural Language Command] --&gt; B[Speech Recognition] C[Gesture Input] --&gt; D[Gesture Recognition] E[Visual Input] --&gt; F[Face Recognition] end subgraph &quot;VLA Processing Layer&quot; G[Language Understanding] --&gt; H[Vision System] H --&gt; I[Action Planning] I --&gt; J[Control System] J --&gt; K[Physical Execution] end subgraph &quot;Perception &amp; Control Layer&quot; L[Camera System] --&gt; H M[Lidar System] --&gt; H N[IMU Sensors] --&gt; J O[Force/Torque Sensors] --&gt; J P[Joint Encoders] --&gt; J end subgraph &quot;Execution Layer&quot; Q[Locomotion Control] --&gt; R[Bipedal Walking] S[Manipulation Control] --&gt; T[Grasping &amp; Placing] U[Balance Control] --&gt; V[Stability Maintenance] end subgraph &quot;Feedback Loop&quot; R --&gt; G T --&gt; H V --&gt; I end A --&gt; G B --&gt; G L --&gt; H M --&gt; H N --&gt; J O --&gt; J P --&gt; J I --&gt; J J --&gt; Q J --&gt; S J --&gt; U Q --&gt; R S --&gt; T U --&gt; V   ","version":"Next","tagName":"h3"},{"title":"System Integration Overview​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#system-integration-overview","content":" The capstone project integrates all previous modules into a unified system:  Module 1 Foundation: Physical AI concepts and humanoid robotics principlesModule 2 ROS 2: Robot Operating System for communication and controlModule 3 Simulation: Digital twin for development and testingModule 4 VLA: Vision-Language-Action integration for autonomy  ","version":"Next","tagName":"h3"},{"title":"Implementation Strategy​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#implementation-strategy","content":" ","version":"Next","tagName":"h2"},{"title":"Phase 1: System Integration​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#phase-1-system-integration","content":" import asyncio import threading import time from typing import Dict, List, Optional, Any from dataclasses import dataclass import json import logging # Configure logging for the capstone project logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) @dataclass class CapstoneConfiguration: &quot;&quot;&quot;Configuration for the capstone project&quot;&quot;&quot; # Hardware specifications target_hardware: str = &quot;NVIDIA Jetson Orin Nano (8GB)&quot; sensors: List[str] = None actuators: List[str] = None # Performance requirements control_frequency: int = 100 # Hz for balance control vision_frequency: int = 30 # Hz for vision processing language_frequency: int = 10 # Hz for language processing # Safety constraints max_torque: float = 100.0 # Maximum joint torque (Nm) max_velocity: float = 2.0 # Maximum joint velocity (rad/s) safety_margin: float = 0.1 # Safety margin for operations # Communication settings ros_namespace: str = &quot;/humanoid_robot&quot; simulation_mode: bool = True def __post_init__(self): if self.sensors is None: self.sensors = [ &quot;RGB_Camera&quot;, &quot;Depth_Camera&quot;, &quot;IMU&quot;, &quot;Force_Torque_Sensors&quot;, &quot;Joint_Encoders&quot;, &quot;Microphone_Array&quot;, &quot;LIDAR&quot; ] if self.actuators is None: self.actuators = [ &quot;Hip_Joints&quot;, &quot;Knee_Joints&quot;, &quot;Ankle_Joints&quot;, &quot;Shoulder_Joints&quot;, &quot;Elbow_Joints&quot;, &quot;Wrist_Joints&quot;, &quot;Gripper_Joints&quot; ] class CapstoneSystem: def __init__(self, config: CapstoneConfiguration): &quot;&quot;&quot; Initialize the complete capstone system Args: config: Configuration parameters for the system &quot;&quot;&quot; self.config = config self.system_state = { 'initialized': False, 'running': False, 'safety_engaged': False, 'active_modules': [], 'performance_metrics': {} } # Initialize all subsystems self.language_system = None self.vision_system = None self.action_system = None self.control_system = None self.communication_bus = None # Initialize safety systems self.safety_manager = SafetyManager(config) # Initialize performance monitors self.performance_monitor = PerformanceMonitor() # Initialize simulation interface self.simulation_interface = None logger.info(&quot;Capstone system initialized with configuration:&quot;) logger.info(f&quot; Target Hardware: {config.target_hardware}&quot;) logger.info(f&quot; Control Frequency: {config.control_frequency}Hz&quot;) logger.info(f&quot; Simulation Mode: {config.simulation_mode}&quot;) async def initialize_system(self): &quot;&quot;&quot;Initialize all subsystems for the capstone project&quot;&quot;&quot; logger.info(&quot;Starting capstone system initialization...&quot;) # Initialize communication bus self.communication_bus = await self._initialize_communication_bus() # Initialize language understanding system self.language_system = await self._initialize_language_system() # Initialize vision system self.vision_system = await self._initialize_vision_system() # Initialize action planning system self.action_system = await self._initialize_action_system() # Initialize control system self.control_system = await self._initialize_control_system() # Initialize simulation interface if in simulation mode if self.config.simulation_mode: self.simulation_interface = await self._initialize_simulation_interface() # Verify all systems are healthy if await self._verify_system_health(): self.system_state['initialized'] = True logger.info(&quot;Capstone system initialization completed successfully&quot;) else: logger.error(&quot;Capstone system initialization failed - some systems unhealthy&quot;) self.system_state['initialized'] = False async def _initialize_communication_bus(self): &quot;&quot;&quot;Initialize the communication bus for inter-module communication&quot;&quot;&quot; logger.info(&quot;Initializing communication bus...&quot;) # This would initialize ROS 2 communication interfaces # For simulation, we'll use a mock implementation return MockCommunicationBus() async def _initialize_language_system(self): &quot;&quot;&quot;Initialize the language understanding system&quot;&quot;&quot; logger.info(&quot;Initializing language system...&quot;) # This would initialize NLP models and language processing # For the capstone, we'll use a sophisticated language system return LanguageSystem(self.communication_bus, self.config) async def _initialize_vision_system(self): &quot;&quot;&quot;Initialize the vision processing system&quot;&quot;&quot; logger.info(&quot;Initializing vision system...&quot;) # This would initialize computer vision models and processing return VisionSystem(self.communication_bus, self.config) async def _initialize_action_system(self): &quot;&quot;&quot;Initialize the action planning system&quot;&quot;&quot; logger.info(&quot;Initializing action planning system...&quot;) # This would initialize action planning and decision making return ActionSystem(self.communication_bus, self.config) async def _initialize_control_system(self): &quot;&quot;&quot;Initialize the control system&quot;&quot;&quot; logger.info(&quot;Initializing control system...&quot;) # This would initialize real-time control systems return ControlSystem(self.communication_bus, self.config) async def _initialize_simulation_interface(self): &quot;&quot;&quot;Initialize simulation interface for development and testing&quot;&quot;&quot; logger.info(&quot;Initializing simulation interface...&quot;) # This would connect to Gazebo, Isaac Sim, or Unity return SimulationInterface(self.config) async def _verify_system_health(self) -&gt; bool: &quot;&quot;&quot;Verify that all subsystems are healthy and communicating properly&quot;&quot;&quot; logger.info(&quot;Verifying system health...&quot;) health_checks = [ await self.language_system.is_healthy(), await self.vision_system.is_healthy(), await self.action_system.is_healthy(), await self.control_system.is_healthy(), await self.safety_manager.is_system_safe() ] all_healthy = all(health_checks) logger.info(f&quot;System health check: {'PASS' if all_healthy else 'FAIL'}&quot;) if not all_healthy: logger.error(&quot;Health check failures:&quot;) if not health_checks[0]: logger.error(&quot; Language system unhealthy&quot;) if not health_checks[1]: logger.error(&quot; Vision system unhealthy&quot;) if not health_checks[2]: logger.error(&quot; Action system unhealthy&quot;) if not health_checks[3]: logger.error(&quot; Control system unhealthy&quot;) if not health_checks[4]: logger.error(&quot; Safety system compromised&quot;) return all_healthy async def start_autonomous_operation(self): &quot;&quot;&quot;Start the complete autonomous operation of the humanoid robot&quot;&quot;&quot; if not self.system_state['initialized']: logger.error(&quot;Cannot start autonomous operation - system not initialized&quot;) return False logger.info(&quot;Starting autonomous humanoid robot operation...&quot;) # Start all subsystems await self.language_system.start() await self.vision_system.start() await self.action_system.start() await self.control_system.start() # Start performance monitoring await self.performance_monitor.start_monitoring() # Engage safety systems await self.safety_manager.enable_monitoring() self.system_state['running'] = True logger.info(&quot;Autonomous operation started successfully&quot;) # Start the main control loop self.main_control_task = asyncio.create_task(self._main_control_loop()) return True async def _main_control_loop(self): &quot;&quot;&quot;Main control loop for the capstone system&quot;&quot;&quot; control_period = 1.0 / self.config.control_frequency vision_period = 1.0 / self.config.vision_frequency language_period = 1.0 / self.config.language_frequency # Track timing for each subsystem last_control_time = time.time() last_vision_time = time.time() last_language_time = time.time() while self.system_state['running']: loop_start = time.time() try: # Perform real-time control tasks (highest priority) current_time = time.time() # Control system updates (100Hz for balance) if current_time - last_control_time &gt;= control_period: await self._update_control_system() last_control_time = current_time # Vision system updates (30Hz for perception) if current_time - last_vision_time &gt;= vision_period: await self._update_vision_system() last_vision_time = current_time # Language system updates (10Hz for command processing) if current_time - last_language_time &gt;= language_period: await self._update_language_system() last_language_time = current_time # Update safety systems continuously await self.safety_manager.monitor_safety() # Update performance metrics await self.performance_monitor.update_metrics() # Calculate loop time loop_time = time.time() - loop_start # Sleep to maintain timing sleep_time = control_period - loop_time if sleep_time &gt; 0: await asyncio.sleep(sleep_time) else: logger.warning(f&quot;Control loop overrun by {abs(sleep_time):.4f}s&quot;) except asyncio.CancelledError: logger.info(&quot;Main control loop cancelled&quot;) break except Exception as e: logger.error(f&quot;Error in main control loop: {e}&quot;) await asyncio.sleep(0.1) # Brief pause to avoid spinning on error async def _update_control_system(self): &quot;&quot;&quot;Update control system for real-time operation&quot;&quot;&quot; await self.control_system.update_real_time_control() async def _update_vision_system(self): &quot;&quot;&quot;Update vision system for perception&quot;&quot;&quot; await self.vision_system.update_perception() async def _update_language_system(self): &quot;&quot;&quot;Update language system for command processing&quot;&quot;&quot; await self.language_system.update_language_processing() async def submit_command(self, command: str, context: Dict = None) -&gt; str: &quot;&quot;&quot;Submit a command to the autonomous system&quot;&quot;&quot; if not self.system_state['running']: logger.error(&quot;Cannot submit command - system not running&quot;) return None correlation_id = f&quot;cmd_{int(time.time() * 1000)}&quot; logger.info(f&quot;Submitting command: '{command}' with ID: {correlation_id}&quot;) # Submit command to language system await self.language_system.submit_command(command, context, correlation_id) return correlation_id async def stop_autonomous_operation(self): &quot;&quot;&quot;Stop the autonomous operation safely&quot;&quot;&quot; logger.info(&quot;Stopping autonomous operation...&quot;) self.system_state['running'] = False # Cancel main control loop if hasattr(self, 'main_control_task'): self.main_control_task.cancel() try: await self.main_control_task except asyncio.CancelledError: pass # Stop all subsystems await self.language_system.stop() await self.vision_system.stop() await self.action_system.stop() await self.control_system.stop() await self.performance_monitor.stop_monitoring() await self.safety_manager.disable_monitoring() logger.info(&quot;Autonomous operation stopped&quot;) async def get_system_status(self) -&gt; Dict: &quot;&quot;&quot;Get comprehensive system status&quot;&quot;&quot; return { 'system_initialized': self.system_state['initialized'], 'system_running': self.system_state['running'], 'safety_engaged': self.system_state['safety_engaged'], 'active_modules': self.system_state['active_modules'], 'performance_metrics': await self.performance_monitor.get_metrics(), 'safety_status': await self.safety_manager.get_status(), 'language_status': await self.language_system.get_status(), 'vision_status': await self.vision_system.get_status(), 'action_status': await self.action_system.get_status(), 'control_status': await self.control_system.get_status(), 'timestamp': time.time() } def get_configuration(self) -&gt; CapstoneConfiguration: &quot;&quot;&quot;Get the system configuration&quot;&quot;&quot; return self.config   ","version":"Next","tagName":"h3"},{"title":"Language System Implementation​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#language-system-implementation","content":" class LanguageSystem: def __init__(self, communication_bus, config: CapstoneConfiguration): &quot;&quot;&quot;Initialize the language system for the capstone project&quot;&quot;&quot; self.communication_bus = communication_bus self.config = config self.is_running = False self.command_queue = asyncio.Queue() self.active_processes = {} self.language_processor = self._initialize_language_processor() self.context_manager = ContextManager() def _initialize_language_processor(self): &quot;&quot;&quot;Initialize sophisticated language processing system&quot;&quot;&quot; # This would load advanced NLP models for humanoid interaction return { 'parser': AdvancedCommandParser(), 'intent_recognizer': IntentRecognizer(), 'context_resolver': ContextResolver(), 'response_generator': ResponseGenerator(), 'dialogue_manager': DialogueManager() } async def start(self): &quot;&quot;&quot;Start the language system&quot;&quot;&quot; self.is_running = True self.processing_task = asyncio.create_task(self._processing_loop()) logger.info(&quot;Language system started&quot;) async def stop(self): &quot;&quot;&quot;Stop the language system&quot;&quot;&quot; self.is_running = False if hasattr(self, 'processing_task'): self.processing_task.cancel() logger.info(&quot;Language system stopped&quot;) async def _processing_loop(self): &quot;&quot;&quot;Main processing loop for language system&quot;&quot;&quot; while self.is_running: try: # Process incoming commands try: command_data = await asyncio.wait_for( self.command_queue.get(), timeout=0.1 ) await self._process_command(command_data) except asyncio.TimeoutError: continue # No command to process, continue loop # Process any queued tasks await self._process_queued_tasks() except asyncio.CancelledError: break except Exception as e: logger.error(f&quot;Error in language processing loop: {e}&quot;) await asyncio.sleep(0.1) async def _process_command(self, command_data: Dict): &quot;&quot;&quot;Process a single command through the VLA pipeline&quot;&quot;&quot; command = command_data['command'] context = command_data.get('context', {}) correlation_id = command_data['correlation_id'] try: logger.info(f&quot;Processing command: '{command}' (ID: {correlation_id})&quot;) # Parse the command parsed_result = await self.language_processor['parser'].parse_command(command) # Recognize intent intent = await self.language_processor['intent_recognizer'].recognize_intent( command, parsed_result ) # Resolve context resolved_context = await self.language_processor['context_resolver'].resolve_context( parsed_result, context, intent ) # Generate initial response initial_response = await self.language_processor['response_generator'].generate_response( intent, resolved_context ) # Update dialogue state await self.language_processor['dialogue_manager'].update_dialogue_state( correlation_id, intent, resolved_context ) # Based on intent, route to appropriate VLA components if intent['action'] in ['navigate', 'move', 'go']: # Send to vision system for environment analysis vision_request = { 'task': 'analyze_navigation_environment', 'target_location': intent['target'], 'context': resolved_context, 'correlation_id': correlation_id } await self.communication_bus.send_message( 'language', 'vision', vision_request ) elif intent['action'] in ['grasp', 'pick_up', 'take']: # Send to vision system for object localization vision_request = { 'task': 'localize_target_object', 'target_object': intent['target'], 'context': resolved_context, 'correlation_id': correlation_id } await self.communication_bus.send_message( 'language', 'vision', vision_request ) elif intent['action'] in ['place', 'put_down']: # Send to vision system for placement location vision_request = { 'task': 'analyze_placement_location', 'target_object': intent['target_object'], 'target_location': intent['target_location'], 'context': resolved_context, 'correlation_id': correlation_id } await self.communication_bus.send_message( 'language', 'vision', vision_request ) else: # Send to action planning for direct execution action_request = { 'task': 'execute_direct_action', 'intent': intent, 'context': resolved_context, 'correlation_id': correlation_id } await self.communication_bus.send_message( 'language', 'action', action_request ) # Send initial response back to user response = { 'type': 'initial_acknowledgment', 'message': initial_response, 'correlation_id': correlation_id } await self.communication_bus.send_message( 'language', 'output', response ) except Exception as e: logger.error(f&quot;Error processing command '{command}': {e}&quot;) # Send error response error_response = { 'type': 'error', 'message': f&quot;Sorry, I encountered an error processing your command: {str(e)}&quot;, 'correlation_id': correlation_id } await self.communication_bus.send_message( 'language', 'output', error_response ) async def submit_command(self, command: str, context: Dict, correlation_id: str): &quot;&quot;&quot;Submit a command for processing&quot;&quot;&quot; command_data = { 'command': command, 'context': context, 'correlation_id': correlation_id, 'timestamp': time.time() } await self.command_queue.put(command_data) async def _process_queued_tasks(self): &quot;&quot;&quot;Process any queued language tasks&quot;&quot;&quot; pass async def is_healthy(self) -&gt; bool: &quot;&quot;&quot;Check if language system is healthy&quot;&quot;&quot; return self.is_running def is_processing(self) -&gt; bool: &quot;&quot;&quot;Check if language system is actively processing&quot;&quot;&quot; return not self.command_queue.empty() async def get_status(self) -&gt; Dict: &quot;&quot;&quot;Get language system status&quot;&quot;&quot; return { 'running': self.is_running, 'queue_size': self.command_queue.qsize(), 'active_processes': len(self.active_processes), 'processor_loaded': self.language_processor is not None } class AdvancedCommandParser: &quot;&quot;&quot;Advanced command parser for humanoid robot commands&quot;&quot;&quot; def __init__(self): self.patterns = { 'navigation': [ r'go to (.+)', r'move to (.+)', r'walk to (.+)', r'navigate to (.+)', r'approach (.+)', r'go to the (.+)' ], 'manipulation': [ r'pick up (.+)', r'grasp (.+)', r'take (.+)', r'get (.+)', r'pick (.+) up', r'lift (.+)', r'hold (.+)' ], 'placement': [ r'put (.+) (?:on|in|at) (.+)', r'place (.+) (?:on|in|at) (.+)', r'drop (.+) (?:on|in|at) (.+)', r'place (.+) down (?:on|in|at) (.+)' ], 'greeting': [ r'hello', r'hi', r'greet (.+)', r'say hello to (.+)', r'wave to (.+)' ], 'following': [ r'follow (.+)', r'come after (.+)', r'accompany (.+)' ] } async def parse_command(self, command: str) -&gt; Dict: &quot;&quot;&quot;Parse natural language command into structured format&quot;&quot;&quot; command_lower = command.lower().strip() result = { 'original_command': command, 'action': 'unknown', 'target': None, 'target_location': None, 'target_object': None, 'confidence': 0.0, 'parsed_elements': [] } # Check each pattern category for action_type, patterns in self.patterns.items(): for pattern in patterns: import re match = re.search(pattern, command_lower) if match: result['action'] = action_type groups = match.groups() if action_type == 'navigation': result['target'] = groups[0] result['target_location'] = groups[0] elif action_type == 'manipulation': result['target'] = groups[0] result['target_object'] = groups[0] elif action_type == 'placement': result['target_object'] = groups[0] result['target_location'] = groups[1] result['target'] = f&quot;{groups[0]} at {groups[1]}&quot; elif action_type in ['greeting', 'following']: result['target'] = groups[0] if groups else None result['confidence'] = 0.9 result['parsed_elements'] = list(groups) break if result['action'] != 'unknown': break # If no pattern matched, try more general parsing if result['action'] == 'unknown': # Simple keyword-based parsing if any(keyword in command_lower for keyword in ['go', 'move', 'walk', 'navigate']): result['action'] = 'navigation' elif any(keyword in command_lower for keyword in ['pick', 'grasp', 'take', 'get']): result['action'] = 'manipulation' elif any(keyword in command_lower for keyword in ['put', 'place', 'drop']): result['action'] = 'placement' result['confidence'] = 0.6 # Lower confidence for keyword-based parsing return result   ","version":"Next","tagName":"h3"},{"title":"Vision System Implementation​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#vision-system-implementation","content":" class VisionSystem: def __init__(self, communication_bus, config: CapstoneConfiguration): &quot;&quot;&quot;Initialize the vision system for the capstone project&quot;&quot;&quot; self.communication_bus = communication_bus self.config = config self.is_running = False self.processing_queue = asyncio.Queue() self.active_processes = {} self.vision_processor = self._initialize_vision_processor() self.scene_cache = {} self.object_database = ObjectDatabase() def _initialize_vision_processor(self): &quot;&quot;&quot;Initialize sophisticated vision processing system&quot;&quot;&quot; # This would load advanced computer vision models return { 'object_detector': AdvancedObjectDetector(), 'pose_estimator': PoseEstimator(), 'scene_analyzer': SceneAnalyzer(), 'tracking_system': MultiObjectTracker(), 'depth_processor': DepthProcessor(), 'semantic_segmenter': SemanticSegmenter() } async def start(self): &quot;&quot;&quot;Start the vision system&quot;&quot;&quot; self.is_running = True self.processing_task = asyncio.create_task(self._processing_loop()) self.sensing_task = asyncio.create_task(self._sensing_loop()) logger.info(&quot;Vision system started&quot;) async def stop(self): &quot;&quot;&quot;Stop the vision system&quot;&quot;&quot; self.is_running = False if hasattr(self, 'processing_task'): self.processing_task.cancel() if hasattr(self, 'sensing_task'): self.sensing_task.cancel() logger.info(&quot;Vision system stopped&quot;) async def _processing_loop(self): &quot;&quot;&quot;Main processing loop for vision system&quot;&quot;&quot; while self.is_running: try: # Process incoming vision requests try: request_data = await asyncio.wait_for( self.processing_queue.get(), timeout=0.1 ) await self._process_vision_request(request_data) except asyncio.TimeoutError: continue # No request to process, continue loop # Process any queued tasks await self._process_queued_tasks() except asyncio.CancelledError: break except Exception as e: logger.error(f&quot;Error in vision processing loop: {e}&quot;) await asyncio.sleep(0.1) async def _sensing_loop(self): &quot;&quot;&quot;Continuous sensing loop for environment monitoring&quot;&quot;&quot; sensing_period = 1.0 / self.config.vision_frequency # e.g., 30Hz while self.is_running: try: # Perform continuous environment sensing current_scene = await self._sense_current_environment() # Cache the scene for quick access timestamp = time.time() self.scene_cache[timestamp] = current_scene # Check if significant changes occurred if await self._has_significant_changes(current_scene): # Notify other modules of environment change change_notification = { 'type': 'environment_change', 'scene_data': current_scene, 'timestamp': timestamp } await self.communication_bus.send_message( 'vision', 'action', change_notification ) # Brief sleep to maintain sensing frequency await asyncio.sleep(sensing_period) except asyncio.CancelledError: break except Exception as e: logger.error(f&quot;Error in vision sensing loop: {e}&quot;) await asyncio.sleep(0.5) async def _sense_current_environment(self) -&gt; Dict: &quot;&quot;&quot;Sense current environment and return scene description&quot;&quot;&quot; # This would interface with actual cameras and sensors # For demonstration, return realistic mock data import random # Simulate getting data from various sensors objects = [ {'name': 'red_cup', 'position': [1.5, 0.5, 0.8], 'confidence': 0.95, 'class': 'cup'}, {'name': 'book', 'position': [0.8, 0.2, 0.85], 'confidence': 0.92, 'class': 'book'}, {'name': 'chair', 'position': [2.0, 1.0, 0.0], 'confidence': 0.88, 'class': 'chair'}, {'name': 'table', 'position': [1.0, 0.0, 0.0], 'confidence': 0.98, 'class': 'table'}, {'name': 'person', 'position': [0.0, 0.0, 0.0], 'confidence': 0.90, 'class': 'person'} ] # Add some random objects to simulate changing environment if random.random() &gt; 0.7: # 30% chance of additional objects objects.append({ 'name': 'bottle', 'position': [random.uniform(0.5, 2.5), random.uniform(0.0, 1.5), 0.85], 'confidence': random.uniform(0.7, 0.95), 'class': 'bottle' }) scene = { 'timestamp': time.time(), 'objects': objects, 'locations': [ {'name': 'kitchen', 'position': [3.0, 0.0, 0.0], 'confidence': 0.95}, {'name': 'living_room', 'position': [0.0, 2.0, 0.0], 'confidence': 0.90}, {'name': 'bedroom', 'position': [-1.0, -1.0, 0.0], 'confidence': 0.85} ], 'robot_position': [0.0, 0.0, 0.0], 'traversable_map': [[1]*20 for _ in range(20)], # 20x20 grid 'lighting_conditions': 'normal', 'camera_status': 'operational', 'depth_map_available': True } return scene async def _process_vision_request(self, request_data: Dict): &quot;&quot;&quot;Process a vision request from another module&quot;&quot;&quot; request = request_data['task'] correlation_id = request_data.get('correlation_id') try: if request == 'analyze_navigation_environment': target_location = request_data.get('target_location') result = await self._analyze_navigation_environment(target_location) elif request == 'localize_target_object': target_object = request_data.get('target_object') result = await self._localize_target_object(target_object) elif request == 'analyze_placement_location': target_object = request_data.get('target_object') target_location = request_data.get('target_location') result = await self._analyze_placement_location(target_object, target_location) elif request == 'detect_people': result = await self._detect_people() elif request == 'analyze_scene': result = await self._analyze_scene() else: raise ValueError(f&quot;Unknown vision request: {request}&quot;) # Send result back to requesting module result_message = { 'type': 'vision_result', 'request': request, 'result': result, 'correlation_id': correlation_id } await self.communication_bus.send_message('vision', 'action', result_message) except Exception as e: logger.error(f&quot;Error processing vision request '{request}': {e}&quot;) # Send error notification error_message = { 'type': 'vision_error', 'error': str(e), 'correlation_id': correlation_id } await self.communication_bus.send_message('vision', 'language', error_message) async def _analyze_navigation_environment(self, target_location: str) -&gt; Dict: &quot;&quot;&quot;Analyze environment for navigation to target location&quot;&quot;&quot; current_scene = await self._sense_current_environment() # Find the target location in the scene target_pos = None for loc in current_scene['locations']: if target_location.lower() in loc['name'].lower(): target_pos = loc['position'] break if not target_pos: # Try to infer location from scene target_pos = self._infer_location_position(target_location, current_scene) # Analyze path to target path_analysis = await self._analyze_navigation_path(current_scene, target_pos) return { 'target_location': target_location, 'target_position': target_pos, 'path_analysis': path_analysis, 'obstacles': path_analysis['obstacles'], 'traversable_areas': path_analysis['traversable_areas'], 'safety_assessment': path_analysis['safety_assessment'] } def _infer_location_position(self, location_name: str, scene: Dict) -&gt; List[float]: &quot;&quot;&quot;Infer position of location based on scene context&quot;&quot;&quot; # This would use semantic understanding and spatial reasoning # For demo, return a reasonable default position location_mapping = { 'kitchen': [3.0, 0.0, 0.0], 'living room': [0.0, 2.0, 0.0], 'bedroom': [-1.0, -1.0, 0.0], 'office': [2.0, -1.0, 0.0], 'dining room': [1.0, 1.0, 0.0] } for key, pos in location_mapping.items(): if key in location_name.lower(): return pos # Default fallback return [random.uniform(-2, 3), random.uniform(-2, 3), 0.0] async def _analyze_navigation_path(self, scene: Dict, target_pos: List[float]) -&gt; Dict: &quot;&quot;&quot;Analyze navigation path to target position&quot;&quot;&quot; # This would implement path planning algorithms # For demo, return mock analysis obstacles = [] for obj in scene['objects']: if obj['confidence'] &gt; 0.7: # High confidence detections dist_to_target = ((obj['position'][0] - target_pos[0])**2 + (obj['position'][1] - target_pos[1])**2)**0.5 if dist_to_target &lt; 1.0: # Within 1 meter of target obstacles.append({ 'name': obj['name'], 'position': obj['position'], 'size_estimate': self._estimate_object_size(obj), 'impact': 'high' if dist_to_target &lt; 0.5 else 'medium' }) traversable_areas = [ {'center': [1.0, 1.0], 'radius': 0.5, 'traversable': True}, {'center': [2.0, 2.0], 'radius': 0.3, 'traversable': True} ] safety_assessment = { 'clear_path_probability': 0.8 if not obstacles else 0.3, 'risk_level': 'low' if len(obstacles) == 0 else 'medium', 'alternative_routes': 2 if obstacles else 0 } return { 'obstacles': obstacles, 'traversable_areas': traversable_areas, 'safety_assessment': safety_assessment, 'recommended_path': self._generate_recommended_path(scene, target_pos) } def _estimate_object_size(self, obj: Dict) -&gt; Dict: &quot;&quot;&quot;Estimate object size from detection&quot;&quot;&quot; # Simplified size estimation based on class and confidence class_sizes = { 'cup': {'width': 0.08, 'height': 0.1, 'depth': 0.08}, 'book': {'width': 0.2, 'height': 0.03, 'depth': 0.15}, 'chair': {'width': 0.5, 'height': 0.8, 'depth': 0.5}, 'table': {'width': 1.0, 'height': 0.75, 'depth': 0.6}, 'person': {'width': 0.6, 'height': 1.7, 'depth': 0.3} } obj_class = obj.get('class', 'unknown') if obj_class in class_sizes: return class_sizes[obj_class] else: # Default size for unknown objects return {'width': 0.1, 'height': 0.1, 'depth': 0.1} def _generate_recommended_path(self, scene: Dict, target_pos: List[float]) -&gt; List[List[float]]: &quot;&quot;&quot;Generate recommended navigation path&quot;&quot;&quot; # Simplified path generation # In practice, would use A*, RRT, or other path planning algorithms current_pos = scene['robot_position'] # Generate a simple path (in practice, would be more sophisticated) path = [current_pos.copy()] # Add intermediate waypoints for i in range(5): # 5 intermediate waypoints t = (i + 1) / 6 # Parameter from 0 to 1 waypoint = [ current_pos[0] + t * (target_pos[0] - current_pos[0]), current_pos[1] + t * (target_pos[1] - current_pos[1]), current_pos[2] + t * (target_pos[2] - current_pos[2]) ] path.append(waypoint) path.append(target_pos.copy()) return path async def _localize_target_object(self, target_object: str) -&gt; Dict: &quot;&quot;&quot;Locate a specific target object in the environment&quot;&quot;&quot; current_scene = await self._sense_current_environment() # Search for target object in detected objects for obj in current_scene['objects']: if target_object.lower() in obj['name'].lower() or \\ target_object.lower() in obj['class'].lower(): return { 'target_object': target_object, 'position': obj['position'], 'confidence': obj['confidence'], 'object_details': obj, 'grasp_feasibility': self._assess_grasp_feasibility(obj) } # If not found, return negative result return { 'target_object': target_object, 'position': None, 'confidence': 0.0, 'found': False, 'suggestions': self._suggest_similar_objects(target_object, current_scene) } def _assess_grasp_feasibility(self, obj: Dict) -&gt; Dict: &quot;&quot;&quot;Assess whether an object can be grasped by the humanoid&quot;&quot;&quot; size = self._estimate_object_size(obj) # Check if object is graspable based on size graspable = (0.02 &lt; size['width'] &lt; 0.2 and # Between 2cm and 20cm 0.02 &lt; size['height'] &lt; 0.3 and # Between 2cm and 30cm obj['confidence'] &gt; 0.8) # High confidence detection return { 'graspable': graspable, 'estimated_weight': self._estimate_weight(obj, size), 'recommended_grasp_type': self._recommend_grasp_type(size), 'approach_vector': [0, 0, -1] # Approach from above } def _estimate_weight(self, obj: Dict, size: Dict) -&gt; float: &quot;&quot;&quot;Estimate object weight based on size and material&quot;&quot;&quot; # Simplified weight estimation volume = size['width'] * size['height'] * size['depth'] # Assume average density of 500 kg/m³ (light objects) density = 500 estimated_weight = volume * density return min(estimated_weight, 2.0) # Cap at 2kg for safety def _recommend_grasp_type(self, size: Dict) -&gt; str: &quot;&quot;&quot;Recommend appropriate grasp type based on object size&quot;&quot;&quot; max_dim = max(size.values()) if max_dim &lt; 0.05: # Small objects return 'pinch' elif max_dim &lt; 0.1: # Medium objects return 'power' else: # Large objects return 'hook' def _suggest_similar_objects(self, target_object: str, scene: Dict) -&gt; List[str]: &quot;&quot;&quot;Suggest similar objects if target not found&quot;&quot;&quot; suggestions = [] target_lower = target_object.lower() for obj in scene['objects']: obj_name = obj['name'].lower() obj_class = obj['class'].lower() # Check for partial matches if (target_lower in obj_name or target_lower in obj_class or obj_name in target_lower or obj_class in target_lower): suggestions.append(obj['name']) return suggestions[:3] # Return top 3 suggestions async def _analyze_placement_location(self, target_object: str, target_location: str) -&gt; Dict: &quot;&quot;&quot;Analyze suitable placement location&quot;&quot;&quot; current_scene = await self._sense_current_environment() # Find suitable placement surface placement_surface = await self._find_placement_surface(target_location, current_scene) if placement_surface: # Check if surface is suitable for the object object_details = await self._localize_target_object(target_object) if object_details['position']: # Object is currently held placement_analysis = await self._analyze_placement_suitability( placement_surface, object_details['object_details'] ) return { 'target_object': target_object, 'target_location': target_location, 'placement_surface': placement_surface, 'suitability_analysis': placement_analysis, 'placement_feasibility': placement_analysis['feasible'] } return { 'target_object': target_object, 'target_location': target_location, 'placement_surface': None, 'suitability_analysis': None, 'placement_feasibility': False, 'recommendations': await self._recommend_alternative_placement(current_scene) } async def _find_placement_surface(self, location_hint: str, scene: Dict) -&gt; Optional[Dict]: &quot;&quot;&quot;Find a suitable placement surface&quot;&quot;&quot; # Look for horizontal surfaces in the scene for obj in scene['objects']: if obj['class'] in ['table', 'counter', 'desk', 'shelf'] and obj['confidence'] &gt; 0.8: if (location_hint.lower() in obj['name'].lower() or location_hint.lower() in obj['class'].lower()): return { 'name': obj['name'], 'position': obj['position'], 'surface_area': self._estimate_surface_area(obj), 'clear_space': self._estimate_clear_space(obj, scene) } # If no specific surface found, return the most suitable one for obj in scene['objects']: if obj['class'] in ['table', 'counter', 'desk'] and obj['confidence'] &gt; 0.7: return { 'name': obj['name'], 'position': obj['position'], 'surface_area': self._estimate_surface_area(obj), 'clear_space': self._estimate_clear_space(obj, scene) } return None def _estimate_surface_area(self, obj: Dict) -&gt; float: &quot;&quot;&quot;Estimate surface area of placement surface&quot;&quot;&quot; size = self._estimate_object_size(obj) # For horizontal surfaces, use width × depth return size['width'] * size['depth'] def _estimate_clear_space(self, surface: Dict, scene: Dict) -&gt; Dict: &quot;&quot;&quot;Estimate clear space on a surface&quot;&quot;&quot; surface_size = self._estimate_object_size(surface) occupied_space = 0 # Check for objects on or near the surface surface_height = surface['position'][2] + surface_size['height']/2 for obj in scene['objects']: if (abs(obj['position'][2] - surface_height) &lt; 0.1 and # On the surface obj['name'] != surface['name']): # Not the surface itself obj_size = self._estimate_object_size(obj) occupied_space += obj_size['width'] * obj_size['depth'] available_area = self._estimate_surface_area(surface) - occupied_space return { 'total_area': self._estimate_surface_area(surface), 'occupied_area': occupied_space, 'available_area': max(0, available_area), 'is_suitable': available_area &gt; 0.01 # At least 100cm² available } async def _analyze_placement_suitability(self, surface: Dict, object_details: Dict) -&gt; Dict: &quot;&quot;&quot;Analyze if placement surface is suitable for object&quot;&quot;&quot; obj_size = self._estimate_object_size(object_details) clear_space = surface['clear_space'] # Check if object fits on surface fits = (obj_size['width'] &lt;= surface['surface_area']**0.5 and obj_size['depth'] &lt;= surface['surface_area']**0.5) # Check if surface has enough clear space has_space = clear_space['available_area'] &gt; (obj_size['width'] * obj_size['depth'] * 1.5) # Check stability factors object_weight = self._estimate_weight(object_details, obj_size) surface_stability = self._assess_surface_stability(surface) return { 'fits': fits, 'has_space': has_space, 'object_weight': object_weight, 'surface_stability': surface_stability, 'feasible': fits and has_space and surface_stability['is_stable'], 'recommendations': [] if fits and has_space else ['Look for larger surface', 'Clear space on current surface'] } def _assess_surface_stability(self, surface: Dict) -&gt; Dict: &quot;&quot;&quot;Assess stability of placement surface&quot;&quot;&quot; # Simplified stability assessment return { 'is_stable': True, 'stability_score': 0.9, 'factors': ['level_surface', 'adequate_size', 'clear_area'] } async def _recommend_alternative_placement(self, scene: Dict) -&gt; List[str]: &quot;&quot;&quot;Recommend alternative placement locations&quot;&quot;&quot; alternatives = [] for obj in scene['objects']: if obj['class'] in ['table', 'counter', 'desk'] and obj['confidence'] &gt; 0.7: clear_space = self._estimate_clear_space(obj, scene) if clear_space['available_area'] &gt; 0.02: # More than 200cm² alternatives.append(f&quot;{obj['name']} - {clear_space['available_area']:.3f}m² available&quot;) return alternatives[:3] # Top 3 alternatives async def _detect_people(self) -&gt; Dict: &quot;&quot;&quot;Detect people in the environment&quot;&quot;&quot; current_scene = await self._sense_current_environment() people = [] for obj in current_scene['objects']: if obj['class'] == 'person' and obj['confidence'] &gt; 0.8: people.append({ 'name': obj.get('name', 'person'), 'position': obj['position'], 'confidence': obj['confidence'], 'visibility': self._assess_person_visibility(obj, current_scene) }) return { 'people_detected': len(people), 'people_details': people, 'primary_person': people[0] if people else None } def _assess_person_visibility(self, person_obj: Dict, scene: Dict) -&gt; str: &quot;&quot;&quot;Assess how visible the person is&quot;&quot;&quot; # Check if person is obstructed obstructions = 0 for obj in scene['objects']: if (obj['class'] != 'person' and obj['confidence'] &gt; 0.7 and self._is_between_observer_and_person(obj, person_obj)): obstructions += 1 if obstructions == 0: return 'clear_view' elif obstructions == 1: return 'partially_obstructed' else: return 'heavily_obstructed' def _is_between_observer_and_person(self, obj: Dict, person: Dict) -&gt; bool: &quot;&quot;&quot;Check if object is between observer (robot) and person&quot;&quot;&quot; # Simplified check - in practice would use more sophisticated geometry robot_pos = [0, 0, 0] # Robot at origin for this example person_pos = person['position'] obj_pos = obj['position'] # Check if object is roughly on the line between robot and person robot_to_person = [person_pos[i] - robot_pos[i] for i in range(3)] robot_to_obj = [obj_pos[i] - robot_pos[i] for i in range(3)] # Calculate dot product to see if object is in the direction of person dot_product = sum(robot_to_obj[i] * robot_to_person[i] for i in range(3)) obj_distance = sum(x**2 for x in robot_to_obj)**0.5 person_distance = sum(x**2 for x in robot_to_person)**0.5 # If object is between robot and person (dot product positive and closer than person) return dot_product &gt; 0 and obj_distance &lt; person_distance * 0.8 async def _analyze_scene(self) -&gt; Dict: &quot;&quot;&quot;Perform comprehensive scene analysis&quot;&quot;&quot; current_scene = await self._sense_current_environment() # Perform detailed analysis analysis = { 'object_inventory': self._create_object_inventory(current_scene), 'spatial_relationships': self._analyze_spatial_relationships(current_scene), 'activity_recognition': self._recognize_activities(current_scene), 'navigation_analysis': await self._analyze_navigation_options(current_scene), 'manipulation_analysis': self._analyze_manipulation_opportunities(current_scene) } return {**current_scene, **analysis} def _create_object_inventory(self, scene: Dict) -&gt; Dict: &quot;&quot;&quot;Create inventory of objects in scene&quot;&quot;&quot; inventory = {} for obj in scene['objects']: obj_class = obj['class'] if obj_class not in inventory: inventory[obj_class] = [] inventory[obj_class].append({ 'name': obj['name'], 'position': obj['position'], 'confidence': obj['confidence'] }) return inventory def _analyze_spatial_relationships(self, scene: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Analyze spatial relationships between objects&quot;&quot;&quot; relationships = [] objects = scene['objects'] for i, obj1 in enumerate(objects): for j, obj2 in enumerate(objects[i+1:], i+1): # Calculate spatial relationship pos1 = obj1['position'] pos2 = obj2['position'] distance = sum((pos1[k] - pos2[k])**2 for k in range(3))**0.5 relationship = { 'object1': obj1['name'], 'object2': obj2['name'], 'distance': distance, 'spatial_relationship': self._classify_spatial_relationship(pos1, pos2, distance) } relationships.append(relationship) return relationships def _classify_spatial_relationship(self, pos1: List[float], pos2: List[float], distance: float) -&gt; str: &quot;&quot;&quot;Classify spatial relationship between two objects&quot;&quot;&quot; if distance &lt; 0.3: return 'adjacent' elif distance &lt; 1.0: return 'near' elif distance &lt; 3.0: return 'in_same_area' else: return 'separate_areas' def _recognize_activities(self, scene: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Recognize potential activities based on objects present&quot;&quot;&quot; activities = [] # Look for combinations that suggest activities object_names = [obj['name'].lower() for obj in scene['objects']] object_classes = [obj['class'].lower() for obj in scene['objects']] if 'person' in object_classes and 'chair' in object_classes: activities.append({ 'activity': 'sitting', 'confidence': 0.8, 'participants': ['person', 'chair'] }) if 'person' in object_classes and 'cup' in object_classes: activities.append({ 'activity': 'drinking', 'confidence': 0.7, 'participants': ['person', 'cup'] }) if 'book' in object_classes and 'table' in object_classes: activities.append({ 'activity': 'reading', 'confidence': 0.6, 'participants': ['book', 'table'] }) return activities async def _analyze_navigation_options(self, scene: Dict) -&gt; Dict: &quot;&quot;&quot;Analyze navigation options in the scene&quot;&quot;&quot; # Identify potential navigation destinations destinations = [] for loc in scene['locations']: if loc['confidence'] &gt; 0.7: destinations.append({ 'name': loc['name'], 'position': loc['position'], 'accessibility': self._assess_accessibility(loc, scene) }) return { 'potential_destinations': destinations, 'obstacle_map': self._create_obstacle_map(scene), 'safe_zones': self._identify_safe_zones(scene), 'navigation_complexity': self._assess_navigation_complexity(scene) } def _assess_accessibility(self, location: Dict, scene: Dict) -&gt; Dict: &quot;&quot;&quot;Assess accessibility of a location&quot;&quot;&quot; robot_pos = scene['robot_position'] loc_pos = location['position'] # Calculate straight-line distance distance = sum((loc_pos[i] - robot_pos[i])**2 for i in range(2))**0.5 # Check for obstacles along the path obstacles = self._find_path_obstacles(robot_pos, loc_pos, scene) return { 'distance': distance, 'obstacles_count': len(obstacles), 'traversable': len(obstacles) &lt; 3, # Fewer than 3 obstacles is acceptable 'estimated_travel_time': distance / 0.5 # Assuming 0.5 m/s walking speed } def _find_path_obstacles(self, start: List[float], end: List[float], scene: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Find obstacles along a path from start to end&quot;&quot;&quot; obstacles = [] for obj in scene['objects']: if obj['confidence'] &gt; 0.8: # High confidence detections # Simplified check - in practice would use more sophisticated path analysis obj_pos = obj['position'] # Check if object is roughly on the path between start and end start_to_end = [end[i] - start[i] for i in range(2)] start_to_obj = [obj_pos[i] - start[i] for i in range(2)] # Calculate projection of object onto path path_length_sq = sum(x**2 for x in start_to_end) if path_length_sq &gt; 0.01: # Avoid division by zero projection_scale = sum(start_to_obj[i] * start_to_end[i] for i in range(2)) / path_length_sq projection_scale = max(0, min(1, projection_scale)) # Clamp to path segment closest_point = [ start[i] + projection_scale * start_to_end[i] for i in range(2) ] # Calculate distance from object to path distance_to_path = sum((obj_pos[i] - closest_point[i])**2 for i in range(2))**0.5 if distance_to_path &lt; 0.5: # Within 50cm of path obstacles.append({ 'name': obj['name'], 'position': obj['position'], 'distance_to_path': distance_to_path }) return obstacles def _create_obstacle_map(self, scene: Dict) -&gt; List[List[float]]: &quot;&quot;&quot;Create a 2D obstacle map from scene&quot;&quot;&quot; # Simplified obstacle map - in practice would be more detailed map_size = 20 # 20x20 grid obstacle_map = [[0.0 for _ in range(map_size)] for _ in range(map_size)] # Convert object positions to grid coordinates grid_resolution = 0.2 # 20cm per grid cell robot_pos = scene['robot_position'] for obj in scene['objects']: if obj['confidence'] &gt; 0.7: grid_x = int((obj['position'][0] - robot_pos[0]) / grid_resolution + map_size/2) grid_y = int((obj['position'][1] - robot_pos[1]) / grid_resolution + map_size/2) if 0 &lt;= grid_x &lt; map_size and 0 &lt;= grid_y &lt; map_size: # Higher value means more obstacle (less traversable) obstacle_map[grid_y][grid_x] = min(1.0, obj['confidence']) return obstacle_map def _identify_safe_zones(self, scene: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Identify safe zones in the environment&quot;&quot;&quot; safe_zones = [] # Look for open areas away from obstacles obstacle_map = self._create_obstacle_map(scene) map_size = len(obstacle_map) for y in range(map_size): for x in range(map_size): if obstacle_map[y][x] &lt; 0.3: # Low obstacle density # Check surrounding area for safety is_safe = True safe_radius = 2 # Check 2-grid-cell radius for dy in range(-safe_radius, safe_radius + 1): for dx in range(-safe_radius, safe_radius + 1): ny, nx = y + dy, x + dx if 0 &lt;= ny &lt; map_size and 0 &lt;= nx &lt; map_size: if obstacle_map[ny][nx] &gt; 0.7: # High obstacle density is_safe = False break if not is_safe: break if is_safe: safe_zones.append({ 'grid_position': [x, y], 'world_position': self._grid_to_world([x, y], scene), 'radius': safe_radius * 0.2 # Convert grid cells to meters }) return safe_zones def _grid_to_world(self, grid_pos: List[int], scene: Dict) -&gt; List[float]: &quot;&quot;&quot;Convert grid coordinates to world coordinates&quot;&quot;&quot; grid_resolution = 0.2 # 20cm per grid cell robot_pos = scene['robot_position'] world_x = robot_pos[0] + (grid_pos[0] - 10) * grid_resolution # 10 is half map size world_y = robot_pos[1] + (grid_pos[1] - 10) * grid_resolution return [world_x, world_y, robot_pos[2]] def _assess_navigation_complexity(self, scene: Dict) -&gt; str: &quot;&quot;&quot;Assess overall navigation complexity&quot;&quot;&quot; obstacle_count = len([obj for obj in scene['objects'] if obj['confidence'] &gt; 0.7]) open_area_ratio = self._calculate_open_area_ratio(scene) if obstacle_count &lt; 3 and open_area_ratio &gt; 0.7: return 'simple' elif obstacle_count &lt; 8 and open_area_ratio &gt; 0.4: return 'moderate' else: return 'complex' def _calculate_open_area_ratio(self, scene: Dict) -&gt; float: &quot;&quot;&quot;Calculate ratio of open to obstructed areas&quot;&quot;&quot; obstacle_map = self._create_obstacle_map(scene) total_cells = len(obstacle_map) * len(obstacle_map[0]) free_cells = sum(1 for row in obstacle_map for cell in row if cell &lt; 0.3) return free_cells / total_cells if total_cells &gt; 0 else 0.0 def _analyze_manipulation_opportunities(self, scene: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Analyze objects suitable for manipulation&quot;&quot;&quot; opportunities = [] for obj in scene['objects']: if obj['confidence'] &gt; 0.8: # High confidence for manipulation size = self._estimate_object_size(obj) # Check if object is manipulable is_manipulable = (0.02 &lt; size['width'] &lt; 0.3 and # 2cm to 30cm width 0.02 &lt; size['height'] &lt; 0.4 and # 2cm to 40cm height 0.02 &lt; size['depth'] &lt; 0.3) # 2cm to 30cm depth if is_manipulable: opportunities.append({ 'object_name': obj['name'], 'object_class': obj['class'], 'position': obj['position'], 'estimated_weight': self._estimate_weight(obj, size), 'manipulability_score': self._calculate_manipulability_score(obj, size), 'recommended_action': self._recommend_manipulation_action(obj, size) }) return opportunities def _calculate_manipulability_score(self, obj: Dict, size: Dict) -&gt; float: &quot;&quot;&quot;Calculate how manipulable an object is&quot;&quot;&quot; # Consider size, weight, and position size_score = min(1.0, 0.2 / max(size.values())) # Prefer medium-sized objects weight = self._estimate_weight(obj, size) weight_score = max(0.0, min(1.0, 2.0 / max(weight, 0.1))) # Prefer light objects position_score = 1.0 if obj['position'][2] &lt; 1.2 else 0.7 # Prefer reachable heights return (size_score * 0.3 + weight_score * 0.4 + position_score * 0.3) def _recommend_manipulation_action(self, obj: Dict, size: Dict) -&gt; str: &quot;&quot;&quot;Recommend appropriate manipulation action&quot;&quot;&quot; max_size = max(size.values()) if max_size &lt; 0.05: # Small objects return 'precision_grasp' elif max_size &lt; 0.15: # Medium objects return 'power_grasp' else: # Large objects return 'two_hand_grasp' async def _has_significant_changes(self, new_scene: Dict) -&gt; bool: &quot;&quot;&quot;Check if there are significant changes in the scene&quot;&quot;&quot; if not self.scene_cache: return True # First scene is always a change # Compare with most recent scene recent_times = sorted(self.scene_cache.keys(), reverse=True) if recent_times: recent_scene = self.scene_cache[recent_times[0]] # Compare number of objects obj_count_diff = abs(len(new_scene['objects']) - len(recent_scene['objects'])) if obj_count_diff &gt; 2: # More than 2 objects difference return True # Compare positions of existing objects position_changes = 0 for new_obj in new_scene['objects']: for recent_obj in recent_scene['objects']: if (new_obj['name'] == recent_obj['name'] and new_obj['class'] == recent_obj['class']): pos_diff = sum((new_obj['position'][i] - recent_obj['position'][i])**2 for i in range(3))**0.5 if pos_diff &gt; 0.2: # Moved more than 20cm position_changes += 1 break if position_changes &gt; 1: # More than 1 object moved significantly return True return False async def _process_queued_tasks(self): &quot;&quot;&quot;Process any queued vision tasks&quot;&quot;&quot; pass async def is_healthy(self) -&gt; bool: &quot;&quot;&quot;Check if vision system is healthy&quot;&quot;&quot; return self.is_running def is_processing(self) -&gt; bool: &quot;&quot;&quot;Check if vision system is actively processing&quot;&quot;&quot; return not self.processing_queue.empty() async def get_status(self) -&gt; Dict: &quot;&quot;&quot;Get vision system status&quot;&quot;&quot; return { 'running': self.is_running, 'queue_size': self.processing_queue.qsize(), 'active_processes': len(self.active_processes), 'cached_scenes': len(self.scene_cache), 'object_database_size': len(self.object_database.entries), 'processor_loaded': self.vision_processor is not None } class ObjectDatabase: &quot;&quot;&quot;Database for storing and recognizing objects&quot;&quot;&quot; def __init__(self): self.entries = {} def add_object(self, obj_data: Dict): &quot;&quot;&quot;Add object to database&quot;&quot;&quot; obj_id = f&quot;{obj_data['class']}_{len(self.entries)}&quot; self.entries[obj_id] = { 'id': obj_id, 'data': obj_data, 'appearance_model': self._extract_appearance_features(obj_data), 'last_seen': time.time() } def _extract_appearance_features(self, obj_data: Dict) -&gt; Dict: &quot;&quot;&quot;Extract appearance features for recognition&quot;&quot;&quot; # This would use computer vision techniques return { 'color_histogram': [0.3, 0.4, 0.3], # Simplified 'shape_descriptor': 'unknown', 'texture_features': 'smooth' } def find_similar_objects(self, query_features: Dict, threshold: float = 0.7) -&gt; List[Dict]: &quot;&quot;&quot;Find objects similar to query features&quot;&quot;&quot; similar = [] for obj_id, obj_data in self.entries.items(): similarity = self._calculate_similarity(query_features, obj_data['appearance_model']) if similarity &gt; threshold: similar.append({ 'id': obj_id, 'similarity': similarity, 'data': obj_data['data'] }) return sorted(similar, key=lambda x: x['similarity'], reverse=True) def _calculate_similarity(self, features1: Dict, features2: Dict) -&gt; float: &quot;&quot;&quot;Calculate similarity between two feature sets&quot;&quot;&quot; # Simplified similarity calculation color_sim = min(1.0, 1.0 - abs(features1.get('color_histogram', [0])[0] - features2.get('color_histogram', [0])[0])) return color_sim * 0.8 + 0.2 # Simplified formula   ","version":"Next","tagName":"h3"},{"title":"Validation and Testing​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#validation-and-testing","content":" ","version":"Next","tagName":"h2"},{"title":"Comprehensive System Validation​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#comprehensive-system-validation","content":" class CapstoneValidator: def __init__(self, capstone_system: CapstoneSystem): &quot;&quot;&quot;Initialize validator for the capstone system&quot;&quot;&quot; self.capstone_system = capstone_system self.test_results = [] self.performance_benchmarks = { 'control_frequency': 100, # Hz 'vision_frequency': 30, # Hz 'language_response_time': 2.0, # seconds 'task_completion_rate': 0.95, # 95% 'safety_incidents': 0, # none allowed 'memory_usage': 0.8, # 80% threshold 'cpu_usage': 0.85 # 85% threshold } async def run_comprehensive_validation(self) -&gt; Dict: &quot;&quot;&quot;Run comprehensive validation of the capstone system&quot;&quot;&quot; logger.info(&quot;Starting comprehensive capstone validation...&quot;) validation_results = { 'system_integration': await self.validate_system_integration(), 'performance_benchmarks': await self.validate_performance_benchmarks(), 'safety_validation': await self.validate_safety_systems(), 'functionality_tests': await self.run_functionality_tests(), 'stress_testing': await self.run_stress_tests(), 'real_world_validation': await self.run_real_world_tests(), 'simulation_validation': await self.run_simulation_tests() } # Calculate overall validation score overall_score = self._calculate_overall_score(validation_results) final_result = { 'overall_score': overall_score, 'validation_results': validation_results, 'summary': self._generate_validation_summary(validation_results), 'recommendations': self._generate_recommendations(validation_results), 'timestamp': time.time() } logger.info(f&quot;Comprehensive validation completed with score: {overall_score:.2f}&quot;) return final_result async def validate_system_integration(self) -&gt; Dict: &quot;&quot;&quot;Validate that all subsystems are properly integrated&quot;&quot;&quot; logger.info(&quot;Validating system integration...&quot;) # Check if all systems are running system_status = await self.capstone_system.get_system_status() integration_checks = { 'language_system_connected': system_status['language_status']['running'], 'vision_system_connected': system_status['vision_status']['running'], 'action_system_connected': system_status['action_status']['running'], 'control_system_connected': system_status['control_status']['running'], 'communication_bus_healthy': True, # This would be checked in real implementation 'safety_system_active': system_status['safety_status']['system_safe'] } # Test cross-module communication communication_tests = await self._test_cross_module_communication() integration_results = { 'checks_passed': sum(1 for check in integration_checks.values() if check), 'total_checks': len(integration_checks), 'individual_checks': integration_checks, 'communication_tests': communication_tests, 'integration_score': self._calculate_integration_score(integration_checks, communication_tests) } return integration_results async def _test_cross_module_communication(self) -&gt; Dict: &quot;&quot;&quot;Test communication between modules&quot;&quot;&quot; # Test language -&gt; vision communication lang_to_vision_success = await self._test_module_communication('language', 'vision') # Test vision -&gt; action communication vision_to_action_success = await self._test_module_communication('vision', 'action') # Test action -&gt; control communication action_to_control_success = await self._test_module_communication('action', 'control') return { 'lang_to_vision': lang_to_vision_success, 'vision_to_action': vision_to_action_success, 'action_to_control': action_to_control_success, 'overall_communication_score': (lang_to_vision_success + vision_to_action_success + action_to_control_success) / 3.0 } async def _test_module_communication(self, source: str, destination: str) -&gt; bool: &quot;&quot;&quot;Test communication between two modules&quot;&quot;&quot; try: # Send test message from source to destination test_message = { 'type': 'test_message', 'source': source, 'destination': destination, 'timestamp': time.time(), 'test_id': f'test_{source}_to_{destination}_{int(time.time())}' } # In a real implementation, this would send the message through the communication bus # and wait for an acknowledgment await asyncio.sleep(0.1) # Simulate communication delay # For this simulation, assume communication is successful # In real implementation, would check for actual acknowledgment return True except Exception as e: logger.error(f&quot;Communication test failed between {source} and {destination}: {e}&quot;) return False def _calculate_integration_score(self, checks: Dict, comm_tests: Dict) -&gt; float: &quot;&quot;&quot;Calculate integration score based on checks and communication tests&quot;&quot;&quot; check_score = sum(1 for check in checks.values() if check) / len(checks) if checks else 0 comm_score = comm_tests.get('overall_communication_score', 0) # Weighted average (checks: 60%, communication: 40%) return check_score * 0.6 + comm_score * 0.4 async def validate_performance_benchmarks(self) -&gt; Dict: &quot;&quot;&quot;Validate that system meets performance benchmarks&quot;&quot;&quot; logger.info(&quot;Validating performance benchmarks...&quot;) # Get current performance metrics system_status = await self.capstone_system.get_system_status() metrics = system_status['performance_metrics'] benchmark_tests = { 'control_frequency_met': metrics.get('control_frequency', 0) &gt;= self.performance_benchmarks['control_frequency'], 'vision_frequency_met': metrics.get('vision_frequency', 0) &gt;= self.performance_benchmarks['vision_frequency'], 'language_response_time_met': metrics.get('language_response_time', float('inf')) &lt;= self.performance_benchmarks['language_response_time'], 'memory_usage_acceptable': metrics.get('memory_usage', 1.0) &lt;= self.performance_benchmarks['memory_usage'], 'cpu_usage_acceptable': metrics.get('cpu_usage', 1.0) &lt;= self.performance_benchmarks['cpu_usage'] } # Calculate performance score performance_score = sum(1 for test in benchmark_tests.values() if test) / len(benchmark_tests) return { 'tests_passed': sum(1 for test in benchmark_tests.values() if test), 'total_tests': len(benchmark_tests), 'individual_tests': benchmark_tests, 'performance_score': performance_score, 'current_metrics': metrics } async def validate_safety_systems(self) -&gt; Dict: &quot;&quot;&quot;Validate safety systems are functioning properly&quot;&quot;&quot; logger.info(&quot;Validating safety systems...&quot;) # Get safety status system_status = await self.capstone_system.get_system_status() safety_status = system_status['safety_status'] safety_tests = { 'emergency_stop_functional': safety_status.get('emergency_stop_available', False), 'collision_avoidance_active': safety_status.get('collision_avoidance_enabled', False), 'torque_limits_enforced': safety_status.get('torque_limits_monitored', False), 'velocity_limits_enforced': safety_status.get('velocity_limits_monitored', False), 'no_current_violations': not safety_status.get('active_violations', []), 'safety_monitoring_active': safety_status.get('monitoring_active', False) } # Run safety-specific tests safety_functionality_tests = await self._run_safety_functionality_tests() safety_results = { 'tests_passed': sum(1 for test in safety_tests.values() if test), 'total_tests': len(safety_tests), 'individual_tests': safety_tests, 'functionality_tests': safety_functionality_tests, 'safety_score': self._calculate_safety_score(safety_tests, safety_functionality_tests) } return safety_results async def _run_safety_functionality_tests(self) -&gt; Dict: &quot;&quot;&quot;Run specific functionality tests for safety systems&quot;&quot;&quot; # Test emergency stop emergency_stop_test = await self._test_emergency_stop() # Test collision avoidance collision_avoidance_test = await self._test_collision_avoidance() # Test safety limits limits_test = await self._test_safety_limits() return { 'emergency_stop': emergency_stop_test, 'collision_avoidance': collision_avoidance_test, 'safety_limits': limits_test, 'overall_safety_functionality': (emergency_stop_test + collision_avoidance_test + limits_test) / 3.0 } async def _test_emergency_stop(self) -&gt; bool: &quot;&quot;&quot;Test emergency stop functionality&quot;&quot;&quot; try: # Simulate emergency stop activation await asyncio.sleep(0.1) # Simulate stop command processing # In real implementation, would trigger and verify stop return True except Exception: return False async def _test_collision_avoidance(self) -&gt; bool: &quot;&quot;&quot;Test collision avoidance functionality&quot;&quot;&quot; try: # Simulate collision scenario and verify avoidance await asyncio.sleep(0.1) # Simulate scenario processing # In real implementation, would create and verify response to collision scenario return True except Exception: return False async def _test_safety_limits(self) -&gt; bool: &quot;&quot;&quot;Test safety limits enforcement&quot;&quot;&quot; try: # Simulate exceeding limits and verify enforcement await asyncio.sleep(0.1) # Simulate limit checking # In real implementation, would test torque, velocity, etc. limits return True except Exception: return False def _calculate_safety_score(self, safety_tests: Dict, functionality_tests: Dict) -&gt; float: &quot;&quot;&quot;Calculate safety score based on tests&quot;&quot;&quot; test_score = sum(1 for test in safety_tests.values() if test) / len(safety_tests) if safety_tests else 0 func_score = functionality_tests.get('overall_safety_functionality', 0) return test_score * 0.5 + func_score * 0.5 async def run_functionality_tests(self) -&gt; Dict: &quot;&quot;&quot;Run functionality tests for the complete system&quot;&quot;&quot; logger.info(&quot;Running functionality tests...&quot;) # Define test scenarios test_scenarios = [ { 'name': 'basic_navigation', 'command': 'Go to the kitchen', 'expected_outcomes': ['navigation_initiated', 'path_planned', 'movement_started'] }, { 'name': 'object_localization', 'command': 'Find the red cup', 'expected_outcomes': ['object_search_initiated', 'object_detected', 'position_known'] }, { 'name': 'grasping_task', 'command': 'Pick up the book', 'expected_outcomes': ['approach_planned', 'grasp_attempted', 'object_grasped'] }, { 'name': 'placement_task', 'command': 'Put the cup on the table', 'expected_outcomes': ['placement_location_identified', 'navigation_to_location', 'object_placed'] }, { 'name': 'greeting_interaction', 'command': 'Say hello to John', 'expected_outcomes': ['person_detected', 'greeting_performed', 'interaction_completed'] } ] test_results = {} for scenario in test_scenarios: result = await self._run_single_functionality_test(scenario) test_results[scenario['name']] = result # Calculate overall functionality score successful_tests = sum(1 for result in test_results.values() if result['success']) functionality_score = successful_tests / len(test_results) if test_results else 0 return { 'test_results': test_results, 'successful_tests': successful_tests, 'total_tests': len(test_results), 'functionality_score': functionality_score } async def _run_single_functionality_test(self, scenario: Dict) -&gt; Dict: &quot;&quot;&quot;Run a single functionality test&quot;&quot;&quot; try: logger.info(f&quot;Running functionality test: {scenario['name']}&quot;) # Submit the test command correlation_id = await self.capstone_system.submit_command( scenario['command'], context={'test_scenario': scenario['name']} ) if not correlation_id: return { 'success': False, 'error': 'Failed to submit command', 'executed_steps': [], 'outcome_attainment': 0.0 } # Monitor the execution start_time = time.time() max_wait_time = 30.0 # Wait up to 30 seconds executed_steps = [] achieved_outcomes = [] while time.time() - start_time &lt; max_wait_time: # In real implementation, would monitor actual execution # For simulation, assume progress is being made await asyncio.sleep(0.5) # Simulate achievement of outcomes if len(achieved_outcomes) &lt; len(scenario['expected_outcomes']): achieved_outcomes.append(scenario['expected_outcomes'][len(achieved_outcomes)]) executed_steps.append(f&quot;Step {len(executed_steps) + 1}: {achieved_outcomes[-1]}&quot;) if len(achieved_outcomes) == len(scenario['expected_outcomes']): break # Calculate outcome attainment outcome_attainment = len(achieved_outcomes) / len(scenario['expected_outcomes']) return { 'success': outcome_attainment &gt;= 0.8, # 80% of outcomes achieved 'executed_steps': executed_steps, 'achieved_outcomes': achieved_outcomes, 'expected_outcomes': scenario['expected_outcomes'], 'outcome_attainment': outcome_attainment, 'execution_time': time.time() - start_time } except Exception as e: logger.error(f&quot;Error in functionality test {scenario['name']}: {e}&quot;) return { 'success': False, 'error': str(e), 'executed_steps': [], 'outcome_attainment': 0.0 } async def run_stress_tests(self) -&gt; Dict: &quot;&quot;&quot;Run stress tests to validate system robustness&quot;&quot;&quot; logger.info(&quot;Running stress tests...&quot;) # Define stress test scenarios stress_scenarios = [ { 'name': 'high_load_concurrent_commands', 'description': 'Multiple simultaneous commands', 'commands': [ 'Go to kitchen', 'Find red cup', 'Pick up book', 'Say hello', 'Turn left', 'Stop' ], 'duration': 60 # seconds }, { 'name': 'long_running_task', 'description': 'Extended operation test', 'commands': ['Patrol the house'], 'duration': 300 # 5 minutes }, { 'name': 'rapid_command_switching', 'description': 'Frequent command changes', 'commands': ['Go to kitchen', 'Go to bedroom', 'Go to living room'] * 10, 'duration': 120 # 2 minutes } ] stress_results = {} for scenario in stress_scenarios: result = await self._run_single_stress_test(scenario) stress_results[scenario['name']] = result return { 'stress_results': stress_results, 'passed_scenarios': sum(1 for result in stress_results.values() if result['passed']), 'total_scenarios': len(stress_results), 'stress_resilience_score': self._calculate_stress_score(stress_results) } async def _run_single_stress_test(self, scenario: Dict) -&gt; Dict: &quot;&quot;&quot;Run a single stress test&quot;&quot;&quot; try: logger.info(f&quot;Running stress test: {scenario['name']}&quot;) start_time = time.time() end_time = start_time + scenario['duration'] commands_sent = 0 errors_encountered = 0 system_state_log = [] while time.time() &lt; end_time: # Send commands in a pattern based on the scenario if scenario['name'] == 'high_load_concurrent_commands': # Send all commands simultaneously (simulate) for cmd in scenario['commands']: try: await self.capstone_system.submit_command(cmd) commands_sent += 1 except: errors_encountered += 1 await asyncio.sleep(2) # Allow time for processing elif scenario['name'] == 'rapid_command_switching': # Rapidly switch between commands for cmd in scenario['commands']: try: await self.capstone_system.submit_command(cmd) commands_sent += 1 await asyncio.sleep(0.1) # Rapid switching except: errors_encountered += 1 # Log system state periodically if int(time.time()) % 10 == 0: # Log every 10 seconds state = await self.capstone_system.get_system_status() system_state_log.append({ 'timestamp': time.time(), 'state': state, 'commands_sent': commands_sent, 'errors': errors_encountered }) await asyncio.sleep(0.5) # Determine if test passed based on error rate and system stability error_rate = errors_encountered / max(commands_sent, 1) passed = error_rate &lt; 0.1 and errors_encountered &lt; 5 # Less than 10% error rate and fewer than 5 errors return { 'passed': passed, 'commands_sent': commands_sent, 'errors_encountered': errors_encountered, 'error_rate': error_rate, 'duration_actual': time.time() - start_time, 'system_stability': self._assess_system_stability(system_state_log), 'final_system_state': await self.capstone_system.get_system_status() } except Exception as e: logger.error(f&quot;Error in stress test {scenario['name']}: {e}&quot;) return { 'passed': False, 'error': str(e), 'commands_sent': 0, 'errors_encountered': 1 } def _assess_system_stability(self, state_log: List[Dict]) -&gt; Dict: &quot;&quot;&quot;Assess system stability during stress test&quot;&quot;&quot; if not state_log: return {'stable': False, 'metrics': {}} # Analyze metrics over time cpu_readings = [entry['state']['performance_metrics'].get('cpu_usage', 0) for entry in state_log] memory_readings = [entry['state']['performance_metrics'].get('memory_usage', 0) for entry in state_log] safety_violations = [entry['state']['safety_status'].get('active_violations', []) for entry in state_log] stability_metrics = { 'average_cpu': sum(cpu_readings) / len(cpu_readings) if cpu_readings else 0, 'max_cpu': max(cpu_readings) if cpu_readings else 0, 'average_memory': sum(memory_readings) / len(memory_readings) if memory_readings else 0, 'max_memory': max(memory_readings) if memory_readings else 0, 'safety_violations_count': sum(len(violations) for violations in safety_violations), 'oscillation_indicators': self._detect_oscillations(cpu_readings) } # Determine stability based on metrics cpu_stable = stability_metrics['max_cpu'] &lt; 0.95 # Less than 95% CPU memory_stable = stability_metrics['max_memory'] &lt; 0.95 # Less than 95% memory safety_stable = stability_metrics['safety_violations_count'] == 0 oscillation_stable = stability_metrics['oscillation_indicators']['severity'] &lt; 0.3 overall_stable = cpu_stable and memory_stable and safety_stable and oscillation_stable return { 'stable': overall_stable, 'metrics': stability_metrics, 'factors': { 'cpu_stable': cpu_stable, 'memory_stable': memory_stable, 'safety_stable': safety_stable, 'oscillation_stable': oscillation_stable } } def _detect_oscillations(self, readings: List[float]) -&gt; Dict: &quot;&quot;&quot;Detect oscillations in system metrics&quot;&quot;&quot; if len(readings) &lt; 3: return {'detected': False, 'severity': 0.0, 'frequency': 0} # Simple oscillation detection: check for fluctuations diffs = [abs(readings[i] - readings[i-1]) for i in range(1, len(readings))] avg_diff = sum(diffs) / len(diffs) if diffs else 0 # Count significant fluctuations (&gt;10% of average value) avg_value = sum(readings) / len(readings) if readings else 1 significant_fluctuations = sum(1 for diff in diffs if diff &gt; 0.1 * avg_value) oscillation_severity = (significant_fluctuations / len(diffs)) * (avg_diff / avg_value) if avg_value &gt; 0 else 0 return { 'detected': oscillation_severity &gt; 0.1, 'severity': oscillation_severity, 'frequency': significant_fluctuations / len(diffs) if diffs else 0 } def _calculate_stress_score(self, stress_results: Dict) -&gt; float: &quot;&quot;&quot;Calculate stress resilience score&quot;&quot;&quot; if not stress_results: return 0.0 passed_tests = sum(1 for result in stress_results.values() if result['passed']) stability_scores = [result['system_stability']['metrics']['oscillation_indicators']['severity'] for result in stress_results.values()] # Lower oscillation severity is better (more stable) avg_stability = sum(stability_scores) / len(stability_scores) if stability_scores else 0 stability_score = max(0, 1 - avg_stability) # Invert so lower oscillation = higher score # Combine test success rate and stability test_success_rate = passed_tests / len(stress_results) return (test_success_rate * 0.6 + stability_score * 0.4) async def run_real_world_tests(self) -&gt; Dict: &quot;&quot;&quot;Run real-world validation tests&quot;&quot;&quot; logger.info(&quot;Running real-world validation tests...&quot;) # Note: In a real implementation, these would require physical hardware # For simulation purposes, we'll return mock results real_world_tests = { 'navigation_accuracy': await self._test_navigation_accuracy(), 'manipulation_precision': await self._test_manipulation_precision(), 'human_interaction_quality': await self._test_human_interaction_quality(), 'environment_adaptability': await self._test_environment_adaptability() } # Calculate real-world validation score successful_tests = sum(1 for result in real_world_tests.values() if result.get('passed', False)) total_tests = len(real_world_tests) real_world_score = successful_tests / total_tests if total_tests &gt; 0 else 0 return { 'tests': real_world_tests, 'successful_tests': successful_tests, 'total_tests': total_tests, 'real_world_score': real_world_score } async def _test_navigation_accuracy(self) -&gt; Dict: &quot;&quot;&quot;Test navigation accuracy in real world&quot;&quot;&quot; # Simulated test results return { 'passed': True, 'accuracy': 0.95, # 95% accuracy 'distance_error_mean': 0.05, # 5cm average error 'completion_rate': 0.98, # 98% successful completions 'obstacle_avoidance_success': 0.99 # 99% successful obstacle avoidance } async def _test_manipulation_precision(self) -&gt; Dict: &quot;&quot;&quot;Test manipulation precision in real world&quot;&quot;&quot; # Simulated test results return { 'passed': True, 'grasp_success_rate': 0.92, # 92% grasp success 'placement_accuracy': 0.88, # 88% placement accuracy 'object_damage_incidents': 0, # No damage incidents 'manipulation_speed': 15.0 # 15 seconds average per manipulation } async def _test_human_interaction_quality(self) -&gt; Dict: &quot;&quot;&quot;Test quality of human interaction&quot;&quot;&quot; # Simulated test results return { 'passed': True, 'understanding_accuracy': 0.94, # 94% command understanding 'response_appropriateness': 0.91, # 91% appropriate responses 'interaction_naturalness': 4.2, # Rating out of 5 'user_satisfaction': 4.5 # Rating out of 5 } async def _test_environment_adaptability(self) -&gt; Dict: &quot;&quot;&quot;Test adaptability to different environments&quot;&quot;&quot; # Simulated test results return { 'passed': True, 'indoor_performance': 0.96, # 96% success indoors 'outdoor_performance': 0.89, # 89% success outdoors 'dynamic_environment_handling': 0.91, # 91% with moving obstacles 'lighting_condition_adaptability': 0.93 # 93% in various lighting } async def run_simulation_tests(self) -&gt; Dict: &quot;&quot;&quot;Run simulation-based validation tests&quot;&quot;&quot; logger.info(&quot;Running simulation validation tests...&quot;) if not self.capstone_system.config.simulation_mode: return { 'tests_run': 0, 'simulation_specific_score': 0.0, 'message': 'Simulation tests skipped - not in simulation mode' } # Run simulation-specific tests simulation_tests = { 'sim_to_real_transfer': await self._test_sim_to_real_transfer(), 'physics_accuracy': await self._test_physics_accuracy(), 'sensor_simulation_validity': await self._test_sensor_simulation_validity(), 'control_stability_simulation': await self._test_control_stability_simulation() } # Calculate simulation score successful_tests = sum(1 for result in simulation_tests.values() if result.get('passed', False)) total_tests = len(simulation_tests) simulation_score = successful_tests / total_tests if total_tests &gt; 0 else 0 return { 'tests': simulation_tests, 'successful_tests': successful_tests, 'total_tests': total_tests, 'simulation_score': simulation_score } async def _test_sim_to_real_transfer(self) -&gt; Dict: &quot;&quot;&quot;Test sim-to-real transfer capability&quot;&quot;&quot; return { 'passed': True, 'transfer_accuracy': 0.87, # 87% of behaviors transfer successfully 'domain_gap_measurement': 0.13, # 13% difference between sim and real 'adaptation_time_required': 120.0 # 2 minutes average adaptation time } async def _test_physics_accuracy(self) -&gt; Dict: &quot;&quot;&quot;Test accuracy of physics simulation&quot;&quot;&quot; return { 'passed': True, 'collision_detection_accuracy': 0.98, # 98% accurate 'balance_simulation_accuracy': 0.94, # 94% accurate 'manipulation_physics_accuracy': 0.91, # 91% accurate 'realism_score': 4.3 # Out of 5 } async def _test_sensor_simulation_validity(self) -&gt; Dict: &quot;&quot;&quot;Test validity of sensor simulation&quot;&quot;&quot; return { 'passed': True, 'camera_simulation_accuracy': 0.95, # 95% accurate 'lidar_simulation_accuracy': 0.93, # 93% accurate 'imu_simulation_accuracy': 0.97, # 97% accurate 'force_torque_simulation_accuracy': 0.92 # 92% accurate } async def _test_control_stability_simulation(self) -&gt; Dict: &quot;&quot;&quot;Test control system stability in simulation&quot;&quot;&quot; return { 'passed': True, 'balance_stability_score': 0.96, # 96% stable 'locomotion_stability_score': 0.94, # 94% stable 'manipulation_stability_score': 0.95, # 95% stable 'control_frequency_maintained': True # 100Hz maintained } def _calculate_overall_score(self, validation_results: Dict) -&gt; float: &quot;&quot;&quot;Calculate overall validation score&quot;&quot;&quot; # Weight different validation aspects weights = { 'system_integration': 0.2, 'performance_benchmarks': 0.15, 'safety_validation': 0.2, 'functionality_tests': 0.2, 'stress_testing': 0.15, 'real_world_validation': 0.05, 'simulation_validation': 0.05 # Only if in simulation mode } # Calculate weighted score total_score = 0.0 for category, weight in weights.items(): if category in validation_results: result = validation_results[category] if isinstance(result, dict) and 'overall_score' in result: total_score += result['overall_score'] * weight elif isinstance(result, dict) and 'functionality_score' in result: total_score += result['functionality_score'] * weight elif isinstance(result, dict) and 'performance_score' in result: total_score += result['performance_score'] * weight elif isinstance(result, dict) and 'safety_score' in result: total_score += result['safety_score'] * weight elif isinstance(result, dict) and 'stress_resilience_score' in result: total_score += result['stress_resilience_score'] * weight elif isinstance(result, dict) and 'real_world_score' in result: total_score += result['real_world_score'] * weight elif isinstance(result, dict) and 'simulation_score' in result: total_score += result['simulation_score'] * weight return total_score def _generate_validation_summary(self, validation_results: Dict) -&gt; Dict: &quot;&quot;&quot;Generate validation summary&quot;&quot;&quot; return { 'total_categories_validated': len(validation_results), 'categories_passed': sum(1 for result in validation_results.values() if isinstance(result, dict) and (result.get('overall_score', 0) &gt;= 0.8 or result.get('functionality_score', 0) &gt;= 0.8 or result.get('performance_score', 0) &gt;= 0.8)), 'critical_failures': self._identify_critical_failures(validation_results), 'performance_insights': self._extract_performance_insights(validation_results), 'safety_assessment': validation_results.get('safety_validation', {}).get('safety_score', 0), 'recommendation_urgency': self._assess_recommendation_urgency(validation_results) } def _identify_critical_failures(self, validation_results: Dict) -&gt; List[str]: &quot;&quot;&quot;Identify critical failures in validation&quot;&quot;&quot; critical_failures = [] # Check for critical safety failures safety_result = validation_results.get('safety_validation', {}) if safety_result.get('safety_score', 1.0) &lt; 0.9: critical_failures.append('Safety systems validation failed') # Check for critical functionality failures func_result = validation_results.get('functionality_tests', {}) if func_result.get('functionality_score', 1.0) &lt; 0.8: critical_failures.append('Basic functionality validation failed') # Check for critical performance failures perf_result = validation_results.get('performance_benchmarks', {}) if perf_result.get('performance_score', 1.0) &lt; 0.7: critical_failures.append('Performance benchmarks validation failed') return critical_failures def _extract_performance_insights(self, validation_results: Dict) -&gt; Dict: &quot;&quot;&quot;Extract performance insights from validation results&quot;&quot;&quot; insights = {} # Performance insights perf_result = validation_results.get('performance_benchmarks', {}) if perf_result: insights['control_performance'] = f&quot;{perf_result.get('performance_score', 0):.1%} of benchmarks met&quot; metrics = perf_result.get('current_metrics', {}) insights['current_cpu_usage'] = f&quot;{metrics.get('cpu_usage', 0)*100:.1f}%&quot; insights['current_memory_usage'] = f&quot;{metrics.get('memory_usage', 0)*100:.1f}%&quot; # Stress test insights stress_result = validation_results.get('stress_testing', {}) if stress_result: insights['stress_resilience'] = f&quot;{stress_result.get('stress_resilience_score', 0):.1%} resilient under stress&quot; return insights def _assess_recommendation_urgency(self, validation_results: Dict) -&gt; str: &quot;&quot;&quot;Assess urgency level for recommendations&quot;&quot;&quot; critical_failures = len(self._identify_critical_failures(validation_results)) if critical_failures &gt; 0: return 'CRITICAL - Immediate attention required' elif validation_results.get('overall_score', 1.0) &lt; 0.8: return 'HIGH - Significant improvements needed' elif validation_results.get('overall_score', 1.0) &lt; 0.9: return 'MEDIUM - Moderate improvements recommended' else: return 'LOW - Minor optimizations possible' def _generate_recommendations(self, validation_results: Dict) -&gt; List[Dict]: &quot;&quot;&quot;Generate recommendations based on validation results&quot;&quot;&quot; recommendations = [] # Safety recommendations safety_result = validation_results.get('safety_validation', {}) if safety_result.get('safety_score', 1.0) &lt; 0.95: recommendations.append({ 'area': 'Safety Systems', 'priority': 'HIGH', 'recommendation': 'Review and enhance safety system implementations', 'justification': f&quot;Safety score of {safety_result.get('safety_score', 0):.2f} is below threshold&quot; }) # Performance recommendations perf_result = validation_results.get('performance_benchmarks', {}) if perf_result.get('performance_score', 1.0) &lt; 0.9: recommendations.append({ 'area': 'Performance Optimization', 'priority': 'HIGH', 'recommendation': 'Optimize system performance to meet benchmarks', 'justification': f&quot;Performance score of {perf_result.get('performance_score', 0):.2f} needs improvement&quot; }) # Functionality recommendations func_result = validation_results.get('functionality_tests', {}) if func_result.get('functionality_score', 1.0) &lt; 0.9: recommendations.append({ 'area': 'Core Functionality', 'priority': 'HIGH', 'recommendation': 'Address functionality gaps identified in testing', 'justification': f&quot;Functionality score of {func_result.get('functionality_score', 0):.2f} indicates issues&quot; }) # Stress resilience recommendations stress_result = validation_results.get('stress_testing', {}) if stress_result.get('stress_resilience_score', 1.0) &lt; 0.85: recommendations.append({ 'area': 'System Robustness', 'priority': 'MEDIUM', 'recommendation': 'Improve system resilience under stress conditions', 'justification': f&quot;Stress resilience score of {stress_result.get('stress_resilience_score', 0):.2f} needs enhancement&quot; }) return recommendations   ","version":"Next","tagName":"h3"},{"title":"Constitution Alignment​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#constitution-alignment","content":" This capstone chapter addresses all constitutional requirements:  ","version":"Next","tagName":"h2"},{"title":"VLA Convergence Mandate (Principle I)​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#vla-convergence-mandate-principle-i","content":" Complete integration of Vision-Language-Action pipelineLanguage as primary control interface for humanoid operationUnified cognitive architecture for autonomous operation  ","version":"Next","tagName":"h3"},{"title":"Real-Time Validation (Principle IV)​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#real-time-validation-principle-iv","content":" 100Hz control loops for humanoid balance and stabilityReal-time performance validation and monitoringTiming constraints for safe humanoid operation  ","version":"Next","tagName":"h3"},{"title":"Anthropomorphic Focus (Principle II)​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#anthropomorphic-focus-principle-ii","content":" Human-like interaction patterns and capabilitiesBipedal locomotion and dexterous manipulationHuman-centered environment operation  ","version":"Next","tagName":"h3"},{"title":"Sim-to-Real Rigor (Principle III)​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#sim-to-real-rigor-principle-iii","content":" Comprehensive validation in both simulation and real environmentsDomain transfer testing for sim-to-real applicationsPhysics accuracy validation for realistic simulation  ","version":"Next","tagName":"h3"},{"title":"Target Hardware Optimization (Constraint)​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#target-hardware-optimization-constraint","content":" Efficient algorithms suitable for Jetson Orin deploymentPerformance optimization for embedded systemsMemory and computation constraints respected  ","version":"Next","tagName":"h3"},{"title":"RAG Stack Integration (Standard V)​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#rag-stack-integration-standard-v","content":" Embedded RAG system for knowledge-based interactionOpenAI Agents/ChatKit SDK integrationFastAPI and database integration for retrieval  ","version":"Next","tagName":"h3"},{"title":"Practical Examples​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#practical-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Complete Autonomous Task Execution​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#example-1-complete-autonomous-task-execution","content":" async def demonstrate_autonomous_task(): &quot;&quot;&quot;Demonstrate complete autonomous task execution&quot;&quot;&quot; print(&quot;=== Capstone Project: Complete Autonomous Task ===&quot;) # Initialize configuration for real hardware config = CapstoneConfiguration( target_hardware=&quot;NVIDIA Jetson Orin Nano (8GB)&quot;, simulation_mode=True, # Set to False for real hardware control_frequency=100, vision_frequency=30, language_frequency=10 ) # Initialize the capstone system capstone_system = CapstoneSystem(config) try: # Initialize the system await capstone_system.initialize_system() print(&quot;✓ System initialized&quot;) # Start autonomous operation success = await capstone_system.start_autonomous_operation() if not success: print(&quot;✗ Failed to start autonomous operation&quot;) return print(&quot;✓ Autonomous operation started&quot;) # Run a complex multi-step task print(&quot;\\nExecuting complex task: 'Go to kitchen, find red cup, pick it up, and bring to living room'&quot;) command = &quot;Go to the kitchen, find the red cup on the counter, pick it up, and bring it to the living room&quot; correlation_id = await capstone_system.submit_command(command) print(f&quot;✓ Command submitted: {command}&quot;) print(f&quot; Correlation ID: {correlation_id}&quot;) # Monitor execution print(&quot;\\nMonitoring execution...&quot;) start_time = time.time() max_execution_time = 120 # 2 minutes max while time.time() - start_time &lt; max_execution_time: status = await capstone_system.get_system_status() print(f&quot; System state: {status['system_running']}, Safety: {status['safety_engaged']}&quot;) # Check if task is complete (simplified check) if status['language_status']['queue_size'] == 0 and status['action_status']['active_tasks'] == 0: print(&quot; ✓ Task appears to be complete&quot;) break await asyncio.sleep(2) # Check every 2 seconds # Get final status final_status = await capstone_system.get_system_status() print(f&quot;\\nFinal system status:&quot;) print(f&quot; Running: {final_status['system_running']}&quot;) print(f&quot; Safety engaged: {final_status['safety_engaged']}&quot;) print(f&quot; Performance: {final_status['performance_metrics']}&quot;) # Run validation print(&quot;\\nRunning comprehensive validation...&quot;) validator = CapstoneValidator(capstone_system) validation_result = await validator.run_comprehensive_validation() print(f&quot;\\nValidation Results:&quot;) print(f&quot; Overall Score: {validation_result['overall_score']:.2f}&quot;) print(f&quot; System Integration: {validation_result['validation_results']['system_integration']['integration_score']:.2f}&quot;) print(f&quot; Performance: {validation_result['validation_results']['performance_benchmarks']['performance_score']:.2f}&quot;) print(f&quot; Safety: {validation_result['validation_results']['safety_validation']['safety_score']:.2f}&quot;) print(f&quot; Functionality: {validation_result['validation_results']['functionality_tests']['functionality_score']:.2f}&quot;) if validation_result['overall_score'] &gt;= 0.9: print(&quot; ✓ EXCELLENT: System validation passed with high score!&quot;) elif validation_result['overall_score'] &gt;= 0.8: print(&quot; ✓ GOOD: System validation passed!&quot;) else: print(&quot; ⚠ CONCERNING: System validation needs improvement&quot;) except Exception as e: print(f&quot;Error during autonomous task execution: {e}&quot;) import traceback traceback.print_exc() finally: # Stop autonomous operation await capstone_system.stop_autonomous_operation() print(&quot;✓ Autonomous operation stopped&quot;) # Run the demonstration if __name__ == &quot;__main__&quot;: asyncio.run(demonstrate_autonomous_task())   ","version":"Next","tagName":"h3"},{"title":"Example 2: Validation Report Generation​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#example-2-validation-report-generation","content":" import json from datetime import datetime class ValidationReportGenerator: def __init__(self, validator: CapstoneValidator): &quot;&quot;&quot;Initialize validation report generator&quot;&quot;&quot; self.validator = validator self.reports = [] async def generate_final_report(self, validation_result: Dict) -&gt; str: &quot;&quot;&quot;Generate comprehensive validation report&quot;&quot;&quot; report = { 'capstone_project_validation_report': { 'timestamp': datetime.now().isoformat(), 'project_phase': 'Capstone Integration', 'system_configuration': self.validator.capstone_system.get_configuration().__dict__, 'validation_summary': validation_result['summary'], 'detailed_results': validation_result['validation_results'], 'overall_score': validation_result['overall_score'], 'recommendations': validation_result['recommendations'], 'conclusion': self._generate_conclusion(validation_result), 'signatures': { 'generated_by': 'Capstone Validation System', 'generated_at': datetime.now().isoformat(), 'validation_confidence': validation_result['overall_score'] } } } # Save report to file filename = f&quot;capstone_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json&quot; with open(filename, 'w') as f: json.dump(report, f, indent=2) self.reports.append(filename) return filename def _generate_conclusion(self, validation_result: Dict) -&gt; Dict: &quot;&quot;&quot;Generate conclusion based on validation results&quot;&quot;&quot; overall_score = validation_result['overall_score'] critical_failures = validation_result['summary']['critical_failures'] safety_score = validation_result['validation_results']['safety_validation']['safety_score'] conclusion = { 'executive_summary': '', 'readiness_assessment': '', 'next_steps': [], 'confidence_level': '' } if critical_failures: conclusion['executive_summary'] = &quot;CRITICAL ISSUES IDENTIFIED - System not ready for deployment&quot; conclusion['readiness_assessment'] = &quot;NOT READY - Critical safety and functionality issues detected&quot; conclusion['confidence_level'] = &quot;LOW&quot; conclusion['next_steps'] = [ &quot;Address all critical failures immediately&quot;, &quot;Re-run validation after fixes&quot;, &quot;Consider additional safety measures&quot; ] elif overall_score &gt;= 0.95 and safety_score &gt;= 0.98: conclusion['executive_summary'] = &quot;EXCELLENT PERFORMANCE - System ready for advanced testing&quot; conclusion['readiness_assessment'] = &quot;READY FOR ADVANCED TESTING - Excellent results across all areas&quot; conclusion['confidence_level'] = &quot;VERY HIGH&quot; conclusion['next_steps'] = [ &quot;Proceed to extended field testing&quot;, &quot;Begin user acceptance testing&quot;, &quot;Prepare for deployment in controlled environments&quot; ] elif overall_score &gt;= 0.90: conclusion['executive_summary'] = &quot;GOOD PERFORMANCE - System ready for field testing&quot; conclusion['readiness_assessment'] = &quot;READY FOR FIELD TESTING - Good performance with minor areas for improvement&quot; conclusion['confidence_level'] = &quot;HIGH&quot; conclusion['next_steps'] = [ &quot;Conduct field testing in real environments&quot;, &quot;Gather user feedback&quot;, &quot;Implement recommended optimizations&quot; ] elif overall_score &gt;= 0.80: conclusion['executive_summary'] = &quot;SATISFACTORY PERFORMANCE - System needs improvement before deployment&quot; conclusion['readiness_assessment'] = &quot;NEEDS IMPROVEMENT - Satisfactory but requires enhancements&quot; conclusion['confidence_level'] = &quot;MEDIUM&quot; conclusion['next_steps'] = [ &quot;Address identified issues&quot;, &quot;Re-run validation after improvements&quot;, &quot;Conduct additional testing in specific areas&quot; ] else: conclusion['executive_summary'] = &quot;INSUFFICIENT PERFORMANCE - Major improvements needed&quot; conclusion['readiness_assessment'] = &quot;NOT READY - Significant issues require attention&quot; conclusion['confidence_level'] = &quot;LOW&quot; conclusion['next_steps'] = [ &quot;Comprehensive system review required&quot;, &quot;Major improvements needed before re-validation&quot;, &quot;Consider redesign of problematic components&quot; ] return conclusion # Example usage async def generate_capstone_report(): &quot;&quot;&quot;Generate a complete capstone validation report&quot;&quot;&quot; # This would be called after running the comprehensive validation pass   ","version":"Next","tagName":"h3"},{"title":"Exercises​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#exercises","content":" ","version":"Next","tagName":"h2"},{"title":"Exercise 1: Complete System Integration​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#exercise-1-complete-system-integration","content":" Implement a complete autonomous humanoid robot system that:  Integrates all four modules (foundations, ROS 2, simulation, VLA)Executes end-to-end tasks with natural language commandsValidates system performance across all metricsHandles errors and safety conditions gracefully  ","version":"Next","tagName":"h3"},{"title":"Exercise 2: Real-World Deployment Validation​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#exercise-2-real-world-deployment-validation","content":" Validate the system in real-world scenarios by:  Testing navigation in actual human environmentsValidating manipulation tasks with real objectsAssessing human-robot interaction qualityMeasuring sim-to-real transfer effectiveness  ","version":"Next","tagName":"h3"},{"title":"Exercise 3: Performance Optimization​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#exercise-3-performance-optimization","content":" Optimize the complete system for performance by:  Reducing latency in the VLA pipelineImproving real-time control responsivenessOptimizing resource usage on target hardwareEnhancing overall system efficiency  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#summary","content":" The capstone project represents the culmination of the Physical AI &amp; Humanoid Robotics curriculum, demonstrating the complete Vision-Language-Action pipeline in an autonomous humanoid robot system. This project validates all the principles established throughout the course: the VLA Convergence Mandate that makes language the primary control interface, the Real-Time Validation requirements for safe humanoid operation, the Anthropomorphic Focus that enables human-centered interaction, and the Sim-to-Real Rigor that ensures effective transfer from simulation to reality. The autonomous humanoid system created in this capstone project demonstrates the ability to receive natural language commands, perceive the environment through vision systems, plan appropriate actions, and execute them safely and effectively in human-centered environments. This represents a significant achievement in Physical AI and marks the transition from learning to practical application of humanoid robotics.  ","version":"Next","tagName":"h2"},{"title":"Further Reading​","type":1,"pageTitle":"Chapter 18 - Capstone: Autonomous Humanoid Robot Project","url":"/docs/chapters/module-4-vla/chapter-18-capstone-project#further-reading","content":" &quot;Humanoid Robotics: A Reference&quot; by Goswami and Vadakkepat (Capstone projects section)&quot;Field Robotics&quot; by Nourbakhsh and Akin (Real-world deployment)&quot;Human-Robot Interaction&quot; by Goodrich and Schultz (Interaction quality)&quot;Robotics Research&quot; - Latest research on autonomous humanoid systems&quot;Physical AI: Principles and Applications&quot; - Advanced Physical AI concepts ","version":"Next","tagName":"h2"}],"options":{"languages":["en"],"id":"default"}}