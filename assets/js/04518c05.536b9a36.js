"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[451],{4128(n,e,t){t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"chapters/module-4-vla/chapter-15-language-understanding","title":"Chapter 15 - Language Understanding in Robotics","description":"Language understanding systems for robotics applications and humanoid robots","source":"@site/docs/chapters/module-4-vla/chapter-15-language-understanding.md","sourceDirName":"chapters/module-4-vla","slug":"/chapters/module-4-vla/chapter-15-language-understanding","permalink":"/docs/chapters/module-4-vla/chapter-15-language-understanding","draft":false,"unlisted":false,"editUrl":"https://github.com/RIMZAASAD/Robotic-ai-Book/edit/main/website/docs/chapters/module-4-vla/chapter-15-language-understanding.md","tags":[],"version":"current","frontMatter":{"title":"Chapter 15 - Language Understanding in Robotics","module":"Vision-Language-Action Pipelines","chapter":15,"description":"Language understanding systems for robotics applications and humanoid robots","learningObjectives":["Implement language understanding for robotics","Apply NLP techniques to humanoid robot interaction","Integrate language with vision and action systems"],"prerequisites":["chapter-14-computer-vision"],"difficulty":"advanced"},"sidebar":"textbookSidebar","previous":{"title":"Chapter 14: Computer Vision for Robotics","permalink":"/docs/chapters/module-4-vla/chapter-14-computer-vision"},"next":{"title":"Chapter 16: Action Planning & Control Systems","permalink":"/docs/chapters/module-4-vla/chapter-16-action-planning"}}');var o=t(4848),a=t(8453);const s={title:"Chapter 15 - Language Understanding in Robotics",module:"Vision-Language-Action Pipelines",chapter:15,description:"Language understanding systems for robotics applications and humanoid robots",learningObjectives:["Implement language understanding for robotics","Apply NLP techniques to humanoid robot interaction","Integrate language with vision and action systems"],prerequisites:["chapter-14-computer-vision"],difficulty:"advanced"},r=void 0,c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Language Understanding in Robotics Context",id:"language-understanding-in-robotics-context",level:2},{value:"The VLA Pipeline Architecture",id:"the-vla-pipeline-architecture",level:3},{value:"Key Challenges in Robotic Language Understanding",id:"key-challenges-in-robotic-language-understanding",level:3},{value:"Ambiguity Resolution",id:"ambiguity-resolution",level:4},{value:"Situated Language Understanding",id:"situated-language-understanding",level:4},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:4},{value:"Natural Language Processing for Robotics",id:"natural-language-processing-for-robotics",level:2},{value:"Command Parsing and Semantic Analysis",id:"command-parsing-and-semantic-analysis",level:3},{value:"Intent Recognition and Dialogue Management",id:"intent-recognition-and-dialogue-management",level:3},{value:"Integration with Vision and Action Systems",id:"integration-with-vision-and-action-systems",level:2},{value:"Vision-Language Integration",id:"vision-language-integration",level:3},{value:"Speech Processing and Natural Interaction",id:"speech-processing-and-natural-interaction",level:2},{value:"Speech-to-Text Integration",id:"speech-to-text-integration",level:3},{value:"Constitution Alignment",id:"constitution-alignment",level:2},{value:"VLA Convergence Mandate (Principle I)",id:"vla-convergence-mandate-principle-i",level:3},{value:"Real-Time Validation (Principle IV)",id:"real-time-validation-principle-iv",level:3},{value:"Anthropomorphic Focus (Principle II)",id:"anthropomorphic-focus-principle-ii",level:3},{value:"Target Hardware Optimization (Constraint)",id:"target-hardware-optimization-constraint",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Humanoid Assistant Dialogue System",id:"example-1-humanoid-assistant-dialogue-system",level:3},{value:"Example 2: Multi-Turn Dialogue Management",id:"example-2-multi-turn-dialogue-management",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Language Understanding Pipeline",id:"exercise-1-language-understanding-pipeline",level:3},{value:"Exercise 2: Context-Aware Resolution",id:"exercise-2-context-aware-resolution",level:3},{value:"Exercise 3: Real-Time Interaction",id:"exercise-3-real-time-interaction",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Implement language understanding for robotics"}),"\n",(0,o.jsx)(e.li,{children:"Apply NLP techniques to humanoid robot interaction"}),"\n",(0,o.jsx)(e.li,{children:"Integrate language with vision and action systems"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"Language understanding forms the Language component of the Vision-Language-Action (VLA) pipeline, which is central to our project's VLA Convergence Mandate principle. For humanoid robots operating in human-centered environments, language understanding systems must be capable of processing natural language commands, engaging in human-like interaction, and translating high-level linguistic goals into concrete robotic actions. This chapter explores natural language processing techniques specifically tailored for robotics applications, with special emphasis on humanoid robot interaction systems that must operate effectively in human environments using natural language as the primary control interface, as mandated by our constitution."}),"\n",(0,o.jsx)(e.h2,{id:"language-understanding-in-robotics-context",children:"Language Understanding in Robotics Context"}),"\n",(0,o.jsx)(e.h3,{id:"the-vla-pipeline-architecture",children:"The VLA Pipeline Architecture"}),"\n",(0,o.jsx)(e.p,{children:"The Vision-Language-Action pipeline in humanoid robots operates as follows:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-mermaid",children:"graph TD\n    A[Natural Language Command] --\x3e B[Language Understanding]\n    B --\x3e C[Action Planning]\n    C --\x3e D[Vision System]\n    D --\x3e E[Action Execution]\n    E --\x3e A\n"})}),"\n",(0,o.jsx)(e.p,{children:"This creates a closed loop where language commands are interpreted, plans are generated, vision systems provide feedback, and actions are executed, with continuous monitoring and adjustment."}),"\n",(0,o.jsx)(e.h3,{id:"key-challenges-in-robotic-language-understanding",children:"Key Challenges in Robotic Language Understanding"}),"\n",(0,o.jsx)(e.h4,{id:"ambiguity-resolution",children:"Ambiguity Resolution"}),"\n",(0,o.jsx)(e.p,{children:"Natural language commands often contain ambiguities that must be resolved through context and perception:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Spatial references"}),': "Pick up the cup" - which cup?']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Temporal references"}),': "Do it again" - what is "it"?']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Pragmatic understanding"}),': "It\'s cold" - should the robot adjust temperature?']}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"situated-language-understanding",children:"Situated Language Understanding"}),"\n",(0,o.jsx)(e.p,{children:"Robotic language understanding must be grounded in the physical environment:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Deixis"}),': Understanding "this", "that", "here", "there" in spatial context']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Perceptual grounding"}),": Connecting words to visual objects and locations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action grounding"}),": Connecting language to executable robot actions"]}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,o.jsx)(e.p,{children:"Language understanding in robotics must integrate with other modalities:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Visual context"}),": Understanding commands based on what the robot sees"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action context"}),": Understanding commands based on current robot state"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Temporal context"}),": Understanding commands based on recent interactions"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"natural-language-processing-for-robotics",children:"Natural Language Processing for Robotics"}),"\n",(0,o.jsx)(e.h3,{id:"command-parsing-and-semantic-analysis",children:"Command Parsing and Semantic Analysis"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import re\nimport spacy\nfrom typing import List, Dict, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n@dataclass\nclass RobotCommand:\n    """Represents a parsed robot command with semantic meaning"""\n    action: str\n    target_object: Optional[str] = None\n    target_location: Optional[str] = None\n    attributes: Dict[str, str] = None\n    confidence: float = 1.0\n\nclass Action(Enum):\n    """Enumeration of supported robot actions"""\n    MOVE = "move"\n    GRASP = "grasp"\n    PLACE = "place"\n    GREET = "greet"\n    FOLLOW = "follow"\n    FIND = "find"\n    REPORT = "report"\n    WAIT = "wait"\n    STOP = "stop"\n\nclass CommandParser:\n    def __init__(self):\n        """Initialize the command parser with spaCy model"""\n        try:\n            self.nlp = spacy.load("en_core_web_sm")\n        except OSError:\n            print("spaCy model not found. Install with: python -m spacy download en_core_web_sm")\n            # Fallback to simple rule-based parsing\n            self.nlp = None\n\n        # Define action patterns for rule-based parsing\n        self.action_patterns = {\n            Action.GRASP: [\n                r"pick up (.+)",\n                r"grab (.+)",\n                r"take (.+)",\n                r"get (.+)",\n                r"lift (.+)"\n            ],\n            Action.PLACE: [\n                r"put (.+) (?:on|in|at) (.+)",\n                r"place (.+) (?:on|in|at) (.+)",\n                r"drop (.+) (?:on|in|at) (.+)",\n                r"set (.+) (?:on|in|at) (.+)"\n            ],\n            Action.MOVE: [\n                r"go to (.+)",\n                r"move to (.+)",\n                r"walk to (.+)",\n                r"navigate to (.+)",\n                r"approach (.+)"\n            ],\n            Action.FOLLOW: [\n                r"follow (.+)",\n                r"come after (.+)",\n                r"accompany (.+)"\n            ],\n            Action.FIND: [\n                r"find (.+)",\n                r"locate (.+)",\n                r"search for (.+)",\n                r"look for (.+)"\n            ],\n            Action.GREET: [\n                r"hello",\n                r"hi",\n                r"greet (.+)",\n                r"say hello to (.+)"\n            ],\n            Action.REPORT: [\n                r"what do you see",\n                r"describe the room",\n                r"report (.+)",\n                r"tell me (.+)"\n            ],\n            Action.WAIT: [\n                r"wait",\n                r"stop",\n                r"pause",\n                r"hold on"\n            ]\n        }\n\n    def parse_command(self, text: str) -> Optional[RobotCommand]:\n        """\n        Parse a natural language command into structured representation\n\n        Args:\n            text: Natural language command\n\n        Returns:\n            Parsed RobotCommand or None if parsing fails\n        """\n        text = text.strip().lower()\n\n        if self.nlp:\n            # Use spaCy for advanced parsing\n            return self._parse_with_spacy(text)\n        else:\n            # Fallback to rule-based parsing\n            return self._parse_with_rules(text)\n\n    def _parse_with_spacy(self, text: str) -> Optional[RobotCommand]:\n        """Parse command using spaCy NLP model"""\n        doc = self.nlp(text)\n\n        # Extract main action (verb)\n        action = self._extract_action(doc)\n        if not action:\n            return None\n\n        # Extract target object\n        target_object = self._extract_target_object(doc)\n\n        # Extract target location\n        target_location = self._extract_target_location(doc)\n\n        # Extract additional attributes\n        attributes = self._extract_attributes(doc)\n\n        return RobotCommand(\n            action=action.value,\n            target_object=target_object,\n            target_location=target_location,\n            attributes=attributes\n        )\n\n    def _extract_action(self, doc) -> Optional[Action]:\n        """Extract the main action from parsed document"""\n        # Look for root verb or main action\n        for token in doc:\n            if token.pos_ == "VERB":\n                # Check if this verb corresponds to a known action\n                verb_lemma = token.lemma_.lower()\n\n                for action_enum in Action:\n                    if verb_lemma in action_enum.value:\n                        return action_enum\n\n                # Check for variations\n                if verb_lemma in ["take", "grasp", "seize", "catch"]:\n                    return Action.GRASP\n                elif verb_lemma in ["put", "place", "set", "drop"]:\n                    return Action.PLACE\n                elif verb_lemma in ["go", "move", "walk", "navigate", "approach"]:\n                    return Action.MOVE\n                elif verb_lemma in ["follow", "accompany", "chase"]:\n                    return Action.FOLLOW\n                elif verb_lemma in ["find", "locate", "search", "look"]:\n                    return Action.FIND\n                elif verb_lemma in ["greet", "hello", "hi", "wave"]:\n                    return Action.GREET\n                elif verb_lemma in ["report", "describe", "tell", "say"]:\n                    return Action.REPORT\n                elif verb_lemma in ["wait", "pause", "stop", "hold"]:\n                    return Action.WAIT\n\n        return None\n\n    def _extract_target_object(self, doc) -> Optional[str]:\n        """Extract target object from parsed document"""\n        # Look for direct objects and noun phrases\n        for token in doc:\n            if token.dep_ == "dobj":  # Direct object\n                # Get the full noun phrase\n                return self._get_full_noun_phrase(token)\n\n            # Look for objects after prepositions\n            if token.pos_ == "NOUN" and token.head.dep_ in ["pobj", "pcomp"]:\n                return self._get_full_noun_phrase(token)\n\n        # If no direct object found, look for noun phrases\n        for chunk in doc.noun_chunks:\n            if chunk.root.pos_ == "NOUN":\n                # Avoid pronouns and articles\n                if chunk.text.lower() not in ["it", "this", "that", "the", "a", "an"]:\n                    return chunk.text\n\n        return None\n\n    def _extract_target_location(self, doc) -> Optional[str]:\n        """Extract target location from parsed document"""\n        # Look for prepositional phrases indicating location\n        for token in doc:\n            if token.pos_ == "ADP" and token.text in ["to", "at", "on", "in", "by", "near"]:\n                # Look for the object of the preposition\n                for child in token.children:\n                    if child.pos_ == "NOUN" or child.pos_ == "PROPN":\n                        return self._get_full_noun_phrase(child)\n\n        # Look for noun chunks that might be locations\n        for chunk in doc.noun_chunks:\n            # Check if this could be a location based on context\n            if any(word in chunk.text.lower() for word in\n                  ["table", "kitchen", "room", "door", "chair", "couch", "bed", "desk"]):\n                return chunk.text\n\n        return None\n\n    def _extract_attributes(self, doc) -> Dict[str, str]:\n        """Extract additional attributes like colors, sizes, etc."""\n        attributes = {}\n\n        for token in doc:\n            if token.pos_ == "ADJ":  # Adjectives\n                # Look for adjectives that modify nouns\n                if token.head.pos_ in ["NOUN", "PROPN"]:\n                    noun_phrase = self._get_full_noun_phrase(token.head)\n                    attributes[f"adjective_{token.text}"] = noun_phrase\n\n            elif token.pos_ == "NUM":  # Numbers\n                # Look for numbers that might indicate quantities\n                attributes[f"number_{token.text}"] = token.head.text\n\n        return attributes\n\n    def _get_full_noun_phrase(self, token) -> str:\n        """Get the full noun phrase starting from a token"""\n        # This is a simplified approach - spaCy has more sophisticated methods\n        phrase = []\n\n        # Add left-side modifiers (adjectives, determiners)\n        for left_token in token.lefts:\n            if left_token.pos_ in ["ADJ", "DET"]:\n                phrase.append(left_token.text)\n\n        # Add the main token\n        phrase.append(token.text)\n\n        # Add right-side modifiers if any\n        for right_token in token.rights:\n            if right_token.pos_ in ["NOUN", "PROPN"] and right_token.dep_ == "compound":\n                phrase.append(right_token.text)\n\n        return " ".join(phrase)\n\n    def _parse_with_rules(self, text: str) -> Optional[RobotCommand]:\n        """Parse command using rule-based patterns (fallback)"""\n        for action, patterns in self.action_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text)\n                if match:\n                    groups = match.groups()\n\n                    if action in [Action.PLACE] and len(groups) >= 2:\n                        # Pattern: "put X on Y" - groups are [object, location]\n                        return RobotCommand(\n                            action=action.value,\n                            target_object=groups[0],\n                            target_location=groups[1]\n                        )\n                    elif len(groups) >= 1:\n                        # Pattern with one main argument\n                        return RobotCommand(\n                            action=action.value,\n                            target_object=groups[0] if len(groups) > 0 else None,\n                            target_location=groups[1] if len(groups) > 1 else None\n                        )\n                    else:\n                        # Pattern without arguments\n                        return RobotCommand(action=action.value)\n\n        # If no pattern matches, return None\n        return None\n\nclass ContextualCommandResolver:\n    def __init__(self, vision_system, robot_state):\n        """\n        Initialize resolver with access to vision and robot state\n\n        Args:\n            vision_system: Interface to robot\'s vision system\n            robot_state: Interface to robot\'s current state\n        """\n        self.vision_system = vision_system\n        self.robot_state = robot_state\n        self.command_parser = CommandParser()\n        self.conversation_context = []\n\n    def resolve_command(self, text: str, world_state: Dict = None) -> Optional[RobotCommand]:\n        """\n        Resolve an ambiguous command using context and perception\n\n        Args:\n            text: Natural language command\n            world_state: Current world state including objects and locations\n\n        Returns:\n            Resolved RobotCommand with specific references\n        """\n        # Parse the command first\n        parsed_command = self.command_parser.parse_command(text)\n        if not parsed_command:\n            return None\n\n        # Resolve ambiguous references using context\n        resolved_command = self._resolve_ambiguous_references(parsed_command, world_state)\n\n        # Add to conversation context\n        self.conversation_context.append({\n            \'command\': text,\n            \'parsed\': parsed_command,\n            \'resolved\': resolved_command,\n            \'timestamp\': time.time()\n        })\n\n        # Keep only recent context (last 10 interactions)\n        if len(self.conversation_context) > 10:\n            self.conversation_context = self.conversation_context[-10:]\n\n        return resolved_command\n\n    def _resolve_ambiguous_references(self, command: RobotCommand, world_state: Dict) -> RobotCommand:\n        """Resolve ambiguous references like \'it\', \'there\', etc. using context"""\n        resolved = RobotCommand(\n            action=command.action,\n            target_object=command.target_object,\n            target_location=command.target_location,\n            attributes=command.attributes or {}\n        )\n\n        # Resolve pronouns and demonstratives\n        if resolved.target_object and resolved.target_object.lower() in ["it", "that", "this"]:\n            # Use vision system to identify the most likely referent\n            resolved.target_object = self._resolve_pronoun_reference(\n                resolved.target_object, world_state\n            )\n\n        if resolved.target_location and resolved.target_location.lower() in ["there", "here", "that place"]:\n            # Use robot state and world state to resolve location\n            resolved.target_location = self._resolve_location_reference(\n                resolved.target_location, world_state\n            )\n\n        return resolved\n\n    def _resolve_pronoun_reference(self, pronoun: str, world_state: Dict) -> Optional[str]:\n        """Resolve pronoun references using visual context"""\n        if not world_state or \'visible_objects\' not in world_state:\n            return None\n\n        visible_objects = world_state[\'visible_objects\']\n\n        if pronoun.lower() == "it":\n            # Use the most recently mentioned or most salient object\n            if len(visible_objects) == 1:\n                return visible_objects[0][\'name\']\n            else:\n                # For now, return the first object - in practice, use more sophisticated resolution\n                return visible_objects[0][\'name\'] if visible_objects else None\n        elif pronoun.lower() in ["this", "that"]:\n            # Use spatial context or the closest object\n            closest_object = self._find_closest_object(visible_objects)\n            return closest_object[\'name\'] if closest_object else None\n\n        return None\n\n    def _resolve_location_reference(self, location_ref: str, world_state: Dict) -> Optional[str]:\n        """Resolve location references like \'here\', \'there\'"""\n        if not world_state:\n            return None\n\n        current_location = world_state.get(\'robot_location\', \'unknown\')\n\n        if location_ref.lower() == "here":\n            return current_location\n        elif location_ref.lower() == "there":\n            # Use the most recently mentioned location from context\n            for entry in reversed(self.conversation_context):\n                if entry[\'resolved\'].target_location:\n                    return entry[\'resolved\'].target_location\n\n        return None\n\n    def _find_closest_object(self, objects: List[Dict]) -> Optional[Dict]:\n        """Find the closest object based on spatial information"""\n        # This would use actual spatial relationships\n        # For now, return the first object\n        return objects[0] if objects else None\n'})}),"\n",(0,o.jsx)(e.h3,{id:"intent-recognition-and-dialogue-management",children:"Intent Recognition and Dialogue Management"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import json\nimport time\nfrom typing import List, Dict, Any\nfrom collections import defaultdict\n\nclass IntentRecognizer:\n    def __init__(self):\n        """Initialize intent recognition system"""\n        self.intents = self._load_intent_definitions()\n        self.context_manager = ContextManager()\n\n    def _load_intent_definitions(self) -> Dict:\n        """Load intent definitions with patterns and required entities"""\n        return {\n            "navigation": {\n                "patterns": [\n                    r"go to (.+)",\n                    r"move to (.+)",\n                    r"walk to (.+)",\n                    r"navigate to (.+)",\n                    r"approach (.+)"\n                ],\n                "required_entities": ["location"],\n                "optional_entities": ["speed", "caution_level"]\n            },\n            "object_manipulation": {\n                "patterns": [\n                    r"pick up (.+)",\n                    r"grab (.+)",\n                    r"take (.+)",\n                    r"get (.+) and put it (?:on|in) (.+)",\n                    r"move (.+) from (.+) to (.+)"\n                ],\n                "required_entities": ["object"],\n                "optional_entities": ["source_location", "target_location"]\n            },\n            "social_interaction": {\n                "patterns": [\n                    r"hello",\n                    r"hi",\n                    r"greet (.+)",\n                    r"introduce yourself",\n                    r"what\'s your name"\n                ],\n                "required_entities": [],\n                "optional_entities": ["person"]\n            },\n            "information_request": {\n                "patterns": [\n                    r"what can you do",\n                    r"help",\n                    r"what\'s possible",\n                    r"capabilities"\n                ],\n                "required_entities": [],\n                "optional_entities": []\n            },\n            "status_inquiry": {\n                "patterns": [\n                    r"what do you see",\n                    r"describe (.+)",\n                    r"report (.+)",\n                    r"tell me about (.+)"\n                ],\n                "required_entities": [],\n                "optional_entities": ["target"]\n            }\n        }\n\n    def recognize_intent(self, text: str) -> Dict[str, Any]:\n        """\n        Recognize intent and extract entities from text\n\n        Args:\n            text: Input text to analyze\n\n        Returns:\n            Dictionary with intent, confidence, and extracted entities\n        """\n        text_lower = text.lower().strip()\n\n        for intent_name, intent_def in self.intents.items():\n            for pattern in intent_def["patterns"]:\n                match = re.search(pattern, text_lower)\n                if match:\n                    entities = {\n                        f"entity_{i}": entity\n                        for i, entity in enumerate(match.groups())\n                    }\n\n                    # Validate required entities are present\n                    required_count = len(intent_def["required_entities"])\n                    if len(entities) >= required_count:\n                        return {\n                            "intent": intent_name,\n                            "confidence": 0.9,  # High confidence for pattern matches\n                            "entities": entities,\n                            "original_text": text\n                        }\n\n        # If no pattern matches, use default intent\n        return {\n            "intent": "unknown",\n            "confidence": 0.0,\n            "entities": {},\n            "original_text": text\n        }\n\n    def process_command_with_context(self, text: str, context: Dict = None) -> Dict[str, Any]:\n        """\n        Process command considering conversation context\n\n        Args:\n            text: Input command text\n            context: Current conversation context\n\n        Returns:\n            Processed command with context resolution\n        """\n        # Recognize intent and entities\n        result = self.recognize_intent(text)\n\n        # Resolve context-dependent references\n        if context:\n            result["entities"] = self._resolve_context_references(\n                result["entities"], context\n            )\n\n        # Update context\n        if context is not None:\n            context["last_command"] = result\n            context["command_history"].append(result)\n\n        return result\n\n    def _resolve_context_references(self, entities: Dict, context: Dict) -> Dict:\n        """Resolve entities that depend on conversation context"""\n        resolved_entities = entities.copy()\n\n        # Resolve pronouns and demonstratives based on context\n        for key, value in resolved_entities.items():\n            if value.lower() in ["it", "this", "that"]:\n                # Resolve based on previous context\n                previous_entities = self._get_recent_entities(context)\n                if previous_entities:\n                    # For "it", use the most recent object\n                    if value.lower() == "it":\n                        for prev_entity_key, prev_value in previous_entities.items():\n                            if "object" in prev_entity_key or "location" in prev_entity_key:\n                                resolved_entities[key] = prev_value\n                                break\n\n        return resolved_entities\n\n    def _get_recent_entities(self, context: Dict) -> Dict:\n        """Get entities from recent conversation"""\n        if "command_history" not in context:\n            return {}\n\n        recent_commands = context["command_history"][-3:]  # Last 3 commands\n        recent_entities = {}\n\n        for cmd in recent_commands:\n            if "entities" in cmd:\n                recent_entities.update(cmd["entities"])\n\n        return recent_entities\n\nclass ContextManager:\n    def __init__(self):\n        """Initialize conversation context manager"""\n        self.conversation_history = []\n        self.object_references = {}\n        self.location_references = {}\n        self.user_preferences = defaultdict(lambda: None)\n\n    def update_context(self, command_result: Dict):\n        """Update context with new command result"""\n        self.conversation_history.append(command_result)\n\n        # Update object and location references if present\n        entities = command_result.get("entities", {})\n        for key, value in entities.items():\n            if "object" in key.lower():\n                self.object_references[value] = len(self.conversation_history) - 1\n            elif "location" in key.lower():\n                self.location_references[value] = len(self.conversation_history) - 1\n\n        # Keep only recent history\n        if len(self.conversation_history) > 20:\n            self.conversation_history = self.conversation_history[-20:]\n\n    def get_context(self) -> Dict:\n        """Get current context for command processing"""\n        return {\n            "conversation_history": self.conversation_history[-10:],  # Last 10 exchanges\n            "object_references": dict(self.object_references),\n            "location_references": dict(self.location_references),\n            "user_preferences": dict(self.user_preferences),\n            "current_time": time.time()\n        }\n\nclass DialogueManager:\n    def __init__(self):\n        """Initialize dialogue management system"""\n        self.intent_recognizer = IntentRecognizer()\n        self.response_generator = ResponseGenerator()\n        self.context_manager = ContextManager()\n\n    def process_input(self, user_input: str, world_state: Dict = None) -> Dict:\n        """\n        Process user input and generate appropriate response\n\n        Args:\n            user_input: Natural language input from user\n            world_state: Current state of the world (objects, locations, etc.)\n\n        Returns:\n            Dictionary with action to take and response to give\n        """\n        # Get current context\n        context = self.context_manager.get_context()\n\n        # Recognize intent with context\n        intent_result = self.intent_recognizer.process_command_with_context(\n            user_input, context\n        )\n\n        # Generate appropriate response based on intent\n        response = self.response_generator.generate_response(\n            intent_result, world_state, context\n        )\n\n        # Update context\n        self.context_manager.update_context({\n            "input": user_input,\n            "intent_result": intent_result,\n            "response": response\n        })\n\n        return {\n            "intent": intent_result["intent"],\n            "entities": intent_result["entities"],\n            "action": response.get("action"),\n            "response_text": response.get("response_text"),\n            "confidence": intent_result["confidence"]\n        }\n\nclass ResponseGenerator:\n    def __init__(self):\n        """Initialize response generation system"""\n        self.response_templates = self._load_response_templates()\n\n    def _load_response_templates(self) -> Dict:\n        """Load response templates for different intents"""\n        return {\n            "navigation": [\n                "I will navigate to {location}.",\n                "Moving toward {location} now.",\n                "Heading to {location}."\n            ],\n            "object_manipulation": [\n                "I will {action} the {object}.",\n                "Attempting to {action} the {object}.",\n                "Processing manipulation of {object}."\n            ],\n            "social_interaction": [\n                "Hello! How can I assist you?",\n                "Greetings! What would you like me to do?",\n                "Hi there! How may I help you?"\n            ],\n            "information_request": [\n                "I can help with navigation, object manipulation, and social interaction.",\n                "My capabilities include moving around, picking up objects, and talking with you.",\n                "I can navigate spaces, manipulate objects, and engage in conversation."\n            ],\n            "status_inquiry": [\n                "I see {objects_count} objects in view.",\n                "The room contains {objects_list}.",\n                "I can see: {objects_list}."\n            ],\n            "unknown": [\n                "I\'m not sure I understand. Could you rephrase that?",\n                "I didn\'t catch that. Can you say it differently?",\n                "I\'m not sure what you mean. Could you be more specific?"\n            ]\n        }\n\n    def generate_response(self, intent_result: Dict, world_state: Dict, context: Dict) -> Dict:\n        """Generate appropriate response based on intent and world state"""\n        intent = intent_result["intent"]\n        entities = intent_result["entities"]\n\n        # Select template based on intent\n        templates = self.response_templates.get(intent, self.response_templates["unknown"])\n        template = templates[0] if templates else "I understand."\n\n        # Fill in template with entities\n        response_text = template\n        for entity_key, entity_value in entities.items():\n            placeholder = "{" + entity_key.replace("entity_", "") + "}"\n            response_text = response_text.replace(placeholder, str(entity_value))\n\n        # Special handling for status inquiries\n        if intent == "status_inquiry" and world_state:\n            if "visible_objects" in world_state:\n                objects = [obj.get("name", "unknown object") for obj in world_state["visible_objects"]]\n                response_text = response_text.replace("{objects_count}", str(len(objects)))\n                response_text = response_text.replace("{objects_list}", ", ".join(objects))\n\n        # Determine action based on intent\n        action = self._map_intent_to_action(intent, entities, world_state)\n\n        return {\n            "response_text": response_text,\n            "action": action,\n            "requires_clarification": self._needs_clarification(intent, entities, world_state)\n        }\n\n    def _map_intent_to_action(self, intent: str, entities: Dict, world_state: Dict) -> Dict:\n        """Map intent to specific robot action"""\n        if intent == "navigation":\n            return {\n                "type": "navigate",\n                "target_location": entities.get("entity_0", "unknown")\n            }\n        elif intent == "object_manipulation":\n            action = entities.get("entity_0", "unknown")\n            target_object = entities.get("entity_1", "unknown")\n            return {\n                "type": "manipulate",\n                "action": action,\n                "target_object": target_object\n            }\n        elif intent == "social_interaction":\n            return {\n                "type": "social",\n                "action": "greet"\n            }\n        elif intent == "information_request":\n            return {\n                "type": "report",\n                "action": "capabilities"\n            }\n        elif intent == "status_inquiry":\n            return {\n                "type": "report",\n                "action": "environment_status"\n            }\n        else:\n            return {\n                "type": "unknown",\n                "action": "awaiting_clarification"\n            }\n\n    def _needs_clarification(self, intent: str, entities: Dict, world_state: Dict) -> bool:\n        """Determine if the command needs clarification"""\n        if intent == "unknown":\n            return True\n\n        # Check if required entities are missing\n        if intent == "object_manipulation" and not entities:\n            return True\n\n        # Check if references are ambiguous\n        if world_state and "visible_objects" in world_state:\n            visible_objects = world_state["visible_objects"]\n            for entity_key, entity_value in entities.items():\n                if entity_value.lower() in ["it", "that", "this"] and len(visible_objects) > 1:\n                    return True\n\n        return False\n'})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-vision-and-action-systems",children:"Integration with Vision and Action Systems"}),"\n",(0,o.jsx)(e.h3,{id:"vision-language-integration",children:"Vision-Language Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass VisualObject:\n    """Represents a detected object with visual and linguistic properties"""\n    id: str\n    name: str\n    bbox: Tuple[int, int, int, int]  # (x, y, width, height)\n    confidence: float\n    position_3d: Optional[Tuple[float, float, float]] = None\n    properties: Dict[str, Any] = None\n\nclass VisionLanguageIntegrator:\n    def __init__(self, vision_system, language_system):\n        """\n        Integrate vision and language systems\n\n        Args:\n            vision_system: Interface to computer vision system\n            language_system: Interface to language understanding system\n        """\n        self.vision_system = vision_system\n        self.language_system = language_system\n        self.object_grounding = ObjectGroundingSystem()\n\n    def process_vision_language_task(self, command_text: str) -> Dict:\n        """\n        Process a task that requires both vision and language understanding\n\n        Args:\n            command_text: Natural language command\n\n        Returns:\n            Dictionary with processed results\n        """\n        # Parse the language command\n        parsed_command = self.language_system.resolve_command(\n            command_text,\n            world_state=self.get_current_world_state()\n        )\n\n        if not parsed_command:\n            return {"success": False, "error": "Could not parse command"}\n\n        # Use vision to identify relevant objects\n        vision_results = self.vision_system.get_current_scene_description()\n\n        # Ground linguistic references to visual objects\n        grounded_command = self.object_grounding.ground_command_to_objects(\n            parsed_command, vision_results\n        )\n\n        # Plan the action based on grounded command\n        action_plan = self._create_action_plan(grounded_command, vision_results)\n\n        return {\n            "success": True,\n            "parsed_command": parsed_command,\n            "vision_results": vision_results,\n            "grounded_command": grounded_command,\n            "action_plan": action_plan\n        }\n\n    def get_current_world_state(self) -> Dict:\n        """Get current world state including visible objects and locations"""\n        # Get results from vision system\n        vision_results = self.vision_system.get_current_scene_description()\n\n        world_state = {\n            "timestamp": time.time(),\n            "visible_objects": [],\n            "robot_location": self.vision_system.get_robot_position(),\n            "traversable_areas": self.vision_system.get_traversable_map()\n        }\n\n        # Convert vision results to standardized format\n        for obj in vision_results.get("objects", []):\n            visual_obj = VisualObject(\n                id=obj.get("id", f"obj_{len(world_state[\'visible_objects\'])}"),\n                name=obj.get("class", "unknown"),\n                bbox=obj.get("bbox", (0, 0, 0, 0)),\n                confidence=obj.get("confidence", 0.0),\n                position_3d=obj.get("position_3d"),\n                properties=obj.get("properties", {})\n            )\n            world_state["visible_objects"].append(visual_obj)\n\n        return world_state\n\n    def _create_action_plan(self, grounded_command: RobotCommand, vision_results: Dict) -> List[Dict]:\n        """Create an action plan based on grounded command and vision results"""\n        action_plan = []\n\n        if grounded_command.action == Action.GRASP.value:\n            # Find the target object\n            target_obj = self._find_object_by_name(\n                grounded_command.target_object,\n                vision_results.get("objects", [])\n            )\n\n            if target_obj:\n                action_plan.extend([\n                    {\n                        "action": "navigate",\n                        "target": target_obj["position_3d"],\n                        "description": f"Navigate to {target_obj[\'name\']}"\n                    },\n                    {\n                        "action": "grasp",\n                        "target": target_obj,\n                        "description": f"Grasp {target_obj[\'name\']}"\n                    }\n                ])\n            else:\n                action_plan.append({\n                    "action": "report",\n                    "target": f"Could not find {grounded_command.target_object}",\n                    "description": "Report failure to find object"\n                })\n\n        elif grounded_command.action == Action.MOVE.value:\n            # Navigate to target location\n            action_plan.append({\n                "action": "navigate",\n                "target": grounded_command.target_location,\n                "description": f"Navigate to {grounded_command.target_location}"\n            })\n\n        elif grounded_command.action == Action.PLACE.value:\n            # This would involve both object and location\n            target_obj = self._find_object_by_name(\n                grounded_command.target_object,\n                vision_results.get("objects", [])\n            )\n            target_location = self._find_location_by_name(\n                grounded_command.target_location,\n                vision_results.get("locations", [])\n            )\n\n            if target_obj and target_location:\n                action_plan.extend([\n                    {\n                        "action": "grasp",\n                        "target": target_obj,\n                        "description": f"Grasp {target_obj[\'name\']}"\n                    },\n                    {\n                        "action": "navigate",\n                        "target": target_location["position"],\n                        "description": f"Navigate to {target_location[\'name\']}"\n                    },\n                    {\n                        "action": "place",\n                        "target": target_location,\n                        "description": f"Place object at {target_location[\'name\']}"\n                    }\n                ])\n\n        return action_plan\n\n    def _find_object_by_name(self, name: str, objects: List[Dict]) -> Optional[Dict]:\n        """Find an object by name from a list of detected objects"""\n        for obj in objects:\n            if obj.get("name", "").lower() == name.lower():\n                return obj\n            # Also check class names and aliases\n            if obj.get("class", "").lower() == name.lower():\n                return obj\n            if obj.get("id", "").lower() == name.lower():\n                return obj\n\n        # If exact match not found, try partial matching\n        for obj in objects:\n            if name.lower() in obj.get("name", "").lower():\n                return obj\n            if name.lower() in obj.get("class", "").lower():\n                return obj\n\n        return None\n\n    def _find_location_by_name(self, name: str, locations: List[Dict]) -> Optional[Dict]:\n        """Find a location by name from a list of known locations"""\n        for loc in locations:\n            if loc.get("name", "").lower() == name.lower():\n                return loc\n\n        return None\n\nclass ObjectGroundingSystem:\n    def __init__(self):\n        """Initialize system for grounding linguistic references to visual objects"""\n        self.similarity_threshold = 0.7\n\n    def ground_command_to_objects(self, command: RobotCommand, vision_results: Dict) -> RobotCommand:\n        """\n        Ground linguistic object references to actual visual objects\n\n        Args:\n            command: Parsed command with potentially ambiguous object references\n            vision_results: Current visual scene description\n\n        Returns:\n            Command with grounded object references\n        """\n        grounded_command = RobotCommand(\n            action=command.action,\n            target_object=command.target_object,\n            target_location=command.target_location,\n            attributes=command.attributes,\n            confidence=command.confidence\n        )\n\n        # Ground target object if specified\n        if command.target_object:\n            grounded_obj = self._ground_object_reference(\n                command.target_object,\n                vision_results.get("objects", [])\n            )\n            if grounded_obj:\n                grounded_command.target_object = grounded_obj\n\n        # Ground target location if specified\n        if command.target_location:\n            grounded_loc = self._ground_location_reference(\n                command.target_location,\n                vision_results.get("locations", [])\n            )\n            if grounded_loc:\n                grounded_command.target_location = grounded_loc\n\n        return grounded_command\n\n    def _ground_object_reference(self, reference: str, objects: List[Dict]) -> Optional[str]:\n        """Ground a linguistic object reference to a visual object"""\n        # Try exact name matching first\n        for obj in objects:\n            if obj.get("name", "").lower() == reference.lower():\n                return obj.get("name")\n\n        # Try class matching\n        for obj in objects:\n            if obj.get("class", "").lower() == reference.lower():\n                return obj.get("name")\n\n        # Try attribute-based matching\n        for obj in objects:\n            obj_attributes = obj.get("attributes", {})\n            if self._match_attributes(reference, obj_attributes):\n                return obj.get("name")\n\n        # If no exact match, try similarity-based matching\n        best_match = self._find_best_similarity_match(reference, objects)\n        if best_match and best_match[1] > self.similarity_threshold:\n            return best_match[0]\n\n        return None\n\n    def _match_attributes(self, reference: str, obj_attributes: Dict) -> bool:\n        """Check if object attributes match the reference"""\n        # This would implement more sophisticated attribute matching\n        # For now, a simple approach\n        reference_lower = reference.lower()\n\n        for attr_key, attr_value in obj_attributes.items():\n            if reference_lower in str(attr_value).lower():\n                return True\n            if reference_lower in attr_key.lower():\n                return True\n\n        return False\n\n    def _find_best_similarity_match(self, reference: str, objects: List[Dict]) -> Optional[Tuple[str, float]]:\n        """Find the object with the best similarity to the reference"""\n        best_match = (None, 0.0)\n\n        for obj in objects:\n            name = obj.get("name", "")\n            obj_class = obj.get("class", "")\n\n            # Calculate similarity scores\n            name_similarity = self._calculate_string_similarity(reference, name)\n            class_similarity = self._calculate_string_similarity(reference, obj_class)\n\n            max_similarity = max(name_similarity, class_similarity)\n\n            if max_similarity > best_match[1]:\n                best_match = (obj.get("name", obj.get("id", "unknown")), max_similarity)\n\n        return best_match if best_match[1] > 0 else None\n\n    def _calculate_string_similarity(self, str1: str, str2: str) -> float:\n        """Calculate similarity between two strings (simplified)"""\n        if not str1 or not str2:\n            return 0.0\n\n        # Convert to lowercase for comparison\n        str1, str2 = str1.lower(), str2.lower()\n\n        # Simple token-based similarity\n        tokens1 = set(str1.split())\n        tokens2 = set(str2.split())\n\n        if not tokens1 and not tokens2:\n            return 1.0\n        if not tokens1 or not tokens2:\n            return 0.0\n\n        # Calculate Jaccard similarity\n        intersection = tokens1.intersection(tokens2)\n        union = tokens1.union(tokens2)\n        jaccard = len(intersection) / len(union)\n\n        return jaccard\n\n    def _ground_location_reference(self, reference: str, locations: List[Dict]) -> Optional[str]:\n        """Ground a linguistic location reference to a visual location"""\n        for loc in locations:\n            if loc.get("name", "").lower() == reference.lower():\n                return loc.get("name")\n\n        # Try partial matching\n        for loc in locations:\n            if reference.lower() in loc.get("name", "").lower():\n                return loc.get("name")\n\n        return None\n'})}),"\n",(0,o.jsx)(e.h2,{id:"speech-processing-and-natural-interaction",children:"Speech Processing and Natural Interaction"}),"\n",(0,o.jsx)(e.h3,{id:"speech-to-text-integration",children:"Speech-to-Text Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import speech_recognition as sr\nimport pyaudio\nfrom typing import Optional, Callable\nimport threading\n\nclass SpeechInterface:\n    def __init__(self, language: str = "en-US"):\n        """\n        Initialize speech interface for humanoid robot\n\n        Args:\n            language: Language code for speech recognition\n        """\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.language = language\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        # Callbacks\n        self.command_callback: Optional[Callable] = None\n        self.listening_callback: Optional[Callable] = None\n\n    def listen_once(self) -> Optional[str]:\n        """\n        Listen for a single command and return the recognized text\n\n        Returns:\n            Recognized text or None if recognition failed\n        """\n        try:\n            with self.microphone as source:\n                if self.listening_callback:\n                    self.listening_callback(True)\n\n                print("Listening...")\n                audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=10)\n\n            # Use Google\'s speech recognition\n            text = self.recognizer.recognize_google(audio, language=self.language)\n            print(f"Recognized: {text}")\n            return text\n\n        except sr.WaitTimeoutError:\n            print("Timeout: No speech detected")\n            return None\n        except sr.UnknownValueError:\n            print("Could not understand audio")\n            return None\n        except sr.RequestError as e:\n            print(f"Could not request results; {e}")\n            return None\n        finally:\n            if self.listening_callback:\n                self.listening_callback(False)\n\n    def start_continuous_listening(self):\n        """Start continuous listening in a separate thread"""\n        self.listening_thread = threading.Thread(target=self._continuous_listening_loop)\n        self.listening_thread.daemon = True\n        self.listening_thread.start()\n\n    def _continuous_listening_loop(self):\n        """Main loop for continuous speech recognition"""\n        with self.microphone as source:\n            while True:\n                try:\n                    print("Waiting for command...")\n                    if self.listening_callback:\n                        self.listening_callback(True)\n\n                    # Listen for audio\n                    audio = self.recognizer.listen(source, timeout=10, phrase_time_limit=10)\n\n                    # Recognize speech\n                    text = self.recognizer.recognize_google(audio, language=self.language)\n                    print(f"Recognized: {text}")\n\n                    # Call command callback if available\n                    if self.command_callback:\n                        self.command_callback(text)\n\n                except sr.WaitTimeoutError:\n                    # Continue waiting for speech\n                    continue\n                except sr.UnknownValueError:\n                    # Could not understand, continue listening\n                    continue\n                except sr.RequestError as e:\n                    print(f"Recognition error: {e}")\n                    continue\n                finally:\n                    if self.listening_callback:\n                        self.listening_callback(False)\n\n    def set_command_callback(self, callback: Callable[[str], None]):\n        """Set callback for when a command is recognized"""\n        self.command_callback = callback\n\n    def set_listening_callback(self, callback: Callable[[bool], None]):\n        """Set callback for when listening state changes"""\n        self.listening_callback = callback\n\nclass TextToSpeech:\n    def __init__(self):\n        """Initialize text-to-speech system"""\n        try:\n            from gtts import gTTS\n            import pygame\n            self.gTTS = gTTS\n            self.pygame = pygame\n            self.pygame.mixer.init()\n            self.use_gtts = True\n        except ImportError:\n            # Fallback to espeak if gtts not available\n            self.use_gtts = False\n            print("gTTS not available, using fallback TTS")\n\n    def speak(self, text: str, language: str = "en"):\n        """\n        Convert text to speech and play it\n\n        Args:\n            text: Text to convert to speech\n            language: Language code\n        """\n        if self.use_gtts:\n            try:\n                # Create speech\n                tts = self.gTTS(text=text, lang=language, slow=False)\n\n                # Save to temporary file and play\n                import io\n                fp = io.BytesIO()\n                tts.write_to_fp(fp)\n                fp.seek(0)\n\n                # Play the audio\n                self.pygame.mixer.music.load(fp)\n                self.pygame.mixer.music.play()\n\n                # Wait for playback to finish\n                while self.pygame.mixer.music.get_busy():\n                    continue\n\n            except Exception as e:\n                print(f"TTS error: {e}")\n        else:\n            # Fallback: print text to console\n            print(f"[TTS]: {text}")\n\nclass HumanoidInteractionManager:\n    def __init__(self, vision_system, language_system, action_system):\n        """\n        Manage the complete interaction pipeline for humanoid robot\n\n        Args:\n            vision_system: Vision processing system\n            language_system: Language understanding system\n            action_system: Action execution system\n        """\n        self.vision_system = vision_system\n        self.language_system = language_system\n        self.action_system = action_system\n\n        # Initialize speech interface\n        self.speech_interface = SpeechInterface()\n        self.text_to_speech = TextToSpeech()\n\n        # Initialize dialogue manager\n        self.dialogue_manager = DialogueManager()\n\n        # Initialize vision-language integrator\n        self.vision_language_integrator = VisionLanguageIntegrator(\n            vision_system, language_system\n        )\n\n    def setup_interaction_callbacks(self):\n        """Set up callbacks for continuous interaction"""\n        def command_callback(text: str):\n            self.process_spoken_command(text)\n\n        def listening_callback(listening: bool):\n            # Update robot\'s listening state for visual feedback\n            self.action_system.set_listening_state(listening)\n\n        self.speech_interface.set_command_callback(command_callback)\n        self.speech_interface.set_listening_callback(listening_callback)\n\n    def process_spoken_command(self, command_text: str):\n        """\n        Process a command received through speech\n\n        Args:\n            command_text: Natural language command from speech recognition\n        """\n        try:\n            # Get current world state\n            world_state = self.vision_language_integrator.get_current_world_state()\n\n            # Process through dialogue manager\n            response = self.dialogue_manager.process_input(command_text, world_state)\n\n            # Generate verbal response\n            response_text = response.get("response_text", "I understand.")\n            self.text_to_speech.speak(response_text)\n\n            # Execute action if specified\n            action = response.get("action")\n            if action:\n                self._execute_action(action, world_state)\n\n        except Exception as e:\n            error_response = "I encountered an error processing your command."\n            self.text_to_speech.speak(error_response)\n            print(f"Error processing command: {e}")\n\n    def _execute_action(self, action: Dict, world_state: Dict):\n        """Execute the specified action"""\n        action_type = action.get("type")\n        target = action.get("target")\n\n        if action_type == "navigate":\n            self.action_system.navigate_to(target)\n        elif action_type == "manipulate":\n            self.action_system.manipulate_object(target)\n        elif action_type == "social":\n            self.action_system.perform_social_action(target)\n        elif action_type == "report":\n            info = self._generate_report(action.get("action"), world_state)\n            self.text_to_speech.speak(info)\n        else:\n            self.text_to_speech.speak("I\'m not sure how to perform that action.")\n\n    def _generate_report(self, report_type: str, world_state: Dict) -> str:\n        """Generate a verbal report based on world state"""\n        if report_type == "environment_status":\n            objects = world_state.get("visible_objects", [])\n            count = len(objects)\n            if count == 0:\n                return "I don\'t see any objects around me."\n            elif count == 1:\n                return f"I see one object: {objects[0].name}."\n            else:\n                obj_names = [obj.name for obj in objects[:3]]  # Limit to first 3\n                if count <= 3:\n                    return f"I see {count} objects: {\', \'.join(obj_names)}."\n                else:\n                    return f"I see {count} objects including: {\', \'.join(obj_names)}."\n        elif report_type == "capabilities":\n            return "I can navigate spaces, manipulate objects, recognize people, and engage in conversation."\n        else:\n            return "I can provide information about my environment and capabilities."\n\n    def start_interaction_loop(self):\n        """Start the complete interaction loop"""\n        print("Starting humanoid interaction system...")\n\n        # Set up callbacks\n        self.setup_interaction_callbacks()\n\n        # Start continuous listening\n        self.speech_interface.start_continuous_listening()\n\n        print("Interaction system ready. Listening for commands...")\n\n    def process_text_command(self, command_text: str) -> Dict:\n        """\n        Process a command provided as text (for testing/debugging)\n\n        Args:\n            command_text: Natural language command as text\n\n        Returns:\n            Dictionary with processing results\n        """\n        # Get current world state\n        world_state = self.vision_language_integrator.get_current_world_state()\n\n        # Process through dialogue manager\n        response = self.dialogue_manager.process_input(command_text, world_state)\n\n        # Generate verbal response\n        response_text = response.get("response_text", "I understand.")\n        self.text_to_speech.speak(response_text)\n\n        # Execute action if specified\n        action = response.get("action")\n        if action:\n            self._execute_action(action, world_state)\n\n        return response\n'})}),"\n",(0,o.jsx)(e.h2,{id:"constitution-alignment",children:"Constitution Alignment"}),"\n",(0,o.jsx)(e.p,{children:"This chapter addresses several constitutional requirements:"}),"\n",(0,o.jsx)(e.h3,{id:"vla-convergence-mandate-principle-i",children:"VLA Convergence Mandate (Principle I)"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Language understanding as the primary control interface for humanoid robots"}),"\n",(0,o.jsx)(e.li,{children:"Integration of language with vision and action systems"}),"\n",(0,o.jsx)(e.li,{children:"Natural language processing for human-like interaction"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"real-time-validation-principle-iv",children:"Real-Time Validation (Principle IV)"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Efficient processing for real-time interaction"}),"\n",(0,o.jsx)(e.li,{children:"Response time considerations for human-robot dialogue"}),"\n",(0,o.jsx)(e.li,{children:"Performance optimization for embedded systems"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"anthropomorphic-focus-principle-ii",children:"Anthropomorphic Focus (Principle II)"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Natural language interaction patterns similar to human communication"}),"\n",(0,o.jsx)(e.li,{children:"Social interaction capabilities for human-centered environments"}),"\n",(0,o.jsx)(e.li,{children:"Context-aware understanding of human commands"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"target-hardware-optimization-constraint",children:"Target Hardware Optimization (Constraint)"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Efficient NLP algorithms suitable for Jetson Orin deployment"}),"\n",(0,o.jsx)(e.li,{children:"Optimized processing pipelines for embedded systems"}),"\n",(0,o.jsx)(e.li,{children:"Memory-efficient language models where applicable"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,o.jsx)(e.h3,{id:"example-1-humanoid-assistant-dialogue-system",children:"Example 1: Humanoid Assistant Dialogue System"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class HumanoidAssistant:\n    def __init__(self):\n        """Initialize a humanoid assistant with complete VLA capabilities"""\n        # Initialize systems (these would be properly connected in real implementation)\n        self.vision_system = MockVisionSystem()  # Placeholder\n        self.language_system = ContextualCommandResolver(None, None)\n        self.action_system = MockActionSystem()  # Placeholder\n\n        # Initialize interaction manager\n        self.interaction_manager = HumanoidInteractionManager(\n            self.vision_system,\n            self.language_system,\n            self.action_system\n        )\n\n    def demonstrate_interaction(self):\n        """Demonstrate various interaction scenarios"""\n\n        # Scenario 1: Simple navigation\n        print("=== Scenario 1: Navigation ===")\n        response = self.interaction_manager.process_text_command("Please go to the kitchen")\n        print(f"Response: {response}")\n\n        # Scenario 2: Object manipulation\n        print("\\n=== Scenario 2: Object Manipulation ===")\n        response = self.interaction_manager.process_text_command("Pick up the red cup and put it on the table")\n        print(f"Response: {response}")\n\n        # Scenario 3: Context-dependent reference\n        print("\\n=== Scenario 3: Context-Dependent Reference ===")\n        response1 = self.interaction_manager.process_text_command("Find the book")\n        print(f"Response 1: {response1}")\n        response2 = self.interaction_manager.process_text_command("Now put it on the shelf")\n        print(f"Response 2: {response2}")\n\n        # Scenario 4: Social interaction\n        print("\\n=== Scenario 4: Social Interaction ===")\n        response = self.interaction_manager.process_text_command("Hello, how are you?")\n        print(f"Response: {response}")\n\nclass MockVisionSystem:\n    """Mock vision system for demonstration purposes"""\n    def get_current_scene_description(self):\n        return {\n            "objects": [\n                {"id": "obj1", "name": "red cup", "class": "cup", "bbox": [100, 100, 50, 50]},\n                {"id": "obj2", "name": "book", "class": "book", "bbox": [200, 150, 80, 100]},\n                {"id": "obj3", "name": "table", "class": "table", "bbox": [0, 300, 400, 200]}\n            ],\n            "locations": [\n                {"name": "kitchen", "position": [2.0, 1.0, 0.0]},\n                {"name": "living room", "position": [0.0, 0.0, 0.0]},\n                {"name": "shelf", "position": [1.5, 0.5, 0.0]}\n            ]\n        }\n\n    def get_robot_position(self):\n        return [0.0, 0.0, 0.0]\n\n    def get_traversable_map(self):\n        return np.ones((10, 10))\n\nclass MockActionSystem:\n    """Mock action system for demonstration purposes"""\n    def navigate_to(self, target):\n        print(f"Navigating to: {target}")\n\n    def manipulate_object(self, target):\n        print(f"Manipulating: {target}")\n\n    def perform_social_action(self, target):\n        print(f"Performing social action: {target}")\n\n    def set_listening_state(self, listening):\n        print(f"Listening state: {listening}")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"example-2-multi-turn-dialogue-management",children:"Example 2: Multi-Turn Dialogue Management"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class MultiTurnDialogueManager:\n    def __init__(self):\n        self.context = {\n            "current_task": None,\n            "task_objects": [],\n            "task_locations": [],\n            "conversation_history": [],\n            "user_preferences": {}\n        }\n        self.language_understanding = IntentRecognizer()\n        self.response_generator = ResponseGenerator()\n\n    def process_multi_turn_dialogue(self, user_input: str, current_world_state: Dict) -> Dict:\n        """\n        Process multi-turn dialogue with context maintenance\n        """\n        # Update conversation history\n        self.context["conversation_history"].append({\n            "speaker": "user",\n            "text": user_input,\n            "timestamp": time.time()\n        })\n\n        # Process the input\n        intent_result = self.language_understanding.process_command_with_context(\n            user_input, self.context\n        )\n\n        # Handle task-specific contexts\n        task_response = self._handle_task_context(intent_result, current_world_state)\n\n        # Generate natural response\n        response = self.response_generator.generate_response(\n            intent_result, current_world_state, self.context\n        )\n\n        # Update context\n        self.context["conversation_history"].append({\n            "speaker": "robot",\n            "text": response["response_text"],\n            "timestamp": time.time(),\n            "intent": intent_result["intent"]\n        })\n\n        return {\n            "response": response["response_text"],\n            "intent": intent_result["intent"],\n            "task_continuation": task_response,\n            "context_updated": True\n        }\n\n    def _handle_task_context(self, intent_result: Dict, world_state: Dict) -> Optional[Dict]:\n        """Handle context for ongoing tasks"""\n        current_task = self.context.get("current_task")\n\n        if current_task:\n            # If we\'re in the middle of a task, check if the user input is related\n            if intent_result["intent"] == "confirmation":\n                return self._handle_confirmation(current_task, intent_result)\n            elif intent_result["intent"] == "correction":\n                return self._handle_correction(current_task, intent_result)\n            elif intent_result["intent"] == "abandon_task":\n                self.context["current_task"] = None\n                return {"action": "task_abandoned", "message": "Task abandoned as requested."}\n\n        # If this is a new task, set it as current\n        if intent_result["intent"] in ["object_manipulation", "navigation"]:\n            self.context["current_task"] = {\n                "intent": intent_result["intent"],\n                "entities": intent_result["entities"],\n                "start_time": time.time()\n            }\n\n        return None\n\n    def _handle_confirmation(self, current_task: Dict, intent_result: Dict) -> Dict:\n        """Handle user confirmation of robot\'s interpretation"""\n        if current_task:\n            return {\n                "action": "task_confirmed",\n                "message": f"Proceeding with {current_task[\'intent\']} task.",\n                "task": current_task\n            }\n        return {"action": "no_task_to_confirm", "message": "No current task to confirm."}\n\n    def _handle_correction(self, current_task: Dict, intent_result: Dict) -> Dict:\n        """Handle user corrections to robot\'s interpretation"""\n        # Extract correction information\n        correction_entities = intent_result.get("entities", {})\n\n        # Update current task with corrections\n        if "target_object" in correction_entities:\n            current_task["entities"]["target_object"] = correction_entities["target_object"]\n\n        if "target_location" in correction_entities:\n            current_task["entities"]["target_location"] = correction_entities["target_location"]\n\n        return {\n            "action": "task_corrected",\n            "message": f"Task updated based on your correction.",\n            "task": current_task\n        }\n'})}),"\n",(0,o.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsx)(e.h3,{id:"exercise-1-language-understanding-pipeline",children:"Exercise 1: Language Understanding Pipeline"}),"\n",(0,o.jsx)(e.p,{children:"Implement a complete language understanding pipeline that:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Parses natural language commands into structured representations"}),"\n",(0,o.jsx)(e.li,{children:"Resolves ambiguous references using visual context"}),"\n",(0,o.jsx)(e.li,{children:"Integrates with vision and action systems"}),"\n",(0,o.jsx)(e.li,{children:"Handles multi-turn dialogue with context maintenance"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-2-context-aware-resolution",children:"Exercise 2: Context-Aware Resolution"}),"\n",(0,o.jsx)(e.p,{children:"Create a system that resolves linguistic references by:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Identifying pronouns and demonstratives in commands"}),"\n",(0,o.jsx)(e.li,{children:"Using visual input to ground ambiguous references"}),"\n",(0,o.jsx)(e.li,{children:"Maintaining conversation context across turns"}),"\n",(0,o.jsx)(e.li,{children:"Handling corrections and clarifications naturally"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-3-real-time-interaction",children:"Exercise 3: Real-Time Interaction"}),"\n",(0,o.jsx)(e.p,{children:"Develop a real-time interaction system that:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Processes speech input continuously"}),"\n",(0,o.jsx)(e.li,{children:"Responds to commands within 2-3 seconds"}),"\n",(0,o.jsx)(e.li,{children:"Provides appropriate verbal feedback"}),"\n",(0,o.jsx)(e.li,{children:"Integrates seamlessly with vision and action systems"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"Language understanding in robotics, particularly for humanoid robots, requires specialized approaches that go beyond traditional NLP. The Vision-Language-Action pipeline demands that language be grounded in the physical environment, with real-time processing capabilities and natural interaction patterns. For humanoid robots operating in human-centered environments, language systems must handle ambiguity, maintain context across conversations, and integrate seamlessly with perception and action systems. The primary control interface through natural language, as mandated by our constitution, requires sophisticated understanding of human communication patterns and the ability to translate high-level linguistic goals into concrete robotic behaviors."}),"\n",(0,o.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:'"Language and Robots" by Tellex and Roy'}),"\n",(0,o.jsx)(e.li,{children:'"Grounded Language Learning and Processing" by Gorniak'}),"\n",(0,o.jsx)(e.li,{children:'"Human-Robot Interaction: A Survey" by Goodrich and Schultz'}),"\n",(0,o.jsx)(e.li,{children:'"Natural Language Processing for Human-Robot Interaction" - Recent research papers'}),"\n",(0,o.jsx)(e.li,{children:'"Situated Language Understanding for Robotics" - ACL and ICRA proceedings'}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);