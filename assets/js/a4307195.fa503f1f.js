"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[479],{2060(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapters/module-3-simulation/chapter-11-unity-hri","title":"Chapter 11 - Unity for Human\u2013Robot Interaction","description":"Unity integration for human-robot interaction scenarios and simulation","source":"@site/docs/chapters/module-3-simulation/chapter-11-unity-hri.md","sourceDirName":"chapters/module-3-simulation","slug":"/chapters/module-3-simulation/chapter-11-unity-hri","permalink":"/docs/chapters/module-3-simulation/chapter-11-unity-hri","draft":false,"unlisted":false,"editUrl":"https://github.com/RIMZAASAD/Robotic-ai-Book/edit/main/website/docs/chapters/module-3-simulation/chapter-11-unity-hri.md","tags":[],"version":"current","frontMatter":{"title":"Chapter 11 - Unity for Human\u2013Robot Interaction","module":"Digital Twin Simulation","chapter":11,"description":"Unity integration for human-robot interaction scenarios and simulation","learningObjectives":["Set up Unity for human-robot interaction simulation","Create interactive environments for HRI research","Implement VLA pipeline integration with Unity"],"prerequisites":["chapter-10-physics-sensor-simulation"],"difficulty":"intermediate"},"sidebar":"textbookSidebar","previous":{"title":"Chapter 10: Physics & Sensor Simulation in Gazebo","permalink":"/docs/chapters/module-3-simulation/chapter-10-physics-sensor-simulation"},"next":{"title":"Chapter 12: NVIDIA Isaac Sim Fundamentals","permalink":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals"}}');var o=i(4848),r=i(8453);const a={title:"Chapter 11 - Unity for Human\u2013Robot Interaction",module:"Digital Twin Simulation",chapter:11,description:"Unity integration for human-robot interaction scenarios and simulation",learningObjectives:["Set up Unity for human-robot interaction simulation","Create interactive environments for HRI research","Implement VLA pipeline integration with Unity"],prerequisites:["chapter-10-physics-sensor-simulation"],difficulty:"intermediate"},s=void 0,l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Unity in the Robotics Ecosystem",id:"unity-in-the-robotics-ecosystem",level:2},{value:"Unity vs. Traditional Robotics Simulation",id:"unity-vs-traditional-robotics-simulation",level:3},{value:"Unity Robotics Hub",id:"unity-robotics-hub",level:3},{value:"Setting Up Unity for Robotics",id:"setting-up-unity-for-robotics",level:2},{value:"Prerequisites and Installation",id:"prerequisites-and-installation",level:3},{value:"Installing Unity Robotics Packages",id:"installing-unity-robotics-packages",level:3},{value:"Basic Project Structure",id:"basic-project-structure",level:3},{value:"ROS-TCP-Connector Integration",id:"ros-tcp-connector-integration",level:2},{value:"Setting up ROS Communication",id:"setting-up-ros-communication",level:3},{value:"Publishing and Subscribing to ROS Topics",id:"publishing-and-subscribing-to-ros-topics",level:3},{value:"Creating Human-Centered Environments",id:"creating-human-centered-environments",level:2},{value:"Environment Design Principles",id:"environment-design-principles",level:3},{value:"Sample Kitchen Environment",id:"sample-kitchen-environment",level:3},{value:"Lighting and Realism",id:"lighting-and-realism",level:3},{value:"Implementing Human-Robot Interaction Scenarios",id:"implementing-human-robot-interaction-scenarios",level:2},{value:"Basic HRI Framework",id:"basic-hri-framework",level:3},{value:"Gesture Recognition and Response",id:"gesture-recognition-and-response",level:3},{value:"Vision-Language-Action Pipeline Integration",id:"vision-language-action-pipeline-integration",level:2},{value:"Unity Camera Integration",id:"unity-camera-integration",level:3},{value:"Speech and Language Integration",id:"speech-and-language-integration",level:3},{value:"Performance Optimization for Real-time HRI",id:"performance-optimization-for-real-time-hri",level:2},{value:"Object Pooling for Dynamic Objects",id:"object-pooling-for-dynamic-objects",level:3},{value:"Level of Detail (LOD) for Complex Environments",id:"level-of-detail-lod-for-complex-environments",level:3},{value:"Constitution Alignment",id:"constitution-alignment",level:2},{value:"Anthropomorphic Focus (Principle II)",id:"anthropomorphic-focus-principle-ii",level:3},{value:"Sim-to-Real Rigor (Principle III)",id:"sim-to-real-rigor-principle-iii",level:3},{value:"Visualization Requirements (Key Standard II)",id:"visualization-requirements-key-standard-ii",level:3},{value:"VLA Convergence Mandate (Principle I)",id:"vla-convergence-mandate-principle-i",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Humanoid Assistant Scenario",id:"example-1-humanoid-assistant-scenario",level:3},{value:"Example 2: Collaborative Task Environment",id:"example-2-collaborative-task-environment",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: HRI Environment Creation",id:"exercise-1-hri-environment-creation",level:3},{value:"Exercise 2: ROS Integration",id:"exercise-2-ros-integration",level:3},{value:"Exercise 3: VLA Pipeline Integration",id:"exercise-3-vla-pipeline-integration",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Set up Unity for human-robot interaction simulation"}),"\n",(0,o.jsx)(e.li,{children:"Create interactive environments for HRI research"}),"\n",(0,o.jsx)(e.li,{children:"Implement VLA pipeline integration with Unity"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"Unity provides a powerful platform for creating realistic human-robot interaction (HRI) scenarios that complement traditional robotics simulation tools. While Gazebo excels at physics simulation, Unity offers superior visual fidelity and interactive capabilities for HRI research. This chapter explores the integration of Unity with ROS 2 for creating immersive human-robot interaction environments, supporting the Vision-Language-Action pipeline through realistic visual simulation and interactive scenarios. Unity's capabilities for creating human-centered environments align with our Anthropomorphic Focus principle, enabling the development of humanoid robots that can operate effectively in spaces designed for human use."}),"\n",(0,o.jsx)(e.h2,{id:"unity-in-the-robotics-ecosystem",children:"Unity in the Robotics Ecosystem"}),"\n",(0,o.jsx)(e.h3,{id:"unity-vs-traditional-robotics-simulation",children:"Unity vs. Traditional Robotics Simulation"}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Aspect"}),(0,o.jsx)(e.th,{children:"Gazebo"}),(0,o.jsx)(e.th,{children:"Unity"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Physics Simulation"}),(0,o.jsx)(e.td,{children:"High-fidelity, real-time"}),(0,o.jsx)(e.td,{children:"Good, with Unity Physics"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Visual Fidelity"}),(0,o.jsx)(e.td,{children:"Moderate"}),(0,o.jsx)(e.td,{children:"Very High"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Human-Centered Environments"}),(0,o.jsx)(e.td,{children:"Basic"}),(0,o.jsx)(e.td,{children:"Excellent"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Interactivity"}),(0,o.jsx)(e.td,{children:"Limited"}),(0,o.jsx)(e.td,{children:"Extensive"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Realism"}),(0,o.jsx)(e.td,{children:"Functional"}),(0,o.jsx)(e.td,{children:"Photorealistic"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"VR/AR Support"}),(0,o.jsx)(e.td,{children:"Limited"}),(0,o.jsx)(e.td,{children:"Excellent"})]})]})]}),"\n",(0,o.jsx)(e.h3,{id:"unity-robotics-hub",children:"Unity Robotics Hub"}),"\n",(0,o.jsx)(e.p,{children:"Unity provides the Robotics Hub package that facilitates integration with ROS 2:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ROS-TCP-Connector"}),": Communication bridge between Unity and ROS 2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"URDF-Importer"}),": Import URDF models directly into Unity"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robotics-Object-Pools"}),": Optimized object management for robotics simulations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Visualizations"}),": Tools for sensor data visualization"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"setting-up-unity-for-robotics",children:"Setting Up Unity for Robotics"}),"\n",(0,o.jsx)(e.h3,{id:"prerequisites-and-installation",children:"Prerequisites and Installation"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Unity Hub"}),": Download from unity3d.com"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Unity Editor"}),": Install version 2021.3 LTS or later"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ROS 2"}),": Ensure ROS 2 (Humble Hawksbill) is installed"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Visual Studio"}),": For C# scripting (Windows) or appropriate IDE"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"installing-unity-robotics-packages",children:"Installing Unity Robotics Packages"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Open Unity Hub and create a new 3D project"}),"\n",(0,o.jsxs)(e.li,{children:["In the Package Manager (Window > Package Manager):","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:'Install "ROS TCP Connector" from Package Manager'}),"\n",(0,o.jsx)(e.li,{children:'Install "URDF Importer" from Package Manager'}),"\n",(0,o.jsx)(e.li,{children:'Install "Cinemachine" for camera control'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"basic-project-structure",children:"Basic Project Structure"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"UnityHRIProject/\n\u251c\u2500\u2500 Assets/\n\u2502   \u251c\u2500\u2500 Scripts/           # C# scripts for robotics integration\n\u2502   \u251c\u2500\u2500 URDFs/            # Imported robot models\n\u2502   \u251c\u2500\u2500 Scenes/           # Unity scenes for different environments\n\u2502   \u251c\u2500\u2500 Materials/        # Visual materials for realistic rendering\n\u2502   \u251c\u2500\u2500 Prefabs/          # Reusable robot and environment objects\n\u2502   \u2514\u2500\u2500 Plugins/          # ROS communication libraries\n\u251c\u2500\u2500 Packages/\n\u2514\u2500\u2500 ProjectSettings/\n"})}),"\n",(0,o.jsx)(e.h2,{id:"ros-tcp-connector-integration",children:"ROS-TCP-Connector Integration"}),"\n",(0,o.jsx)(e.h3,{id:"setting-up-ros-communication",children:"Setting up ROS Communication"}),"\n",(0,o.jsx)(e.p,{children:"First, install the ROS-TCP-Connector in Unity:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// RobotCommunicator.cs - Basic ROS communication setup\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Std;\n\npublic class RobotCommunicator : MonoBehaviour\n{\n    ROSConnection ros;\n    public string rosIPAddress = "127.0.0.1";\n    public int rosPort = 10000;\n\n    void Start()\n    {\n        // Get the ROS connection static instance\n        ros = ROSConnection.instance;\n        ros.RegisteredGID += OnRegisteredGID;\n\n        // Set the IP address and port for the ROS connection\n        ros.Initialize(rosIPAddress, rosPort);\n\n        Debug.Log($"ROS Connection initialized to {rosIPAddress}:{rosPort}");\n    }\n\n    void OnRegisteredGID(ulong GID)\n    {\n        Debug.Log($"Connected to ROS with GID: {GID}");\n    }\n\n    void OnDestroy()\n    {\n        if (ros != null)\n        {\n            ros.RegisteredGID -= OnRegisteredGID;\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"publishing-and-subscribing-to-ros-topics",children:"Publishing and Subscribing to ROS Topics"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\nusing RosMessageTypes.Geometry;\n\npublic class HumanoidController : MonoBehaviour\n{\n    ROSConnection ros;\n\n    // Topics\n    public string jointStatesTopic = "/joint_states";\n    public string cameraTopic = "/camera/image_raw";\n\n    // Robot joint transforms\n    public Transform leftHip;\n    public Transform leftKnee;\n    public Transform leftAnkle;\n    public Transform rightHip;\n    public Transform rightKnee;\n    public Transform rightAnkle;\n\n    void Start()\n    {\n        ros = ROSConnection.instance;\n\n        // Subscribe to joint states\n        ros.Subscribe<JointStateMsg>(jointStatesTopic, JointStateCallback);\n    }\n\n    void JointStateCallback(JointStateMsg jointState)\n    {\n        // Update robot joint positions based on ROS messages\n        for (int i = 0; i < jointState.name.Count; i++)\n        {\n            string jointName = jointState.name[i];\n            double jointPosition = jointState.position[i];\n\n            Transform jointTransform = GetJointTransformByName(jointName);\n            if (jointTransform != null)\n            {\n                // Apply rotation based on joint position\n                jointTransform.localRotation = Quaternion.Euler(\n                    (float)jointPosition * Mathf.Rad2Deg, 0, 0);\n            }\n        }\n    }\n\n    Transform GetJointTransformByName(string name)\n    {\n        switch (name)\n        {\n            case "left_hip_joint": return leftHip;\n            case "left_knee_joint": return leftKnee;\n            case "left_ankle_joint": return leftAnkle;\n            case "right_hip_joint": return rightHip;\n            case "right_knee_joint": return rightKnee;\n            case "right_ankle_joint": return rightAnkle;\n            default: return null;\n        }\n    }\n\n    void Update()\n    {\n        // Send periodic status updates\n        SendRobotStatus();\n    }\n\n    void SendRobotStatus()\n    {\n        // Create and publish robot status message\n        // Implementation depends on specific requirements\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"creating-human-centered-environments",children:"Creating Human-Centered Environments"}),"\n",(0,o.jsx)(e.h3,{id:"environment-design-principles",children:"Environment Design Principles"}),"\n",(0,o.jsx)(e.p,{children:"For effective HRI simulation, environments should reflect human-centered design:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Proportional Accuracy"}),": Furniture and spaces at human scale"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation Paths"}),": Clear pathways for both humans and robots"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Interaction Points"}),": Counters, tables, and surfaces for object exchange"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Accessibility Features"}),": Doors, ramps, and controls usable by humanoid robots"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"sample-kitchen-environment",children:"Sample Kitchen Environment"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class KitchenEnvironment : MonoBehaviour\n{\n    [Header("Furniture References")]\n    public Transform counterTop;\n    public Transform fridge;\n    public Transform table;\n    public Transform cabinet;\n\n    [Header("Interaction Points")]\n    public Transform[] pickupPoints;\n    public Transform[] placePoints;\n    public Transform[] navigationWaypoints;\n\n    void Start()\n    {\n        SetupEnvironment();\n    }\n\n    void SetupEnvironment()\n    {\n        // Configure counter height for humanoid interaction\n        if (counterTop != null)\n        {\n            counterTop.position = new Vector3(0, 0.9f, 0); // Standard counter height\n        }\n\n        // Setup pickup and place points\n        SetupInteractionPoints();\n\n        // Create navigation mesh for pathfinding\n        CreateNavigationMesh();\n    }\n\n    void SetupInteractionPoints()\n    {\n        // Define points where robot can interact with objects\n        // These should be at appropriate heights for humanoid manipulation\n        for (int i = 0; i < pickupPoints.Length; i++)\n        {\n            // Ensure points are reachable by humanoid arm\n            if (pickupPoints[i].position.y < 0.6f || pickupPoints[i].position.y > 1.2f)\n            {\n                Debug.LogWarning($"Pickup point {i} may be unreachable for humanoid");\n            }\n        }\n    }\n\n    void CreateNavigationMesh()\n    {\n        // In practice, you\'d use Unity\'s NavMesh system\n        // This is a placeholder for navigation setup\n        Debug.Log("Navigation mesh setup completed");\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"lighting-and-realism",children:"Lighting and Realism"}),"\n",(0,o.jsx)(e.p,{children:"For photorealistic HRI scenarios:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.Rendering;\n\npublic class EnvironmentLighting : MonoBehaviour\n{\n    [Header("Lighting Setup")]\n    public Light mainLight;\n    public Light[] fillLights;\n    public bool useHDRP = false;\n\n    void Start()\n    {\n        ConfigureLighting();\n    }\n\n    void ConfigureLighting()\n    {\n        if (useHDRP)\n        {\n            ConfigureHDRPLighting();\n        }\n        else\n        {\n            ConfigureBuiltInLighting();\n        }\n    }\n\n    void ConfigureHDRPLighting()\n    {\n        // Configure High Definition Render Pipeline\n        RenderPipelineManager.beginCameraRendering += OnBeginCameraRendering;\n    }\n\n    void ConfigureBuiltInLighting()\n    {\n        // Configure built-in render pipeline\n        if (mainLight != null)\n        {\n            mainLight.type = LightType.Directional;\n            mainLight.intensity = 1.2f;\n            mainLight.color = Color.white;\n            mainLight.shadows = LightShadows.Soft;\n        }\n\n        // Add ambient lighting\n        RenderSettings.ambientLight = new Color(0.4f, 0.4f, 0.4f, 1);\n        RenderSettings.ambientMode = UnityEngine.Rendering.AmbientMode.Trilight;\n    }\n\n    void OnBeginCameraRendering(ScriptableRenderContext context, Camera camera)\n    {\n        // HDRP-specific rendering setup\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"implementing-human-robot-interaction-scenarios",children:"Implementing Human-Robot Interaction Scenarios"}),"\n",(0,o.jsx)(e.h3,{id:"basic-hri-framework",children:"Basic HRI Framework"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class HRIManager : MonoBehaviour\n{\n    [Header("Human Interaction")]\n    public GameObject humanCharacter;\n    public float interactionDistance = 2.0f;\n\n    [Header("Robot Configuration")]\n    public GameObject humanoidRobot;\n    public Transform[] interactionZones;\n\n    [Header("Communication")]\n    public string speechTopic = "/speech_recognition";\n    public string gestureTopic = "/gesture_detection";\n\n    private List<InteractionEvent> activeInteractions = new List<InteractionEvent>();\n\n    void Update()\n    {\n        CheckForInteractions();\n        UpdateInteractionStatus();\n    }\n\n    void CheckForInteractions()\n    {\n        if (humanCharacter == null || humanoidRobot == null) return;\n\n        float distance = Vector3.Distance(\n            humanCharacter.transform.position,\n            humanoidRobot.transform.position\n        );\n\n        if (distance <= interactionDistance)\n        {\n            // Trigger interaction logic\n            HandleProximityInteraction();\n        }\n    }\n\n    void HandleProximityInteraction()\n    {\n        // Implement interaction logic based on distance and context\n        Debug.Log("Human-Robot interaction detected!");\n\n        // Possible interactions:\n        // - Greeting sequence\n        // - Task assignment\n        // - Object handover\n        // - Navigation assistance\n    }\n\n    void UpdateInteractionStatus()\n    {\n        // Update UI elements, send ROS messages, etc.\n    }\n}\n\n[System.Serializable]\npublic class InteractionEvent\n{\n    public string eventType;\n    public float timestamp;\n    public Vector3 humanPosition;\n    public Vector3 robotPosition;\n    public string context;\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"gesture-recognition-and-response",children:"Gesture Recognition and Response"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class GestureRecognition : MonoBehaviour\n{\n    [Header("Gesture Configuration")]\n    public Transform humanHandLeft;\n    public Transform humanHandRight;\n    public float gestureThreshold = 0.1f;\n\n    [Header("Gesture Responses")]\n    public GameObject robotHead;\n    public float headTurnSpeed = 2.0f;\n\n    private Vector3 lastLeftHandPos;\n    private Vector3 lastRightHandPos;\n    private bool gestureDetected = false;\n\n    void Start()\n    {\n        if (humanHandLeft != null) lastLeftHandPos = humanHandLeft.position;\n        if (humanHandRight != null) lastRightHandPos = humanHandRight.position;\n    }\n\n    void Update()\n    {\n        DetectGestures();\n        RespondToGestures();\n    }\n\n    void DetectGestures()\n    {\n        if (humanHandLeft == null || humanHandRight == null) return;\n\n        Vector3 currentLeftPos = humanHandLeft.position;\n        Vector3 currentRightPos = humanHandRight.position;\n\n        // Calculate movement vectors\n        Vector3 leftMovement = currentLeftPos - lastLeftHandPos;\n        Vector3 rightMovement = currentRightPos - lastRightHandPos;\n\n        // Detect specific gestures\n        if (leftMovement.magnitude > gestureThreshold)\n        {\n            DetectLeftHandGesture(leftMovement);\n        }\n\n        if (rightMovement.magnitude > gestureThreshold)\n        {\n            DetectRightHandGesture(rightMovement);\n        }\n\n        // Update positions for next frame\n        lastLeftHandPos = currentLeftPos;\n        lastRightHandPos = currentRightPos;\n    }\n\n    void DetectLeftHandGesture(Vector3 movement)\n    {\n        // Example: Wave gesture (horizontal movement)\n        if (Mathf.Abs(movement.x) > Mathf.Abs(movement.y) &&\n            Mathf.Abs(movement.x) > Mathf.Abs(movement.z))\n        {\n            if (movement.x > 0)\n            {\n                Debug.Log("Left hand wave right detected");\n                RespondToWave();\n            }\n            else\n            {\n                Debug.Log("Left hand wave left detected");\n            }\n        }\n    }\n\n    void DetectRightHandGesture(Vector3 movement)\n    {\n        // Example: Point gesture (forward movement)\n        if (movement.z > gestureThreshold * 2)\n        {\n            Debug.Log("Pointing gesture detected");\n            LookAtPoint();\n        }\n    }\n\n    void RespondToWave()\n    {\n        // Robot responds to wave with head movement or speech\n        StartCoroutine(RobotWaveResponse());\n    }\n\n    void LookAtPoint()\n    {\n        // Robot looks in the direction pointed by human\n        if (robotHead != null)\n        {\n            Vector3 lookDirection = (humanHandRight.position - transform.position).normalized;\n            Quaternion targetRotation = Quaternion.LookRotation(lookDirection, Vector3.up);\n            robotHead.rotation = Quaternion.Slerp(\n                robotHead.rotation,\n                targetRotation,\n                headTurnSpeed * Time.deltaTime\n            );\n        }\n    }\n\n    System.Collections.IEnumerator RobotWaveResponse()\n    {\n        // Simple head nod response\n        if (robotHead != null)\n        {\n            Vector3 originalRotation = robotHead.localEulerAngles;\n            for (float t = 0; t < 1; t += Time.deltaTime * 2)\n            {\n                robotHead.localRotation = Quaternion.Euler(\n                    originalRotation.x + Mathf.Sin(t * Mathf.PI * 4) * 10,\n                    originalRotation.y,\n                    originalRotation.z\n                );\n                yield return null;\n            }\n            robotHead.localEulerAngles = originalRotation;\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"vision-language-action-pipeline-integration",children:"Vision-Language-Action Pipeline Integration"}),"\n",(0,o.jsx)(e.h3,{id:"unity-camera-integration",children:"Unity Camera Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\nusing System.Collections;\n\npublic class UnityCameraPublisher : MonoBehaviour\n{\n    ROSConnection ros;\n    public string cameraTopic = "/unity_camera/image_raw";\n    public Camera unityCamera;\n    public int imageWidth = 640;\n    int imageHeight = 480;\n    int publishRate = 30; // Hz\n\n    RenderTexture renderTexture;\n    Texture2D texture2D;\n\n    void Start()\n    {\n        ros = ROSConnection.instance;\n\n        // Create render texture for camera\n        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);\n        if (unityCamera != null)\n        {\n            unityCamera.targetTexture = renderTexture;\n        }\n\n        // Start coroutine to publish images at desired rate\n        StartCoroutine(PublishCameraImages());\n    }\n\n    IEnumerator PublishCameraImages()\n    {\n        float frameTime = 1.0f / publishRate;\n\n        while (true)\n        {\n            yield return new WaitForSeconds(frameTime);\n            PublishImage();\n        }\n    }\n\n    void PublishImage()\n    {\n        if (unityCamera == null) return;\n\n        // Create temporary render texture to read pixels\n        RenderTexture currentRT = RenderTexture.active;\n        RenderTexture.active = renderTexture;\n\n        if (texture2D == null)\n        {\n            texture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n        }\n\n        texture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        texture2D.Apply();\n\n        // Convert to ROS image format\n        ImageMsg rosImage = CreateROSImage(texture2D);\n\n        // Publish to ROS\n        ros.Publish(cameraTopic, rosImage);\n\n        RenderTexture.active = currentRT;\n    }\n\n    ImageMsg CreateROSImage(Texture2D texture)\n    {\n        // Convert Unity texture to ROS Image message\n        ImageMsg img = new ImageMsg();\n        img.header = new std_msgs.Header();\n        img.header.stamp = new builtin_interfaces.Time();\n        img.header.frame_id = "unity_camera_optical_frame";\n\n        img.height = (uint)texture.height;\n        img.width = (uint)texture.width;\n        img.encoding = "rgb8";\n        img.is_bigendian = false;\n        img.step = (uint)(texture.width * 3); // 3 bytes per pixel for RGB\n\n        // Convert texture to byte array\n        Color32[] colors = texture.GetPixels32();\n        byte[] imageData = new byte[colors.Length * 3];\n\n        for (int i = 0; i < colors.Length; i++)\n        {\n            imageData[i * 3] = colors[i].r;\n            imageData[i * 3 + 1] = colors[i].g;\n            imageData[i * 3 + 2] = colors[i].b;\n        }\n\n        img.data = imageData;\n        return img;\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"speech-and-language-integration",children:"Speech and Language Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Std;\nusing RosMessageTypes.Diagnostic;\n\npublic class SpeechIntegration : MonoBehaviour\n{\n    ROSConnection ros;\n    public string speechCommandTopic = "/speech_commands";\n    public string textToSpeechTopic = "/tts_input";\n\n    [Header("Speech Configuration")]\n    public float confidenceThreshold = 0.7f;\n    public string[] validCommands = {\n        "hello", "stop", "go", "pick up", "put down", "follow me"\n    };\n\n    void Start()\n    {\n        ros = ROSConnection.instance;\n\n        // Subscribe to speech recognition results\n        ros.Subscribe<StringMsg>("/speech_recognition", SpeechCallback);\n    }\n\n    void SpeechCallback(StringMsg speechMsg)\n    {\n        string recognizedText = speechMsg.data;\n        Debug.Log($"Speech recognized: {recognizedText}");\n\n        ProcessSpeechCommand(recognizedText);\n    }\n\n    void ProcessSpeechCommand(string command)\n    {\n        // Check if command is in our valid command list\n        foreach (string validCmd in validCommands)\n        {\n            if (command.ToLower().Contains(validCmd.ToLower()))\n            {\n                ExecuteCommand(validCmd);\n                return;\n            }\n        }\n\n        // If no valid command found, publish to TTS\n        StringMsg ttsMsg = new StringMsg();\n        ttsMsg.data = $"I don\'t understand the command: {command}";\n        ros.Publish(textToSpeechTopic, ttsMsg);\n    }\n\n    void ExecuteCommand(string command)\n    {\n        Debug.Log($"Executing command: {command}");\n\n        // Publish command-specific messages to ROS\n        StringMsg cmdMsg = new StringMsg();\n        cmdMsg.data = command;\n        ros.Publish(speechCommandTopic, cmdMsg);\n\n        // Provide feedback\n        StringMsg feedbackMsg = new StringMsg();\n        feedbackMsg.data = $"Okay, I will {command}";\n        ros.Publish(textToSpeechTopic, feedbackMsg);\n    }\n\n    // Method to simulate speech input (for testing)\n    public void SimulateSpeechInput(string text)\n    {\n        StringMsg speechMsg = new StringMsg();\n        speechMsg.data = text;\n        SpeechCallback(speechMsg);\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"performance-optimization-for-real-time-hri",children:"Performance Optimization for Real-time HRI"}),"\n",(0,o.jsx)(e.h3,{id:"object-pooling-for-dynamic-objects",children:"Object Pooling for Dynamic Objects"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class ObjectPooler : MonoBehaviour\n{\n    [System.Serializable]\n    public class Pool\n    {\n        public string tag;\n        public GameObject prefab;\n        public int size;\n    }\n\n    public List<Pool> pools;\n    public Dictionary<string, Queue<GameObject>> poolDictionary;\n\n    void Start()\n    {\n        poolDictionary = new Dictionary<string, Queue<GameObject>>();\n\n        foreach (Pool pool in pools)\n        {\n            Queue<GameObject> objectPool = new Queue<GameObject>();\n\n            for (int i = 0; i < pool.size; i++)\n            {\n                GameObject obj = Instantiate(pool.prefab);\n                obj.SetActive(false);\n                objectPool.Enqueue(obj);\n            }\n\n            poolDictionary.Add(pool.tag, objectPool);\n        }\n    }\n\n    public GameObject SpawnFromPool(string tag, Vector3 position, Quaternion rotation)\n    {\n        if (!poolDictionary.ContainsKey(tag))\n        {\n            Debug.LogWarning($"Pool with tag {tag} doesn\'t exist!");\n            return null;\n        }\n\n        GameObject objectToSpawn = poolDictionary[tag].Dequeue();\n        objectToSpawn.SetActive(true);\n        objectToSpawn.transform.position = position;\n        objectToSpawn.transform.rotation = rotation;\n\n        // Add object back to pool when deactivated\n        PoolObject poolObj = objectToSpawn.GetComponent<PoolObject>();\n        if (poolObj == null)\n        {\n            poolObj = objectToSpawn.AddComponent<PoolObject>();\n        }\n        poolObj.pool = this;\n        poolObj.tag = tag;\n\n        poolDictionary[tag].Enqueue(objectToSpawn);\n        return objectToSpawn;\n    }\n}\n\npublic class PoolObject : MonoBehaviour\n{\n    public ObjectPooler pool;\n    public string tag;\n\n    void OnDisable()\n    {\n        if (pool != null)\n        {\n            pool.SpawnFromPool(tag, transform.position, transform.rotation);\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"level-of-detail-lod-for-complex-environments",children:"Level of Detail (LOD) for Complex Environments"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\n[RequireComponent(typeof(MeshRenderer))]\npublic class HRILODSystem : MonoBehaviour\n{\n    [Header("LOD Configuration")]\n    public float[] lodDistances = { 10f, 30f, 50f };\n    public Mesh[] lodMeshes;\n\n    [Header("Performance Settings")]\n    public bool enableLOD = true;\n    public float updateInterval = 0.1f;\n\n    private MeshRenderer meshRenderer;\n    private Transform cameraTransform;\n    private float lastUpdateTime;\n    private int currentLOD = 0;\n\n    void Start()\n    {\n        meshRenderer = GetComponent<MeshRenderer>();\n        cameraTransform = Camera.main.transform;\n\n        if (lodMeshes.Length == 0)\n        {\n            // If no LOD meshes provided, disable LOD\n            enableLOD = false;\n            return;\n        }\n    }\n\n    void Update()\n    {\n        if (!enableLOD || cameraTransform == null) return;\n\n        if (Time.time - lastUpdateTime > updateInterval)\n        {\n            UpdateLOD();\n            lastUpdateTime = Time.time;\n        }\n    }\n\n    void UpdateLOD()\n    {\n        if (lodMeshes.Length == 0) return;\n\n        float distance = Vector3.Distance(transform.position, cameraTransform.position);\n\n        int newLOD = 0;\n        for (int i = 0; i < lodDistances.Length; i++)\n        {\n            if (distance > lodDistances[i])\n            {\n                newLOD = i + 1;\n            }\n            else\n            {\n                break;\n            }\n        }\n\n        // Clamp to available LOD levels\n        newLOD = Mathf.Min(newLOD, lodMeshes.Length - 1);\n\n        if (newLOD != currentLOD)\n        {\n            UpdateMesh(newLOD);\n            currentLOD = newLOD;\n        }\n    }\n\n    void UpdateMesh(int lodLevel)\n    {\n        if (lodLevel < lodMeshes.Length)\n        {\n            MeshFilter meshFilter = GetComponent<MeshFilter>();\n            if (meshFilter != null)\n            {\n                meshFilter.mesh = lodMeshes[lodLevel];\n            }\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"constitution-alignment",children:"Constitution Alignment"}),"\n",(0,o.jsx)(e.p,{children:"This chapter addresses several constitutional requirements:"}),"\n",(0,o.jsx)(e.h3,{id:"anthropomorphic-focus-principle-ii",children:"Anthropomorphic Focus (Principle II)"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Human-centered environment design for humanoid robot interaction"}),"\n",(0,o.jsx)(e.li,{children:"Proportional accuracy for human-sized furniture and spaces"}),"\n",(0,o.jsx)(e.li,{children:"Interaction scenarios designed for human-robot collaboration"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"sim-to-real-rigor-principle-iii",children:"Sim-to-Real Rigor (Principle III)"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Realistic visual simulation for vision system training"}),"\n",(0,o.jsx)(e.li,{children:"Proper sensor simulation integration with Unity cameras"}),"\n",(0,o.jsx)(e.li,{children:"Environment complexity matching real-world scenarios"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"visualization-requirements-key-standard-ii",children:"Visualization Requirements (Key Standard II)"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"High-fidelity visual rendering for realistic HRI scenarios"}),"\n",(0,o.jsx)(e.li,{children:"Proper material and lighting setup for photorealistic environments"}),"\n",(0,o.jsx)(e.li,{children:"Clear examples with proper code formatting"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"vla-convergence-mandate-principle-i",children:"VLA Convergence Mandate (Principle I)"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Integration of vision, language, and action systems in Unity"}),"\n",(0,o.jsx)(e.li,{children:"Camera integration for vision processing"}),"\n",(0,o.jsx)(e.li,{children:"Speech and command processing for language interface"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,o.jsx)(e.h3,{id:"example-1-humanoid-assistant-scenario",children:"Example 1: Humanoid Assistant Scenario"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Actionlib;\n\npublic class HumanoidAssistant : MonoBehaviour\n{\n    ROSConnection ros;\n    public Transform[] servicePoints;\n    public float serviceRadius = 2.0f;\n\n    [Header("Service Actions")]\n    public string navigationTopic = "/move_base_simple/goal";\n    public string manipulationTopic = "/manipulation_controller/command";\n\n    void Start()\n    {\n        ros = ROSConnection.instance;\n        Debug.Log("Humanoid assistant initialized");\n    }\n\n    void Update()\n    {\n        CheckForServiceRequests();\n    }\n\n    void CheckForServiceRequests()\n    {\n        // Check if human is within service radius of any service point\n        foreach (Transform servicePoint in servicePoints)\n        {\n            Collider[] hitColliders = Physics.OverlapSphere(servicePoint.position, serviceRadius);\n            foreach (Collider collider in hitColliders)\n            {\n                if (collider.CompareTag("Human"))\n                {\n                    HandleServiceRequest(servicePoint, collider.gameObject);\n                    break;\n                }\n            }\n        }\n    }\n\n    void HandleServiceRequest(Transform servicePoint, GameObject human)\n    {\n        Debug.Log($"Service request detected at {servicePoint.name}");\n\n        // Move to service point\n        MoveToLocation(servicePoint.position);\n\n        // Wait for service completion\n        StartCoroutine(ProvideService(servicePoint, human));\n    }\n\n    void MoveToLocation(Vector3 targetPosition)\n    {\n        // Publish navigation goal to ROS\n        // Implementation would involve sending a PoseStamped message\n        Debug.Log($"Moving to {targetPosition}");\n    }\n\n    System.Collections.IEnumerator ProvideService(Transform servicePoint, GameObject human)\n    {\n        yield return new WaitForSeconds(2.0f); // Simulate service time\n\n        // Publish service completion\n        Debug.Log("Service completed");\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"example-2-collaborative-task-environment",children:"Example 2: Collaborative Task Environment"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class CollaborativeTaskManager : MonoBehaviour\n{\n    [Header("Task Configuration")]\n    public List<TaskDefinition> tasks;\n    public Transform[] workstations;\n    public GameObject[] objectsToManipulate;\n\n    [Header("Human-Robot Collaboration")]\n    public float collaborationDistance = 1.5f;\n    public float taskCompletionThreshold = 0.1f;\n\n    private int currentTaskIndex = 0;\n    private bool taskInProgress = false;\n\n    void Update()\n    {\n        if (tasks.Count > 0 && !taskInProgress)\n        {\n            StartNextTask();\n        }\n    }\n\n    void StartNextTask()\n    {\n        if (currentTaskIndex >= tasks.Count) return;\n\n        TaskDefinition currentTask = tasks[currentTaskIndex];\n        Debug.Log($"Starting task: {currentTask.taskName}");\n\n        // Set up the task environment\n        SetupTaskEnvironment(currentTask);\n\n        taskInProgress = true;\n    }\n\n    void SetupTaskEnvironment(TaskDefinition task)\n    {\n        // Position objects according to task requirements\n        for (int i = 0; i < task.objectPositions.Count; i++)\n        {\n            if (i < objectsToManipulate.Length)\n            {\n                objectsToManipulate[i].transform.position = task.objectPositions[i];\n            }\n        }\n\n        // Enable task-specific interactions\n        EnableTaskInteractions(task);\n    }\n\n    void EnableTaskInteractions(TaskDefinition task)\n    {\n        // Enable specific interaction modes based on task\n        switch (task.taskType)\n        {\n            case TaskType.Assembly:\n                EnableAssemblyMode();\n                break;\n            case TaskType.Transport:\n                EnableTransportMode();\n                break;\n            case TaskType.Inspection:\n                EnableInspectionMode();\n                break;\n        }\n    }\n\n    void EnableAssemblyMode()\n    {\n        Debug.Log("Assembly mode enabled - human and robot can collaborate on assembly tasks");\n    }\n\n    void EnableTransportMode()\n    {\n        Debug.Log("Transport mode enabled - robot can assist with object transport");\n    }\n\n    void EnableInspectionMode()\n    {\n        Debug.Log("Inspection mode enabled - robot can assist with quality inspection");\n    }\n}\n\n[System.Serializable]\npublic class TaskDefinition\n{\n    public string taskName;\n    public TaskType taskType;\n    public List<Vector3> objectPositions;\n    public List<Transform> targetPositions;\n    public string completionCriteria;\n}\n\npublic enum TaskType\n{\n    Assembly,\n    Transport,\n    Inspection,\n    Maintenance\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsx)(e.h3,{id:"exercise-1-hri-environment-creation",children:"Exercise 1: HRI Environment Creation"}),"\n",(0,o.jsx)(e.p,{children:"Create a Unity scene that includes:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"A human-centered environment (e.g., kitchen, office, or living room)"}),"\n",(0,o.jsx)(e.li,{children:"Properly scaled furniture for humanoid robot interaction"}),"\n",(0,o.jsx)(e.li,{children:"Interactive elements for human-robot collaboration"}),"\n",(0,o.jsx)(e.li,{children:"Realistic lighting and materials"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-2-ros-integration",children:"Exercise 2: ROS Integration"}),"\n",(0,o.jsx)(e.p,{children:"Implement ROS communication in Unity that:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Subscribes to joint states and visualizes robot in Unity"}),"\n",(0,o.jsx)(e.li,{children:"Publishes camera images from Unity to ROS"}),"\n",(0,o.jsx)(e.li,{children:"Integrates speech recognition and text-to-speech"}),"\n",(0,o.jsx)(e.li,{children:"Handles basic navigation and manipulation commands"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-3-vla-pipeline-integration",children:"Exercise 3: VLA Pipeline Integration"}),"\n",(0,o.jsx)(e.p,{children:"Create a complete Unity scene that demonstrates:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Vision system integration with Unity cameras"}),"\n",(0,o.jsx)(e.li,{children:"Language processing for human commands"}),"\n",(0,o.jsx)(e.li,{children:"Action execution for robot behaviors"}),"\n",(0,o.jsx)(e.li,{children:"Real-time interaction between human and robot avatars"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"Unity provides a powerful platform for creating realistic human-robot interaction scenarios that complement traditional robotics simulation. The high visual fidelity and interactive capabilities of Unity make it ideal for developing and testing humanoid robots in human-centered environments. Integration with ROS 2 enables the full Vision-Language-Action pipeline to be tested in photorealistic settings, supporting the development of robots that can operate effectively in spaces designed for human use."}),"\n",(0,o.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:'"Unity Robotics Package Documentation" - Official Unity Robotics Hub guide'}),"\n",(0,o.jsx)(e.li,{children:'"Human-Robot Interaction: Fundamentals and Implementation" by Chen and Sauser'}),"\n",(0,o.jsx)(e.li,{children:'"Unity in Action" by Joe Hocking (for Unity fundamentals)'}),"\n",(0,o.jsx)(e.li,{children:'"ROS Robotics Projects" by Anil Mahtani (for ROS-Unity integration)'}),"\n",(0,o.jsx)(e.li,{children:'"Computer Graphics: Principles and Practice" for realistic rendering'}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>a,x:()=>s});var t=i(6540);const o={},r=t.createContext(o);function a(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);