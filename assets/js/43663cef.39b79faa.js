"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[635],{4542(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"chapters/module-3-simulation/chapter-13-isaac-sdk","title":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","description":"Isaac SDK for perception systems and synthetic data generation in humanoid robotics","source":"@site/docs/chapters/module-3-simulation/chapter-13-isaac-sdk.md","sourceDirName":"chapters/module-3-simulation","slug":"/chapters/module-3-simulation/chapter-13-isaac-sdk","permalink":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk","draft":false,"unlisted":false,"editUrl":"https://github.com/RIMZAASAD/Robotic-ai-Book/edit/main/website/docs/chapters/module-3-simulation/chapter-13-isaac-sdk.md","tags":[],"version":"current","frontMatter":{"title":"Chapter 13 - Isaac SDK for Perception & Synthetic Data","module":"Digital Twin Simulation","chapter":13,"description":"Isaac SDK for perception systems and synthetic data generation in humanoid robotics","learningObjectives":["Use Isaac SDK for perception system development","Generate synthetic data for computer vision training","Implement perception pipelines for humanoid robots"],"prerequisites":["chapter-12-isaac-sim-fundamentals"],"difficulty":"advanced"},"sidebar":"textbookSidebar","previous":{"title":"Chapter 12: NVIDIA Isaac Sim Fundamentals","permalink":"/docs/chapters/module-3-simulation/chapter-12-isaac-sim-fundamentals"},"next":{"title":"Chapter 14: Computer Vision for Robotics","permalink":"/docs/chapters/module-4-vla/chapter-14-computer-vision"}}');var i=t(4848),o=t(8453);const s={title:"Chapter 13 - Isaac SDK for Perception & Synthetic Data",module:"Digital Twin Simulation",chapter:13,description:"Isaac SDK for perception systems and synthetic data generation in humanoid robotics",learningObjectives:["Use Isaac SDK for perception system development","Generate synthetic data for computer vision training","Implement perception pipelines for humanoid robots"],prerequisites:["chapter-12-isaac-sim-fundamentals"],difficulty:"advanced"},r=void 0,c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Isaac SDK Overview",id:"isaac-sdk-overview",level:2},{value:"Key Components for Perception",id:"key-components-for-perception",level:3},{value:"Isaac Sim Perception Tools",id:"isaac-sim-perception-tools",level:4},{value:"Isaac ROS Integration",id:"isaac-ros-integration",level:4},{value:"Synthetic Data Generation Pipeline",id:"synthetic-data-generation-pipeline",level:3},{value:"Setting Up Perception Systems",id:"setting-up-perception-systems",level:2},{value:"Basic Perception Setup",id:"basic-perception-setup",level:3},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:2},{value:"Basic Data Collection Setup",id:"basic-data-collection-setup",level:3},{value:"Advanced Perception Pipelines",id:"advanced-perception-pipelines",level:2},{value:"Multi-Modal Perception",id:"multi-modal-perception",level:3},{value:"Humanoid-Specific Perception Tasks",id:"humanoid-specific-perception-tasks",level:2},{value:"Manipulation Perception",id:"manipulation-perception",level:3},{value:"Constitution Alignment",id:"constitution-alignment",level:2},{value:"Sim-to-Real Rigor (Principle III)",id:"sim-to-real-rigor-principle-iii",level:3},{value:"VLA Convergence Mandate (Principle I)",id:"vla-convergence-mandate-principle-i",level:3},{value:"Visualization Requirements (Key Standard II)",id:"visualization-requirements-key-standard-ii",level:3},{value:"Target Hardware Optimization",id:"target-hardware-optimization",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Synthetic Dataset for Object Detection",id:"example-1-synthetic-dataset-for-object-detection",level:3},{value:"Example 2: Humanoid Manipulation Perception Pipeline",id:"example-2-humanoid-manipulation-perception-pipeline",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Perception Pipeline Implementation",id:"exercise-1-perception-pipeline-implementation",level:3},{value:"Exercise 2: Synthetic Data Generation",id:"exercise-2-synthetic-data-generation",level:3},{value:"Exercise 3: Multi-Modal Fusion",id:"exercise-3-multi-modal-fusion",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function p(e){const n={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use Isaac SDK for perception system development"}),"\n",(0,i.jsx)(n.li,{children:"Generate synthetic data for computer vision training"}),"\n",(0,i.jsx)(n.li,{children:"Implement perception pipelines for humanoid robots"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"The Isaac SDK provides powerful tools for developing perception systems and generating synthetic data that are essential for the Vision component of the Vision-Language-Action pipeline in humanoid robotics. With its advanced rendering capabilities and realistic sensor simulation, Isaac Sim enables the generation of high-quality synthetic datasets that can be used to train computer vision models for humanoid robots. This chapter explores the Isaac SDK's perception tools and synthetic data generation capabilities, with special attention to creating datasets that match the requirements for humanoid robot vision systems and support the Sim-to-Real Rigor principle from our project constitution."}),"\n",(0,i.jsx)(n.h2,{id:"isaac-sdk-overview",children:"Isaac SDK Overview"}),"\n",(0,i.jsx)(n.h3,{id:"key-components-for-perception",children:"Key Components for Perception"}),"\n",(0,i.jsx)(n.p,{children:"The Isaac SDK includes several key components specifically designed for perception system development:"}),"\n",(0,i.jsx)(n.h4,{id:"isaac-sim-perception-tools",children:"Isaac Sim Perception Tools"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Create labeled training data for computer vision"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Simulation"}),": Accurate modeling of cameras, LIDAR, and other sensors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ground Truth Annotation"}),": Automatic generation of semantic segmentation, depth maps, and bounding boxes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Domain Randomization"}),": Techniques to improve sim-to-real transfer"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"isaac-ros-integration",children:"Isaac ROS Integration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Bridge"}),": Seamless integration with Robot Operating System 2"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Message Types"}),": Support for standard ROS sensor message formats"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception Pipelines"}),": Integration with ROS perception frameworks"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"synthetic-data-generation-pipeline",children:"Synthetic Data Generation Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"The synthetic data generation pipeline in Isaac SDK consists of several stages:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scene Configuration"}),": Setting up realistic environments with objects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Placement"}),": Positioning virtual sensors to capture relevant views"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Domain Randomization"}),": Varying environmental parameters for robustness"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Collection"}),": Capturing images, depth maps, and annotations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Post-Processing"}),": Converting data to formats suitable for training"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"setting-up-perception-systems",children:"Setting Up Perception Systems"}),"\n",(0,i.jsx)(n.h3,{id:"basic-perception-setup",children:"Basic Perception Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# perception_setup.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.sensor import Camera\nimport numpy as np\nimport carb\n\nclass HumanoidPerceptionSystem:\n    def __init__(self):\n        # Initialize Isaac Sim world\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Perception components\n        self.cameras = {}\n        self.perception_pipelines = {}\n\n        # Data collection parameters\n        self.collection_enabled = False\n        self.data_buffer = []\n\n        # Domain randomization parameters\n        self.domain_randomization = {\n            \'lighting\': True,\n            \'textures\': True,\n            \'object_poses\': True,\n            \'camera_poses\': True\n        }\n\n    def setup_robot_cameras(self, robot_path):\n        """Set up cameras for humanoid robot perception"""\n\n        # Head-mounted RGB camera for primary vision\n        head_camera = Camera(\n            prim_path=f"{robot_path}/head_camera",\n            frequency=30,\n            resolution=(640, 480),\n            position=np.array([0.1, 0.0, 0.1]),  # Offset from head\n            orientation=np.array([0, 0, 0, 1])\n        )\n\n        # Add perception outputs\n        head_camera.add_ground_truth_to_frame(\n            "/World/StaticObjects",\n            "/World/DynamicObjects"\n        )\n\n        # Enable semantic segmentation\n        head_camera.add_semantic_segmentation_to_frame(\n            "/World/StaticObjects",\n            "/World/DynamicObjects"\n        )\n\n        # Enable depth map generation\n        head_camera.add_distance_to_image_plane_to_frame()\n\n        # Enable surface normals\n        head_camera.add_surface_normals_to_frame()\n\n        self.cameras[\'head\'] = head_camera\n\n        # Chest-mounted wide-angle camera for environment awareness\n        chest_camera = Camera(\n            prim_path=f"{robot_path}/chest_camera",\n            frequency=15,  # Lower frequency for wider FOV\n            resolution=(800, 600),\n            position=np.array([0.0, 0.0, 0.3]),  # Chest level\n            orientation=np.array([0, 0, 0, 1])\n        )\n\n        # Configure wide FOV (120 degrees)\n        from omni.isaac.core.utils.prims import get_prim_at_path\n        camera_prim = get_prim_at_path(f"{robot_path}/chest_camera")\n\n        from pxr import UsdGeom\n        UsdGeom.CameraAPI(camera_prim).GetHorizontalApertureAttr().Set(43.2)  # 120 degree FOV\n        UsdGeom.CameraAPI(camera_prim).GetFocalLengthAttr().Set(18.0)  # Corresponding focal length\n\n        # Add same perception outputs as head camera\n        chest_camera.add_ground_truth_to_frame(\n            "/World/StaticObjects",\n            "/World/DynamicObjects"\n        )\n        chest_camera.add_semantic_segmentation_to_frame(\n            "/World/StaticObjects",\n            "/World/DynamicObjects"\n        )\n        chest_camera.add_distance_to_image_plane_to_frame()\n\n        self.cameras[\'chest\'] = chest_camera\n\n        return head_camera, chest_camera\n\n    def setup_perception_pipeline(self):\n        """Set up perception processing pipeline"""\n\n        # Define perception tasks\n        perception_tasks = {\n            \'object_detection\': {\n                \'input\': \'rgb\',\n                \'output\': \'bounding_boxes\',\n                \'model\': \'yolo\'\n            },\n            \'semantic_segmentation\': {\n                \'input\': \'rgb\',\n                \'output\': \'class_masks\',\n                \'model\': \'deeplab\'\n            },\n            \'depth_estimation\': {\n                \'input\': \'stereo_pair\',  # Or use depth from simulation\n                \'output\': \'depth_map\',\n                \'model\': \'monodepth\'\n            },\n            \'pose_estimation\': {\n                \'input\': \'rgb\',\n                \'output\': \'object_poses\',\n                \'model\': \'keypoint_rcnn\'\n            }\n        }\n\n        # Initialize pipeline components\n        for task_name, task_config in perception_tasks.items():\n            self.perception_pipelines[task_name] = self.initialize_perception_task(\n                task_name, task_config\n            )\n\n    def initialize_perception_task(self, task_name, config):\n        """Initialize a specific perception task"""\n        if task_name == \'object_detection\':\n            return ObjectDetectionPipeline(config)\n        elif task_name == \'semantic_segmentation\':\n            return SemanticSegmentationPipeline(config)\n        elif task_name == \'depth_estimation\':\n            return DepthEstimationPipeline(config)\n        elif task_name == \'pose_estimation\':\n            return PoseEstimationPipeline(config)\n        else:\n            raise ValueError(f"Unknown perception task: {task_name}")\n\nclass PerceptionPipeline:\n    """Base class for perception pipelines"""\n    def __init__(self, config):\n        self.config = config\n        self.enabled = True\n\n    def process(self, input_data):\n        """Process input data and return results"""\n        raise NotImplementedError\n\nclass ObjectDetectionPipeline(PerceptionPipeline):\n    def process(self, rgb_image):\n        """Detect objects in RGB image"""\n        # In simulation, we can use ground truth\n        # In real implementation, this would use a trained model\n        bounding_boxes = self.generate_bounding_boxes(rgb_image)\n        return bounding_boxes\n\n    def generate_bounding_boxes(self, image):\n        """Generate bounding boxes from ground truth"""\n        # This would interface with Isaac Sim\'s ground truth system\n        return []\n\nclass SemanticSegmentationPipeline(PerceptionPipeline):\n    def process(self, rgb_image):\n        """Generate semantic segmentation"""\n        # Use Isaac Sim\'s semantic segmentation\n        segmentation_map = self.get_semantic_segmentation(rgb_image)\n        return segmentation_map\n\n    def get_semantic_segmentation(self, image):\n        """Get semantic segmentation from Isaac Sim"""\n        # Implementation would use Isaac Sim\'s segmentation API\n        return np.zeros_like(image)\n\nclass DepthEstimationPipeline(PerceptionPipeline):\n    def process(self, depth_data):\n        """Process depth information"""\n        # In simulation, depth is available directly\n        return depth_data\n\nclass PoseEstimationPipeline(PerceptionPipeline):\n    def process(self, rgb_image, depth_data):\n        """Estimate object poses"""\n        # Combine RGB and depth for pose estimation\n        poses = self.estimate_poses(rgb_image, depth_data)\n        return poses\n\n    def estimate_poses(self, rgb_image, depth_data):\n        """Estimate poses using RGB-D data"""\n        # Implementation would use Isaac Sim\'s pose estimation tools\n        return []\n'})}),"\n",(0,i.jsx)(n.h2,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,i.jsx)(n.h3,{id:"basic-data-collection-setup",children:"Basic Data Collection Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# synthetic_data_generator.py\nimport omni\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nfrom PIL import Image\nimport numpy as np\nimport json\nimport os\nfrom datetime import datetime\n\nclass SyntheticDataGenerator:\n    def __init__(self, output_dir="synthetic_data"):\n        self.output_dir = output_dir\n        self.data_counter = 0\n\n        # Create output directory structure\n        os.makedirs(f"{output_dir}/images", exist_ok=True)\n        os.makedirs(f"{output_dir}/labels", exist_ok=True)\n        os.makedirs(f"{output_dir}/depth", exist_ok=True)\n        os.makedirs(f"{output_dir}/seg", exist_ok=True)\n\n        # Data annotation formats\n        self.annotation_formats = {\n            \'coco\': self.save_as_coco,\n            \'yolo\': self.save_as_yolo,\n            \'kitti\': self.save_as_kitti\n        }\n\n        # Domain randomization parameters\n        self.randomization_params = {\n            \'lighting\': {\n                \'intensity_range\': (0.5, 2.0),\n                \'color_temperature_range\': (4000, 8000)\n            },\n            \'textures\': {\n                \'roughness_range\': (0.0, 1.0),\n                \'metallic_range\': (0.0, 1.0)\n            },\n            \'object_poses\': {\n                \'position_jitter\': 0.1,\n                \'rotation_jitter\': 0.1\n            }\n        }\n\n    def collect_data_batch(self, camera, num_samples=100, annotation_format=\'coco\'):\n        """Collect a batch of synthetic data"""\n\n        print(f"Collecting {num_samples} samples...")\n\n        batch_data = []\n\n        for i in range(num_samples):\n            # Apply domain randomization\n            self.apply_domain_randomization()\n\n            # Render frame\n            omni.timeline.get_timeline_interface().update_current_time(1.0/30.0)  # 30 FPS\n\n            # Get data from camera\n            rgb_image = self.get_rgb_image(camera)\n            depth_map = self.get_depth_map(camera)\n            seg_map = self.get_segmentation_map(camera)\n            ground_truth = self.get_ground_truth(camera)\n\n            # Save data\n            sample_id = f"{self.data_counter:06d}"\n            self.save_sample(sample_id, rgb_image, depth_map, seg_map, ground_truth)\n\n            # Store annotation\n            annotation = self.create_annotation(sample_id, ground_truth)\n            batch_data.append(annotation)\n\n            self.data_counter += 1\n\n            if i % 10 == 0:\n                print(f"Collected {i+1}/{num_samples} samples")\n\n        # Save annotations in specified format\n        self.save_annotations(batch_data, annotation_format)\n\n        print(f"Data collection completed. {num_samples} samples saved to {self.output_dir}")\n        return batch_data\n\n    def get_rgb_image(self, camera):\n        """Get RGB image from camera"""\n        # Get the latest RGB frame\n        rgb_data = camera.get_rgb()\n        return rgb_data\n\n    def get_depth_map(self, camera):\n        """Get depth map from camera"""\n        # Get the latest depth frame\n        depth_data = camera.get_depth()\n        return depth_data\n\n    def get_segmentation_map(self, camera):\n        """Get semantic segmentation map from camera"""\n        # Get the latest segmentation frame\n        seg_data = camera.get_semantic_segmentation()\n        return seg_data\n\n    def get_ground_truth(self, camera):\n        """Get ground truth annotations from Isaac Sim"""\n        # This would interface with Isaac Sim\'s ground truth system\n        ground_truth = {\n            \'bounding_boxes\': [],\n            \'object_poses\': [],\n            \'class_labels\': [],\n            \'instance_ids\': []\n        }\n        return ground_truth\n\n    def apply_domain_randomization(self):\n        """Apply domain randomization to improve sim-to-real transfer"""\n\n        # Randomize lighting\n        if self.randomization_params[\'lighting\']:\n            self.randomize_lighting()\n\n        # Randomize textures\n        if self.randomization_params[\'textures\']:\n            self.randomize_textures()\n\n        # Randomize object poses\n        if self.randomization_params[\'object_poses\']:\n            self.randomize_object_poses()\n\n    def randomize_lighting(self):\n        """Randomize lighting conditions"""\n        from omni.isaac.core.utils.prims import get_prim_at_path\n        from pxr import UsdLux\n\n        # Get all lights in the scene\n        light_prims = [prim for prim in omni.usd.get_context().get_stage().TraverseAll()\n                      if prim.IsA(UsdLux.DistantLight) or prim.IsA(UsdLux.DomeLight)]\n\n        for light_prim in light_prims:\n            if light_prim.IsA(UsdLux.DistantLight):\n                # Randomize intensity\n                intensity_range = self.randomization_params[\'lighting\'][\'intensity_range\']\n                new_intensity = np.random.uniform(*intensity_range)\n\n                light_api = UsdLux.DistantLight(light_prim)\n                light_api.GetIntensityAttr().Set(new_intensity)\n\n    def randomize_textures(self):\n        """Randomize material properties"""\n        # Implementation would randomize material properties\n        # such as roughness, metallic, and color values\n        pass\n\n    def randomize_object_poses(self):\n        """Randomize object positions and orientations"""\n        # Get all dynamic objects in the scene\n        # Add small random perturbations to their poses\n        pass\n\n    def save_sample(self, sample_id, rgb_image, depth_map, seg_map, ground_truth):\n        """Save a single data sample"""\n\n        # Save RGB image\n        rgb_pil = Image.fromarray(rgb_image)\n        rgb_pil.save(f"{self.output_dir}/images/{sample_id}.png")\n\n        # Save depth map\n        depth_normalized = ((depth_map - depth_map.min()) /\n                           (depth_map.max() - depth_map.min()) * 255).astype(np.uint8)\n        depth_pil = Image.fromarray(depth_normalized)\n        depth_pil.save(f"{self.output_dir}/depth/{sample_id}.png")\n\n        # Save segmentation map\n        seg_normalized = ((seg_map - seg_map.min()) /\n                         (seg_map.max() - seg_map.min()) * 255).astype(np.uint8)\n        seg_pil = Image.fromarray(seg_normalized)\n        seg_pil.save(f"{self.output_dir}/seg/{sample_id}.png")\n\n        # Save raw data as numpy arrays for more precise processing\n        np.save(f"{self.output_dir}/depth/{sample_id}_raw.npy", depth_map)\n        np.save(f"{self.output_dir}/seg/{sample_id}_raw.npy", seg_map)\n\n    def create_annotation(self, sample_id, ground_truth):\n        """Create annotation for a sample"""\n        annotation = {\n            \'id\': sample_id,\n            \'file_name\': f"{sample_id}.png",\n            \'width\': 640,  # Assuming 640x480 resolution\n            \'height\': 480,\n            \'annotations\': ground_truth\n        }\n        return annotation\n\n    def save_annotations(self, batch_data, format_type=\'coco\'):\n        """Save annotations in specified format"""\n        if format_type in self.annotation_formats:\n            self.annotation_formats[format_type](batch_data)\n        else:\n            raise ValueError(f"Unknown annotation format: {format_type}")\n\n    def save_as_coco(self, batch_data):\n        """Save annotations in COCO format"""\n        coco_format = {\n            \'info\': {\n                \'year\': datetime.now().year,\n                \'version\': \'1.0\',\n                \'description\': \'Synthetic Humanoid Perception Dataset\',\n                \'contributor\': \'Isaac Sim Synthetic Data Generator\',\n                \'date_created\': datetime.now().isoformat()\n            },\n            \'images\': [],\n            \'annotations\': [],\n            \'categories\': []\n        }\n\n        # Convert our data to COCO format\n        for i, sample in enumerate(batch_data):\n            coco_format[\'images\'].append({\n                \'id\': i,\n                \'file_name\': sample[\'file_name\'],\n                \'width\': sample[\'width\'],\n                \'height\': sample[\'height\'],\n                \'date_captured\': datetime.now().isoformat()\n            })\n\n        # Save to file\n        with open(f"{self.output_dir}/annotations.json", \'w\') as f:\n            json.dump(coco_format, f, indent=2)\n\n    def save_as_yolo(self, batch_data):\n        """Save annotations in YOLO format"""\n        # YOLO format implementation\n        pass\n\n    def save_as_kitti(self, batch_data):\n        """Save annotations in KITTI format"""\n        # KITTI format implementation\n        pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-perception-pipelines",children:"Advanced Perception Pipelines"}),"\n",(0,i.jsx)(n.h3,{id:"multi-modal-perception",children:"Multi-Modal Perception"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# multimodal_perception.py\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport cv2\n\nclass MultiModalPerception:\n    def __init__(self, camera_intrinsics, robot_state):\n        self.camera_intrinsics = camera_intrinsics\n        self.robot_state = robot_state\n\n        # Initialize different perception modules\n        self.vision_module = VisionPerception()\n        self.depth_module = DepthPerception()\n        self.fusion_module = SensorFusion()\n\n        # Object detection and tracking\n        self.object_detector = ObjectDetectionSystem()\n        self.object_tracker = MultiObjectTracker()\n\n    def process_multimodal_input(self, rgb_image, depth_map, imu_data, joint_states):\n        """Process multimodal sensor input"""\n\n        # Process visual information\n        visual_features = self.vision_module.extract_features(rgb_image)\n        object_detections = self.object_detector.detect_objects(rgb_image)\n\n        # Process depth information\n        point_cloud = self.depth_module.generate_point_cloud(rgb_image, depth_map)\n        surface_normals = self.depth_module.estimate_surface_normals(point_cloud)\n\n        # Fuse sensor information\n        fused_state = self.fusion_module.fuse_sensors(\n            visual_features,\n            point_cloud,\n            imu_data,\n            joint_states\n        )\n\n        # Update object tracking\n        tracked_objects = self.object_tracker.update_tracks(\n            object_detections,\n            fused_state\n        )\n\n        return {\n            \'fused_state\': fused_state,\n            \'tracked_objects\': tracked_objects,\n            \'point_cloud\': point_cloud,\n            \'surface_normals\': surface_normals\n        }\n\nclass VisionPerception:\n    def __init__(self):\n        # Initialize vision models\n        self.feature_extractor = self.load_feature_extractor()\n        self.classifier = self.load_classifier()\n\n    def extract_features(self, image):\n        """Extract visual features from image"""\n        # This would typically use a CNN or other deep learning model\n        features = np.random.random((256,))  # Placeholder\n        return features\n\n    def load_feature_extractor(self):\n        """Load pre-trained feature extraction model"""\n        # Implementation would load a pre-trained model\n        return None\n\n    def load_classifier(self):\n        """Load object classification model"""\n        # Implementation would load a classification model\n        return None\n\nclass DepthPerception:\n    def __init__(self):\n        self.camera_matrix = None\n\n    def generate_point_cloud(self, rgb_image, depth_map):\n        """Generate 3D point cloud from RGB-D data"""\n        height, width = depth_map.shape\n        camera_matrix = self.camera_matrix\n\n        # Create coordinate grids\n        x, y = np.meshgrid(np.arange(width), np.arange(height))\n\n        # Convert to 3D coordinates\n        x_3d = (x - camera_matrix[0, 2]) * depth_map / camera_matrix[0, 0]\n        y_3d = (y - camera_matrix[1, 2]) * depth_map / camera_matrix[1, 1]\n\n        # Stack into point cloud\n        point_cloud = np.stack([x_3d, y_3d, depth_map], axis=-1)\n\n        return point_cloud\n\n    def estimate_surface_normals(self, point_cloud):\n        """Estimate surface normals from point cloud"""\n        # Simple normal estimation using neighboring points\n        normals = np.zeros_like(point_cloud)\n\n        # Implementation would estimate normals using point cloud processing\n        # This is a simplified version\n        for i in range(1, point_cloud.shape[0]-1):\n            for j in range(1, point_cloud.shape[1]-1):\n                # Get neighboring points\n                p_center = point_cloud[i, j]\n                p_right = point_cloud[i, j+1]\n                p_down = point_cloud[i+1, j]\n\n                # Compute normal using cross product\n                v1 = p_right - p_center\n                v2 = p_down - p_center\n                normal = np.cross(v1, v2)\n                normal = normal / (np.linalg.norm(normal) + 1e-8)  # Normalize\n\n                normals[i, j] = normal\n\n        return normals\n\nclass SensorFusion:\n    def __init__(self):\n        # Initialize fusion algorithms\n        self.kalman_filter = self.initialize_kalman_filter()\n\n    def initialize_kalman_filter(self):\n        """Initialize Kalman filter for sensor fusion"""\n        # Implementation would create a Kalman filter\n        return None\n\n    def fuse_sensors(self, visual_features, point_cloud, imu_data, joint_states):\n        """Fuse information from multiple sensors"""\n\n        # Combine all sensor data into a unified state estimate\n        fused_state = {\n            \'position\': self.estimate_position(imu_data, joint_states),\n            \'orientation\': self.estimate_orientation(imu_data),\n            \'velocity\': self.estimate_velocity(imu_data),\n            \'environment_map\': self.build_environment_map(point_cloud),\n            \'object_map\': self.build_object_map(visual_features)\n        }\n\n        return fused_state\n\n    def estimate_position(self, imu_data, joint_states):\n        """Estimate robot position using IMU and joint data"""\n        # Integrate IMU acceleration data\n        # Use forward kinematics from joint states\n        position = np.array([0.0, 0.0, 0.0])  # Placeholder\n        return position\n\n    def estimate_orientation(self, imu_data):\n        """Estimate robot orientation using IMU"""\n        # Integrate gyroscope data, correct with accelerometer\n        orientation = np.array([0.0, 0.0, 0.0, 1.0])  # Quaternion\n        return orientation\n\n    def estimate_velocity(self, imu_data):\n        """Estimate robot velocity using IMU"""\n        velocity = np.array([0.0, 0.0, 0.0])  # Placeholder\n        return velocity\n\n    def build_environment_map(self, point_cloud):\n        """Build environment map from point cloud"""\n        # Implementation would create a 3D map\n        return {}\n\n    def build_object_map(self, visual_features):\n        """Build object map from visual features"""\n        # Implementation would identify and map objects\n        return {}\n\nclass ObjectDetectionSystem:\n    def __init__(self):\n        self.detection_model = self.load_detection_model()\n        self.confidence_threshold = 0.5\n\n    def load_detection_model(self):\n        """Load object detection model"""\n        # Implementation would load a pre-trained model\n        return None\n\n    def detect_objects(self, image):\n        """Detect objects in image"""\n        # This would run the detection model\n        # Return bounding boxes, class labels, and confidence scores\n        detections = [\n            {\n                \'bbox\': [x, y, w, h],\n                \'class\': \'object_class\',\n                \'confidence\': 0.8,\n                \'mask\': None  # Segmentation mask if available\n            }\n            for x, y, w, h in [(50, 50, 100, 100), (200, 150, 80, 80)]  # Example detections\n        ]\n        return detections\n\nclass MultiObjectTracker:\n    def __init__(self):\n        self.trackers = {}\n        self.next_id = 0\n\n    def update_tracks(self, detections, state):\n        """Update object tracks with new detections"""\n        # Implementation would use data association and tracking algorithms\n        # such as SORT, Deep SORT, or other multi-object tracking methods\n        tracked_objects = []\n\n        for detection in detections:\n            # Simple assignment based on overlap\n            assigned = False\n            for track_id, track in self.trackers.items():\n                if self.bbox_overlap(track[\'bbox\'], detection[\'bbox\']) > 0.3:\n                    # Update existing track\n                    track[\'bbox\'] = detection[\'bbox\']\n                    track[\'class\'] = detection[\'class\']\n                    track[\'confidence\'] = detection[\'confidence\']\n                    tracked_objects.append({\n                        \'id\': track_id,\n                        \'bbox\': track[\'bbox\'],\n                        \'class\': track[\'class\'],\n                        \'confidence\': track[\'confidence\']\n                    })\n                    assigned = True\n                    break\n\n            if not assigned:\n                # Create new track\n                new_id = self.next_id\n                self.trackers[new_id] = {\n                    \'bbox\': detection[\'bbox\'],\n                    \'class\': detection[\'class\'],\n                    \'confidence\': detection[\'confidence\'],\n                    \'age\': 0\n                }\n                tracked_objects.append({\n                    \'id\': new_id,\n                    \'bbox\': detection[\'bbox\'],\n                    \'class\': detection[\'class\'],\n                    \'confidence\': detection[\'confidence\']\n                })\n                self.next_id += 1\n\n        return tracked_objects\n\n    def bbox_overlap(self, bbox1, bbox2):\n        """Calculate overlap between two bounding boxes"""\n        x1, y1, w1, h1 = bbox1\n        x2, y2, w2, h2 = bbox2\n\n        # Calculate intersection\n        left = max(x1, x2)\n        top = max(y1, y2)\n        right = min(x1 + w1, x2 + w2)\n        bottom = min(y1 + h1, y2 + h2)\n\n        if right < left or bottom < top:\n            return 0.0\n\n        intersection = (right - left) * (bottom - top)\n        area1 = w1 * h1\n        area2 = w2 * h2\n        union = area1 + area2 - intersection\n\n        return intersection / union if union > 0 else 0.0\n'})}),"\n",(0,i.jsx)(n.h2,{id:"humanoid-specific-perception-tasks",children:"Humanoid-Specific Perception Tasks"}),"\n",(0,i.jsx)(n.h3,{id:"manipulation-perception",children:"Manipulation Perception"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# manipulation_perception.py\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass ManipulationPerception:\n    def __init__(self, robot_config):\n        self.robot_config = robot_config\n        self.gripper_camera = None\n        self.arm_joints = robot_config.get_arm_joints()\n\n    def perceive_graspable_objects(self, scene_rgb, scene_depth):\n        """Identify objects that can be grasped by the humanoid"""\n\n        # Detect objects in the scene\n        detections = self.detect_objects(scene_rgb)\n\n        # Filter for graspable objects\n        graspable_objects = []\n        for detection in detections:\n            if self.is_graspable(detection, scene_depth):\n                grasp_poses = self.compute_grasp_poses(detection, scene_depth)\n                detection[\'grasp_poses\'] = grasp_poses\n                graspable_objects.append(detection)\n\n        return graspable_objects\n\n    def detect_objects(self, rgb_image):\n        """Detect objects using perception system"""\n        # Use the multi-modal perception system\n        detector = ObjectDetectionSystem()\n        return detector.detect_objects(rgb_image)\n\n    def is_graspable(self, object_detection, depth_map):\n        """Determine if an object is graspable"""\n        # Check if object is within reach\n        object_center_3d = self.project_to_3d(\n            object_detection[\'bbox\'],\n            depth_map\n        )\n\n        # Check if object is reachable by the arm\n        if self.is_reachable(object_center_3d):\n            # Check object size is appropriate for hand\n            bbox = object_detection[\'bbox\']\n            width, height = bbox[2], bbox[3]\n            object_size = np.sqrt(width**2 + height**2)\n\n            if 20 < object_size < 200:  # Reasonable size range\n                return True\n\n        return False\n\n    def compute_grasp_poses(self, object_detection, depth_map):\n        """Compute potential grasp poses for an object"""\n\n        # Get 3D position of object\n        bbox = object_detection[\'bbox\']\n        x, y, w, h = bbox\n\n        # Sample multiple points on the object\n        grasp_candidates = []\n\n        # Center grasp\n        center_3d = self.project_to_3d(\n            (x + w/2, y + h/2, w, h),\n            depth_map\n        )\n\n        # Generate grasp poses around the object\n        for angle in np.linspace(0, 2*np.pi, 8):\n            offset_x = 0.05 * np.cos(angle)  # 5cm offset\n            offset_y = 0.05 * np.sin(angle)\n\n            grasp_pose = {\n                \'position\': center_3d + np.array([offset_x, offset_y, 0]),\n                \'orientation\': self.compute_preferred_orientation(\n                    center_3d, angle\n                ),\n                \'approach_direction\': np.array([0, 0, -1]),  # From above\n                \'grasp_type\': \'top_grasp\'\n            }\n\n            grasp_candidates.append(grasp_pose)\n\n        return grasp_candidates\n\n    def project_to_3d(self, bbox, depth_map):\n        """Project 2D bounding box center to 3D"""\n        x, y, w, h = bbox\n        center_x, center_y = int(x + w/2), int(y + h/2)\n\n        # Get depth at center\n        depth = depth_map[center_y, center_x]\n\n        # Project to 3D using camera intrinsics\n        # This is a simplified version - would need actual camera matrix\n        fx, fy = 616.171, 616.171  # Typical for 640x480 camera\n        cx, cy = 319.5, 239.5\n\n        x_3d = (center_x - cx) * depth / fx\n        y_3d = (center_y - cy) * depth / fy\n        z_3d = depth\n\n        return np.array([x_3d, y_3d, z_3d])\n\n    def compute_preferred_orientation(self, object_pos, approach_angle):\n        """Compute preferred grasp orientation"""\n        # For simple objects, approach from the top or side\n        # depending on object shape and orientation\n\n        # Default orientation - gripper pointing down\n        rotation = R.from_euler(\'xyz\', [0, 0, approach_angle]).as_quat()\n\n        return rotation\n\n    def is_reachable(self, object_position):\n        """Check if object is within robot\'s reach"""\n        # Get current end-effector position\n        current_ee_pos = self.get_end_effector_position()\n\n        # Calculate distance\n        distance = np.linalg.norm(object_position - current_ee_pos)\n\n        # Check if within reach (simplified - would need proper IK)\n        max_reach = self.robot_config.get_max_arm_reach()\n\n        return distance < max_reach\n\n    def get_end_effector_position(self):\n        """Get current end-effector position from robot state"""\n        # This would interface with the robot\'s forward kinematics\n        return np.array([0.5, 0.0, 1.0])  # Placeholder\n\nclass NavigationPerception:\n    def __init__(self, robot_config):\n        self.robot_config = robot_config\n        self.navigation_map = None\n\n    def perceive_navigable_environment(self, rgb_image, depth_map):\n        """Perceive the navigable environment for humanoid navigation"""\n\n        # Create traversability map\n        traversability_map = self.create_traversability_map(depth_map)\n\n        # Detect obstacles\n        obstacles = self.detect_obstacles(rgb_image, depth_map)\n\n        # Detect walkable surfaces\n        walkable_surfaces = self.detect_walkable_surfaces(depth_map)\n\n        # Detect stairs, ramps, and other navigation challenges\n        navigation_features = self.detect_navigation_features(rgb_image, depth_map)\n\n        return {\n            \'traversability_map\': traversability_map,\n            \'obstacles\': obstacles,\n            \'walkable_surfaces\': walkable_surfaces,\n            \'navigation_features\': navigation_features\n        }\n\n    def create_traversability_map(self, depth_map):\n        """Create a traversability map from depth data"""\n        # Analyze surface normals to determine walkability\n        height, width = depth_map.shape\n\n        traversability = np.ones((height, width))  # 1.0 = traversable, 0.0 = not traversable\n\n        # Calculate surface normals\n        for i in range(1, height-1):\n            for j in range(1, width-1):\n                # Simple normal estimation\n                dz_dx = (depth_map[i, j+1] - depth_map[i, j-1]) / 2\n                dz_dy = (depth_map[i+1, j] - depth_map[i-1, j]) / 2\n\n                normal = np.array([-dz_dx, -dz_dy, 1.0])\n                normal = normal / np.linalg.norm(normal)\n\n                # Check if surface is too steep (humanoid can handle ~30 degrees)\n                angle_with_vertical = np.arccos(np.abs(normal[2]))\n                max_walkable_angle = np.radians(30)  # 30 degrees\n\n                if angle_with_vertical > max_walkable_angle:\n                    traversability[i, j] = 0.0  # Too steep\n\n        return traversability\n\n    def detect_obstacles(self, rgb_image, depth_map):\n        """Detect obstacles in the environment"""\n        # Combine visual and depth information\n        obstacles = []\n\n        # Use depth to find obstacles\n        # Objects closer than robot height are potential obstacles\n        robot_height = self.robot_config.get_height()\n        obstacle_mask = depth_map < robot_height\n\n        # Find connected components as individual obstacles\n        from scipy import ndimage\n        labeled_obstacles, num_obstacles = ndimage.label(obstacle_mask)\n\n        for i in range(1, num_obstacles + 1):\n            # Get bounding box of each obstacle\n            obstacle_pixels = np.where(labeled_obstacles == i)\n            if len(obstacle_pixels[0]) > 10:  # Only consider substantial obstacles\n                min_row, max_row = obstacle_pixels[0].min(), obstacle_pixels[0].max()\n                min_col, max_col = obstacle_pixels[1].min(), obstacle_pixels[1].max()\n\n                obstacle = {\n                    \'bbox\': [min_col, min_row, max_col - min_col, max_row - min_row],\n                    \'center_3d\': self.project_to_3d(\n                        [min_col + (max_col - min_col)/2, min_row + (max_row - min_row)/2, 0, 0],\n                        depth_map\n                    ),\n                    \'type\': \'obstacle\'\n                }\n                obstacles.append(obstacle)\n\n        return obstacles\n\n    def detect_walkable_surfaces(self, depth_map):\n        """Detect walkable surfaces"""\n        # Surfaces that are flat and at appropriate height\n        walkable_surfaces = []\n\n        # Look for surfaces at foot level (0.1m above ground)\n        ground_level = depth_map.min()\n        foot_level = ground_level + 0.1\n\n        # Find surfaces within a range of foot level\n        surface_mask = np.abs(depth_map - foot_level) < 0.05  # 5cm tolerance\n\n        # Find connected components as individual surfaces\n        from scipy import ndimage\n        labeled_surfaces, num_surfaces = ndimage.label(surface_mask)\n\n        for i in range(1, num_surfaces + 1):\n            surface_pixels = np.where(labeled_surfaces == i)\n            if len(surface_pixels[0]) > 100:  # Only consider substantial surfaces\n                min_row, max_row = surface_pixels[0].min(), surface_pixels[0].max()\n                min_col, max_col = surface_pixels[1].min(), surface_pixels[1].max()\n\n                surface = {\n                    \'bbox\': [min_col, min_row, max_col - min_col, max_row - min_row],\n                    \'center_3d\': self.project_to_3d(\n                        [min_col + (max_col - min_col)/2, min_row + (max_row - min_row)/2, 0, 0],\n                        depth_map\n                    ),\n                    \'type\': \'walkable_surface\'\n                }\n                walkable_surfaces.append(surface)\n\n        return walkable_surfaces\n\n    def detect_navigation_features(self, rgb_image, depth_map):\n        """Detect special navigation features like stairs, doors, etc."""\n        navigation_features = {\n            \'stairs\': [],\n            \'doors\': [],\n            \'ramps\': [],\n            \'elevators\': [],\n            \'narrow_passages\': []\n        }\n\n        # This would use specialized detection algorithms\n        # For now, we\'ll simulate detection of some features\n\n        # Detect stairs based on depth discontinuities\n        stairs = self.detect_stairs(depth_map)\n        navigation_features[\'stairs\'] = stairs\n\n        # Detect doors based on visual patterns (simplified)\n        doors = self.detect_doors(rgb_image)\n        navigation_features[\'doors\'] = doors\n\n        return navigation_features\n\n    def detect_stairs(self, depth_map):\n        """Detect stairs in depth map"""\n        stairs = []\n\n        # Look for regular step patterns in depth\n        # This is a simplified approach\n        height, width = depth_map.shape\n\n        # Look for horizontal bands of consistent depth (steps)\n        for row in range(0, height, 20):  # Check every 20 pixels\n            row_depths = depth_map[row:row+20, :]\n            unique_depths = np.unique(row_depths)\n\n            # Look for discrete depth levels that might indicate steps\n            if len(unique_depths) > 2:  # Multiple depth levels\n                # Check if they form a regular pattern\n                depth_diffs = np.diff(np.sort(unique_depths))\n\n                # If differences are roughly consistent, might be stairs\n                if np.std(depth_diffs) < np.mean(depth_diffs) * 0.3:\n                    stair_region = {\n                        \'bbox\': [0, row, width, 20],\n                        \'depth_levels\': unique_depths.tolist(),\n                        \'type\': \'stairs\'\n                    }\n                    stairs.append(stair_region)\n\n        return stairs\n\n    def detect_doors(self, rgb_image):\n        """Detect doors in RGB image"""\n        doors = []\n\n        # Use color and shape cues to detect doors\n        # This is a simplified approach using color segmentation\n\n        # Convert to HSV for better color detection\n        hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)\n\n        # Look for common door colors (brown, white, metal-like)\n        # Brown door detection\n        lower_brown = np.array([10, 50, 50])\n        upper_brown = np.array([30, 255, 255])\n        brown_mask = cv2.inRange(hsv, lower_brown, upper_brown)\n\n        # White door detection\n        lower_white = np.array([0, 0, 200])\n        upper_white = np.array([180, 50, 255])\n        white_mask = cv2.inRange(hsv, lower_white, upper_white)\n\n        combined_mask = cv2.bitwise_or(brown_mask, white_mask)\n\n        # Find contours that could be doors\n        from scipy import ndimage\n        labeled_doors, num_doors = ndimage.label(combined_mask > 0)\n\n        for i in range(1, num_doors + 1):\n            door_pixels = np.where(labeled_doors == i)\n            if len(door_pixels[0]) > 500:  # Only consider substantial regions\n                min_row, max_row = door_pixels[0].min(), door_pixels[0].max()\n                min_col, max_col = door_pixels[1].min(), door_pixels[1].max()\n\n                # Check aspect ratio (doors are typically taller than wide)\n                height, width = max_row - min_row, max_col - min_col\n                aspect_ratio = height / width if width > 0 else 0\n\n                if 1.5 < aspect_ratio < 4.0:  # Reasonable door aspect ratio\n                    door = {\n                        \'bbox\': [min_col, min_row, width, height],\n                        \'aspect_ratio\': aspect_ratio,\n                        \'type\': \'door\'\n                    }\n                    doors.append(door)\n\n        return doors\n'})}),"\n",(0,i.jsx)(n.h2,{id:"constitution-alignment",children:"Constitution Alignment"}),"\n",(0,i.jsx)(n.p,{children:"This chapter addresses several constitutional requirements:"}),"\n",(0,i.jsx)(n.h3,{id:"sim-to-real-rigor-principle-iii",children:"Sim-to-Real Rigor (Principle III)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Synthetic data generation with domain randomization for sim-to-real transfer"}),"\n",(0,i.jsx)(n.li,{children:"Realistic sensor simulation matching hardware specifications"}),"\n",(0,i.jsx)(n.li,{children:"Perception pipelines designed for real-world deployment"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"vla-convergence-mandate-principle-i",children:"VLA Convergence Mandate (Principle I)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Vision system integration with the VLA pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Semantic segmentation and object detection for scene understanding"}),"\n",(0,i.jsx)(n.li,{children:"Multi-modal perception for action planning"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"visualization-requirements-key-standard-ii",children:"Visualization Requirements (Key Standard II)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use of Mermaid diagrams for perception pipeline architecture"}),"\n",(0,i.jsx)(n.li,{children:"Proper code formatting and documentation standards"}),"\n",(0,i.jsx)(n.li,{children:"Clear examples for complex perception systems"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"target-hardware-optimization",children:"Target Hardware Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Efficient perception algorithms suitable for Jetson Orin deployment"}),"\n",(0,i.jsx)(n.li,{children:"Optimized data processing pipelines for embedded systems"}),"\n",(0,i.jsx)(n.li,{children:"Performance considerations for real-time operation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,i.jsx)(n.h3,{id:"example-1-synthetic-dataset-for-object-detection",children:"Example 1: Synthetic Dataset for Object Detection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def create_object_detection_dataset():\n    """Create a synthetic dataset for training object detection models"""\n\n    # Initialize the synthetic data generator\n    generator = SyntheticDataGenerator(output_dir="datasets/object_detection")\n\n    # Set up a scene with common household objects\n    setup_household_scene()\n\n    # Configure domain randomization for robust training\n    generator.randomization_params = {\n        \'lighting\': {\n            \'intensity_range\': (0.3, 2.5),\n            \'color_temperature_range\': (3000, 8000)\n        },\n        \'textures\': {\n            \'roughness_range\': (0.1, 0.9),\n            \'metallic_range\': (0.0, 0.3)\n        },\n        \'object_poses\': {\n            \'position_jitter\': 0.15,\n            \'rotation_jitter\': 0.2\n        }\n    }\n\n    # Collect data for different object categories\n    object_categories = [\n        "cup", "bottle", "box", "phone", "book",\n        "fruit", "toy", "tool", "container"\n    ]\n\n    for category in object_categories:\n        print(f"Collecting data for {category}...")\n\n        # Place objects of this category in scene\n        place_category_objects(category)\n\n        # Collect 500 samples for this category\n        generator.collect_data_batch(\n            camera=get_active_camera(),\n            num_samples=500,\n            annotation_format=\'coco\'\n        )\n\n    print("Object detection dataset creation completed!")\n\ndef setup_household_scene():\n    """Set up a household environment for data collection"""\n    # Implementation would create a kitchen/living room scene\n    # with appropriate furniture and surfaces\n    pass\n\ndef place_category_objects(category):\n    """Place objects of a specific category in the scene"""\n    # Implementation would place objects of the specified category\n    # in various positions and orientations\n    pass\n\ndef get_active_camera():\n    """Get the active camera for data collection"""\n    # Implementation would return the configured camera\n    pass\n'})}),"\n",(0,i.jsx)(n.h3,{id:"example-2-humanoid-manipulation-perception-pipeline",children:"Example 2: Humanoid Manipulation Perception Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def setup_manipulation_perception_pipeline():\n    """Set up perception pipeline for humanoid manipulation tasks"""\n\n    # Initialize robot configuration\n    robot_config = {\n        \'arm_joints\': [\'shoulder\', \'elbow\', \'wrist\'],\n        \'hand_type\': \'anthropomorphic\',\n        \'max_arm_reach\': 1.2,  # meters\n        \'gripper_aperture\': 0.1  # meters\n    }\n\n    # Initialize perception modules\n    manipulation_perceptor = ManipulationPerception(robot_config)\n    vision_system = MultiModalPerception(\n        camera_intrinsics=get_camera_intrinsics(),\n        robot_state=get_robot_state()\n    )\n\n    def perception_callback(rgb_image, depth_map, imu_data, joint_states):\n        """Callback function for perception processing"""\n\n        # Process multimodal input\n        multimodal_output = vision_system.process_multimodal_input(\n            rgb_image, depth_map, imu_data, joint_states\n        )\n\n        # Identify graspable objects\n        graspable_objects = manipulation_perceptor.perceive_graspable_objects(\n            rgb_image, depth_map\n        )\n\n        # Update manipulation planning\n        update_manipulation_planner(graspable_objects, multimodal_output)\n\n        return {\n            \'graspable_objects\': graspable_objects,\n            \'environment_state\': multimodal_output\n        }\n\n    return perception_callback\n\ndef update_manipulation_planner(objects, state):\n    """Update manipulation planner with perception results"""\n    # This would interface with the manipulation planning system\n    # to select appropriate grasp poses and motion plans\n    pass\n\ndef get_camera_intrinsics():\n    """Get camera intrinsic parameters"""\n    # Return camera matrix and distortion coefficients\n    camera_matrix = np.array([\n        [616.171, 0, 319.5],\n        [0, 616.171, 239.5],\n        [0, 0, 1]\n    ])\n    return camera_matrix\n\ndef get_robot_state():\n    """Get current robot state"""\n    # Return current joint states, end-effector pose, etc.\n    return {}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-1-perception-pipeline-implementation",children:"Exercise 1: Perception Pipeline Implementation"}),"\n",(0,i.jsx)(n.p,{children:"Implement a complete perception pipeline that:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrates RGB and depth data for 3D object detection"}),"\n",(0,i.jsx)(n.li,{children:"Uses semantic segmentation for scene understanding"}),"\n",(0,i.jsx)(n.li,{children:"Implements multi-object tracking for dynamic environments"}),"\n",(0,i.jsx)(n.li,{children:"Provides outputs suitable for humanoid manipulation planning"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-2-synthetic-data-generation",children:"Exercise 2: Synthetic Data Generation"}),"\n",(0,i.jsx)(n.p,{children:"Create a synthetic dataset generator that:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implements domain randomization for robust training"}),"\n",(0,i.jsx)(n.li,{children:"Generates multiple annotation formats (COCO, YOLO, KITTI)"}),"\n",(0,i.jsx)(n.li,{children:"Creates diverse scenarios for humanoid navigation"}),"\n",(0,i.jsx)(n.li,{children:"Optimizes for sim-to-real transfer learning"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-3-multi-modal-fusion",children:"Exercise 3: Multi-Modal Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Develop a sensor fusion system that:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Combines visual, depth, and IMU data"}),"\n",(0,i.jsx)(n.li,{children:"Implements Kalman filtering for state estimation"}),"\n",(0,i.jsx)(n.li,{children:"Provides robust perception in challenging conditions"}),"\n",(0,i.jsx)(n.li,{children:"Integrates with the VLA pipeline for action planning"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"The Isaac SDK provides powerful tools for developing perception systems and generating synthetic data essential for humanoid robotics. The synthetic data generation capabilities, combined with realistic sensor simulation and domain randomization, enable the creation of robust perception systems that can operate effectively in real-world environments. Understanding these tools is crucial for implementing the Vision component of the Vision-Language-Action pipeline and achieving effective sim-to-real transfer in humanoid robot applications."}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'"Isaac Sim Synthetic Data Generation Guide" - NVIDIA Developer documentation'}),"\n",(0,i.jsx)(n.li,{children:'"Computer Vision for Robotics" by J\xe4hne and Scharr'}),"\n",(0,i.jsx)(n.li,{children:'"Robotics, Vision and Control" by Peter Corke (Perception chapter)'}),"\n",(0,i.jsx)(n.li,{children:'"Multiple View Geometry in Computer Vision" by Hartley and Zisserman'}),"\n",(0,i.jsx)(n.li,{children:'"Deep Learning for Perception" - Recent research papers and tutorials'}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);