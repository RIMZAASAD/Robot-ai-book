"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[477],{8245(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"chapters/module-4-vla/chapter-14-computer-vision","title":"Chapter 14 - Computer Vision for Robotics","description":"Computer vision techniques specifically for robotics applications and humanoid robots","source":"@site/docs/chapters/module-4-vla/chapter-14-computer-vision.md","sourceDirName":"chapters/module-4-vla","slug":"/chapters/module-4-vla/chapter-14-computer-vision","permalink":"/docs/chapters/module-4-vla/chapter-14-computer-vision","draft":false,"unlisted":false,"editUrl":"https://github.com/RIMZAASAD/Robotic-ai-Book/edit/main/website/docs/chapters/module-4-vla/chapter-14-computer-vision.md","tags":[],"version":"current","frontMatter":{"title":"Chapter 14 - Computer Vision for Robotics","module":"Vision-Language-Action Pipelines","chapter":14,"description":"Computer vision techniques specifically for robotics applications and humanoid robots","learningObjectives":["Implement computer vision techniques for robotics","Apply vision algorithms to humanoid robot perception","Integrate vision with action planning systems"],"prerequisites":["chapter-13-isaac-sdk"],"difficulty":"advanced"},"sidebar":"textbookSidebar","previous":{"title":"Chapter 13: Isaac SDK for Perception & Synthetic Data","permalink":"/docs/chapters/module-3-simulation/chapter-13-isaac-sdk"},"next":{"title":"Chapter 15: Language Understanding in Robotics","permalink":"/docs/chapters/module-4-vla/chapter-15-language-understanding"}}');var o=t(4848),s=t(8453);const r={title:"Chapter 14 - Computer Vision for Robotics",module:"Vision-Language-Action Pipelines",chapter:14,description:"Computer vision techniques specifically for robotics applications and humanoid robots",learningObjectives:["Implement computer vision techniques for robotics","Apply vision algorithms to humanoid robot perception","Integrate vision with action planning systems"],prerequisites:["chapter-13-isaac-sdk"],difficulty:"advanced"},a=void 0,c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Robotics-Specific Computer Vision Challenges",id:"robotics-specific-computer-vision-challenges",level:2},{value:"Motion and Dynamics",id:"motion-and-dynamics",level:3},{value:"Real-Time Constraints",id:"real-time-constraints",level:3},{value:"Physical Interaction Context",id:"physical-interaction-context",level:3},{value:"Essential Vision Techniques for Robotics",id:"essential-vision-techniques-for-robotics",level:2},{value:"Feature Detection and Matching",id:"feature-detection-and-matching",level:3},{value:"Object Detection for Robotics",id:"object-detection-for-robotics",level:3},{value:"3D Vision and Depth Processing",id:"3d-vision-and-depth-processing",level:2},{value:"Depth-Based Perception for Humanoid Robots",id:"depth-based-perception-for-humanoid-robots",level:3},{value:"Real-Time Performance Optimization",id:"real-time-performance-optimization",level:2},{value:"Efficient Vision Processing for Embedded Systems",id:"efficient-vision-processing-for-embedded-systems",level:3},{value:"Constitution Alignment",id:"constitution-alignment",level:2},{value:"VLA Convergence Mandate (Principle I)",id:"vla-convergence-mandate-principle-i",level:3},{value:"Real-Time Validation (Principle IV)",id:"real-time-validation-principle-iv",level:3},{value:"Target Hardware Optimization (Constraint)",id:"target-hardware-optimization-constraint",level:3},{value:"Sim-to-Real Rigor (Principle III)",id:"sim-to-real-rigor-principle-iii",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Humanoid Navigation Vision System",id:"example-1-humanoid-navigation-vision-system",level:3},{value:"Example 2: Manipulation Vision for Humanoid Robot",id:"example-2-manipulation-vision-for-humanoid-robot",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Vision Pipeline Optimization",id:"exercise-1-vision-pipeline-optimization",level:3},{value:"Exercise 2: 3D Perception System",id:"exercise-2-3d-perception-system",level:3},{value:"Exercise 3: Robot-Integrated Vision",id:"exercise-3-robot-integrated-vision",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement computer vision techniques for robotics"}),"\n",(0,o.jsx)(n.li,{children:"Apply vision algorithms to humanoid robot perception"}),"\n",(0,o.jsx)(n.li,{children:"Integrate vision with action planning systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Computer vision forms the foundation of the Vision component in the Vision-Language-Action (VLA) pipeline, which is central to our project's VLA Convergence Mandate principle. For humanoid robots operating in human-centered environments, computer vision systems must be robust, real-time capable, and specifically adapted to the unique challenges of humanoid perception. This chapter explores computer vision techniques tailored for robotics applications, with special emphasis on humanoid robot perception systems that must operate under the constraints of embedded hardware like the NVIDIA Jetson Orin Nano while maintaining the real-time performance requirements for safety and stability."}),"\n",(0,o.jsx)(n.h2,{id:"robotics-specific-computer-vision-challenges",children:"Robotics-Specific Computer Vision Challenges"}),"\n",(0,o.jsx)(n.h3,{id:"motion-and-dynamics",children:"Motion and Dynamics"}),"\n",(0,o.jsx)(n.p,{children:"Unlike traditional computer vision applications, robotics vision systems must operate under continuous motion:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Ego-motion compensation"}),": The robot's own movement affects visual input"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Temporal consistency"}),": Maintaining consistent object tracking during robot motion"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motion blur"}),": Fast robot movements can cause image blur"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Rolling shutter effects"}),": Common in robot-mounted cameras"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"real-time-constraints",children:"Real-Time Constraints"}),"\n",(0,o.jsx)(n.p,{children:"Robotics vision systems have strict timing requirements:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Control loop frequencies"}),": Vision processing must align with control frequencies (often 30-100Hz)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Predictable latency"}),": Deterministic processing times for safe operation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource efficiency"}),": Optimized for embedded hardware constraints"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"physical-interaction-context",children:"Physical Interaction Context"}),"\n",(0,o.jsx)(n.p,{children:"Robotics vision is always in service of physical interaction:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action-oriented perception"}),": Vision output directly drives motor actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"3D understanding"}),": Depth and spatial relationships are critical"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation planning"}),": Need to understand graspable surfaces and object properties"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"essential-vision-techniques-for-robotics",children:"Essential Vision Techniques for Robotics"}),"\n",(0,o.jsx)(n.h3,{id:"feature-detection-and-matching",children:"Feature Detection and Matching"}),"\n",(0,o.jsx)(n.p,{children:"For humanoid robots navigating and interacting in human environments, robust feature detection is essential:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nfrom typing import List, Tuple, Optional\n\nclass RobotFeatureDetector:\n    def __init__(self, detector_type: str = "orb", matching_threshold: float = 0.75):\n        """\n        Initialize feature detector for robotic applications\n\n        Args:\n            detector_type: Type of detector (\'orb\', \'sift\', \'akaze\')\n            matching_threshold: Threshold for good matches\n        """\n        self.detector_type = detector_type\n        self.matching_threshold = matching_threshold\n\n        # Initialize detector based on type\n        if detector_type == "orb":\n            self.detector = cv2.ORB_create(\n                nfeatures=500,\n                scaleFactor=1.2,\n                nlevels=8,\n                edgeThreshold=31,\n                patchSize=31,\n                fastThreshold=20\n            )\n        elif detector_type == "sift":\n            self.detector = cv2.SIFT_create(\n                nfeatures=400,\n                contrastThreshold=0.04,\n                edgeThreshold=10,\n                sigma=1.6\n            )\n        elif detector_type == "akaze":\n            self.detector = cv2.AKAZE_create()\n\n        # Initialize matcher\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING if detector_type == "orb" else cv2.NORM_L2, crossCheck=False)\n\n    def detect_and_compute(self, image: np.ndarray) -> Tuple[List[cv2.KeyPoint], np.ndarray]:\n        """\n        Detect features and compute descriptors\n\n        Args:\n            image: Input image (grayscale recommended)\n\n        Returns:\n            Tuple of (keypoints, descriptors)\n        """\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image\n\n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n\n        if descriptors is None:\n            return [], np.array([])\n\n        return keypoints, descriptors\n\n    def match_features(self, desc1: np.ndarray, desc2: np.ndarray) -> List[cv2.DMatch]:\n        """\n        Match features between two descriptor sets\n\n        Args:\n            desc1: Descriptors from first image\n            desc2: Descriptors from second image\n\n        Returns:\n            List of good matches\n        """\n        if desc1.size == 0 or desc2.size == 0:\n            return []\n\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n\n        # Apply Lowe\'s ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < self.matching_threshold * n.distance:\n                    good_matches.append(m)\n\n        return good_matches\n\n    def estimate_motion(self, prev_image: np.ndarray, curr_image: np.ndarray) -> Optional[np.ndarray]:\n        """\n        Estimate ego-motion between two images\n\n        Args:\n            prev_image: Previous image frame\n            curr_image: Current image frame\n\n        Returns:\n            3x3 transformation matrix or None if insufficient matches\n        """\n        # Detect features in both images\n        prev_kp, prev_desc = self.detect_and_compute(prev_image)\n        curr_kp, curr_desc = self.detect_and_compute(curr_image)\n\n        if prev_desc is None or curr_desc is None:\n            return None\n\n        # Match features\n        matches = self.match_features(prev_desc, curr_desc)\n\n        if len(matches) < 10:  # Need minimum matches for reliable estimation\n            return None\n\n        # Get corresponding points\n        prev_points = np.float32([prev_kp[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n        curr_points = np.float32([curr_kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n        # Estimate transformation (try homography first, then fundamental matrix)\n        transformation, mask = cv2.findHomography(\n            prev_points, curr_points,\n            cv2.RANSAC,\n            ransacReprojThreshold=5.0\n        )\n\n        return transformation\n\nclass VisualOdometry:\n    def __init__(self, feature_detector: RobotFeatureDetector):\n        self.feature_detector = feature_detector\n        self.prev_image = None\n        self.accumulated_transform = np.eye(3)\n        self.position = np.array([0.0, 0.0, 0.0])  # x, y, theta\n\n    def process_frame(self, image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        """\n        Process a frame and update position estimate\n\n        Args:\n            image: Current camera frame\n\n        Returns:\n            Tuple of (current_position, transformation_matrix)\n        """\n        if self.prev_image is None:\n            self.prev_image = image.copy()\n            return self.position, np.eye(3)\n\n        # Estimate motion\n        transform = self.feature_detector.estimate_motion(self.prev_image, image)\n\n        if transform is not None:\n            # Update accumulated transformation\n            self.accumulated_transform = self.accumulated_transform @ transform\n\n            # Extract position from transformation\n            dx = transform[0, 2]\n            dy = transform[1, 2]\n            dtheta = np.arctan2(transform[1, 0], transform[0, 0])\n\n            # Update position (simplified - assumes small rotations)\n            self.position[0] += dx\n            self.position[1] += dy\n            self.position[2] += dtheta\n\n        self.prev_image = image.copy()\n        return self.position, self.accumulated_transform\n'})}),"\n",(0,o.jsx)(n.h3,{id:"object-detection-for-robotics",children:"Object Detection for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"For humanoid robots, object detection must be adapted to the physical interaction context:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\nimport torchvision\nfrom torchvision import transforms\nfrom typing import Dict, List, Tuple\nimport time\n\nclass RoboticObjectDetector:\n    def __init__(self, model_type: str = \"yolo\", confidence_threshold: float = 0.5):\n        \"\"\"\n        Initialize object detector for robotic applications\n\n        Args:\n            model_type: Type of model ('yolo', 'faster_rcnn', 'ssd')\n            confidence_threshold: Minimum confidence for detections\n        \"\"\"\n        self.model_type = model_type\n        self.confidence_threshold = confidence_threshold\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Load pre-trained model\n        if model_type == \"faster_rcnn\":\n            self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n                weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n            )\n        elif model_type == \"ssd\":\n            self.model = torchvision.models.detection.ssd300_vgg16(\n                weights=torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n            )\n        elif model_type == \"yolo\":\n            # For YOLO, we'd typically use ultralytics or similar\n            # Here we'll use a generic approach\n            self.model = self._load_yolo_model()\n\n        self.model.to(self.device)\n        self.model.eval()\n\n        # Preprocessing transforms\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        # COCO class names (first 20 for common objects)\n        self.coco_names = [\n            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n            'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n            'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n            'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n            'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n            'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n            'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n            'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n            'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n    def _load_yolo_model(self):\n        \"\"\"Load YOLO model (placeholder - would use actual YOLO implementation)\"\"\"\n        # In practice, this would load a YOLO model\n        # For now, we'll use Faster R-CNN as a substitute\n        return torchvision.models.detection.fasterrcnn_resnet50_fpn(\n            weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n        )\n\n    def detect_objects(self, image: np.ndarray, return_time: bool = False) -> Dict:\n        \"\"\"\n        Detect objects in image with timing information\n\n        Args:\n            image: Input image (H, W, C, RGB format)\n            return_time: Whether to return processing time\n\n        Returns:\n            Dictionary with detections and metadata\n        \"\"\"\n        start_time = time.time()\n\n        # Preprocess image\n        if isinstance(image, np.ndarray):\n            # Convert from numpy to tensor\n            image_tensor = self.transform(image).unsqueeze(0)  # Add batch dimension\n        else:\n            image_tensor = image\n\n        image_tensor = image_tensor.to(self.device)\n\n        # Run inference\n        with torch.no_grad():\n            predictions = self.model(image_tensor)\n\n        # Process predictions\n        pred = predictions[0]  # Get first (and only) image results\n\n        # Filter by confidence\n        scores = pred['scores'].cpu().numpy()\n        keep_indices = scores >= self.confidence_threshold\n\n        filtered_boxes = pred['boxes'][keep_indices].cpu().numpy()\n        filtered_labels = pred['labels'][keep_indices].cpu().numpy()\n        filtered_scores = scores[keep_indices]\n\n        # Convert to robot-friendly format\n        detections = []\n        for i in range(len(filtered_boxes)):\n            box = filtered_boxes[i]\n            label = int(filtered_labels[i])\n            score = float(filtered_scores[i])\n\n            detection = {\n                'bbox': [float(x) for x in box],  # [x1, y1, x2, y2]\n                'label': self.coco_names[label] if label < len(self.coco_names) else f'unknown_{label}',\n                'confidence': score,\n                'class_id': label\n            }\n            detections.append(detection)\n\n        end_time = time.time()\n        processing_time = end_time - start_time if return_time else None\n\n        result = {\n            'detections': detections,\n            'image_shape': image.shape if isinstance(image, np.ndarray) else image_tensor.shape[-2:],\n            'confidence_threshold': self.confidence_threshold\n        }\n\n        if return_time:\n            result['processing_time'] = processing_time\n\n        return result\n\n    def get_robot_reachable_objects(self, image: np.ndarray, camera_intrinsics: np.ndarray,\n                                  robot_position: np.ndarray, max_distance: float = 1.5) -> List[Dict]:\n        \"\"\"\n        Get objects that are potentially reachable by the robot\n\n        Args:\n            image: Input image\n            camera_intrinsics: 3x3 camera intrinsic matrix\n            robot_position: Robot position in world coordinates [x, y, z]\n            max_distance: Maximum reach distance in meters\n\n        Returns:\n            List of reachable objects with 3D positions\n        \"\"\"\n        # Get object detections\n        detection_result = self.detect_objects(image)\n        detections = detection_result['detections']\n\n        reachable_objects = []\n\n        for detection in detections:\n            bbox = detection['bbox']\n\n            # Get 2D center of bounding box\n            center_x = int((bbox[0] + bbox[2]) / 2)\n            center_y = int((bbox[1] + bbox[3]) / 2)\n\n            # Note: This is simplified - would need actual depth information\n            # For now, we'll assume objects are at a known distance or use depth from other sensors\n            # In practice, this would integrate with depth information\n\n            object_3d = {\n                **detection,\n                'pixel_coords': [center_x, center_y],\n                'estimated_distance': self._estimate_distance_from_size(detection, image.shape)\n            }\n\n            # Check if object is within reach\n            if object_3d['estimated_distance'] <= max_distance:\n                reachable_objects.append(object_3d)\n\n        return reachable_objects\n\n    def _estimate_distance_from_size(self, detection: Dict, image_shape: Tuple) -> float:\n        \"\"\"\n        Estimate distance based on object size in image\n        This is a simplified approach - in practice, would use depth sensor data\n        \"\"\"\n        bbox = detection['bbox']\n        bbox_width = bbox[2] - bbox[0]\n        bbox_height = bbox[3] - bbox[1]\n\n        # Use a simple size-distance relationship (calibrated for common objects)\n        # This is a placeholder - real implementation would use calibrated data\n        image_width = image_shape[1] if len(image_shape) == 3 else image_shape[1]\n        object_width_ratio = bbox_width / image_width\n\n        # Simplified distance estimation (in meters)\n        # In practice, this would use more sophisticated geometric relationships\n        estimated_distance = 1.0 / (object_width_ratio + 0.01)  # Add small value to avoid division by zero\n\n        return min(estimated_distance, 5.0)  # Cap at 5 meters\n"})}),"\n",(0,o.jsx)(n.h2,{id:"3d-vision-and-depth-processing",children:"3D Vision and Depth Processing"}),"\n",(0,o.jsx)(n.h3,{id:"depth-based-perception-for-humanoid-robots",children:"Depth-Based Perception for Humanoid Robots"}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots require sophisticated 3D perception for navigation and manipulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import open3d as o3d\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nfrom typing import List, Tuple, Optional\n\nclass DepthProcessor:\n    def __init__(self, camera_intrinsics: np.ndarray, voxel_size: float = 0.01):\n        """\n        Initialize depth processor for 3D perception\n\n        Args:\n            camera_intrinsics: 3x3 camera intrinsic matrix\n            voxel_size: Size of voxels for point cloud processing\n        """\n        self.camera_intrinsics = camera_intrinsics\n        self.voxel_size = voxel_size\n\n        # Extract intrinsic parameters\n        self.fx = camera_intrinsics[0, 0]\n        self.fy = camera_intrinsics[1, 1]\n        self.cx = camera_intrinsics[0, 2]\n        self.cy = camera_intrinsics[1, 2]\n\n    def depth_to_pointcloud(self, depth_image: np.ndarray, rgb_image: Optional[np.ndarray] = None) -> o3d.geometry.PointCloud:\n        """\n        Convert depth image to point cloud\n\n        Args:\n            depth_image: Depth image (H, W) in meters\n            rgb_image: Optional RGB image for color information\n\n        Returns:\n            Open3D point cloud\n        """\n        height, width = depth_image.shape\n\n        # Create coordinate grids\n        y, x = np.mgrid[0:height, 0:width]\n\n        # Convert to 3D coordinates\n        x_3d = (x - self.cx) * depth_image / self.fx\n        y_3d = (y - self.cy) * depth_image / self.fy\n        z_3d = depth_image\n\n        # Stack into point cloud\n        points = np.stack([x_3d, y_3d, z_3d], axis=-1).reshape(-1, 3)\n\n        # Remove invalid points (where depth is 0 or invalid)\n        valid_mask = (z_3d > 0) & (z_3d < 10)  # Valid range: 0 to 10 meters\n        valid_points = points[valid_mask.flatten()]\n\n        # Create point cloud\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(valid_points)\n\n        # Add colors if RGB image provided\n        if rgb_image is not None:\n            # Reshape and filter colors to match valid points\n            colors = rgb_image.reshape(-1, 3) / 255.0  # Normalize to [0,1]\n            valid_colors = colors[valid_mask.flatten()]\n            pcd.colors = o3d.utility.Vector3dVector(valid_colors)\n\n        return pcd\n\n    def segment_planes(self, pointcloud: o3d.geometry.PointCloud, distance_threshold: float = 0.01,\n                      ransac_n: int = 3, num_iterations: int = 1000) -> Tuple[np.ndarray, o3d.geometry.PointCloud]:\n        """\n        Segment planar surfaces (like floors, tables) from point cloud\n\n        Args:\n            pointcloud: Input point cloud\n            distance_threshold: Maximum distance to plane\n            ransac_n: Number of points for RANSAC\n            num_iterations: Number of RANSAC iterations\n\n        Returns:\n            Tuple of (plane_model, inlier_cloud)\n        """\n        # Downsample point cloud for efficiency\n        downsampled = pointcloud.voxel_down_sample(voxel_size=self.voxel_size * 2)\n\n        # Segment plane using RANSAC\n        plane_model, inliers = downsampled.segment_plane(\n            distance_threshold=distance_threshold,\n            ransac_n=ransac_n,\n            num_iterations=num_iterations\n        )\n\n        # Extract inlier and outlier point clouds\n        inlier_cloud = downsampled.select_by_index(inliers)\n        outlier_cloud = downsampled.select_by_index(inliers, invert=True)\n\n        return plane_model, inlier_cloud\n\n    def find_objects_on_surface(self, pointcloud: o3d.geometry.PointCloud, surface_model: np.ndarray,\n                               min_distance: float = 0.02, max_distance: float = 0.5) -> List[o3d.geometry.PointCloud]:\n        """\n        Find objects positioned on a surface\n\n        Args:\n            pointcloud: Input point cloud (with surface points removed)\n            surface_model: Plane model coefficients [a, b, c, d] for ax+by+cz+d=0\n            min_distance: Minimum distance above surface\n            max_distance: Maximum distance above surface\n\n        Returns:\n            List of point clouds representing individual objects\n        """\n        # Calculate distances from surface for all points\n        points = np.asarray(pointcloud.points)\n\n        # Plane equation: ax + by + cz + d = 0\n        a, b, c, d = surface_model\n        distances = np.abs(a * points[:, 0] + b * points[:, 1] + c * points[:, 2] + d) / np.sqrt(a**2 + b**2 + c**2)\n\n        # Filter points that are at the right height above the surface\n        height_mask = (distances >= min_distance) & (distances <= max_distance)\n        object_points = points[height_mask]\n\n        if len(object_points) == 0:\n            return []\n\n        # Create new point cloud with object points\n        object_pcd = o3d.geometry.PointCloud()\n        object_pcd.points = o3d.utility.Vector3dVector(object_points)\n\n        # Cluster the points to separate individual objects\n        labels = np.array(object_pcd.cluster_dbscan(eps=self.voxel_size * 5, min_points=10, print_progress=False))\n\n        # Group points by cluster label\n        clusters = []\n        for label in set(labels):\n            if label == -1:  # Skip noise points\n                continue\n\n            cluster_indices = np.where(labels == label)[0]\n            cluster_pcd = object_pcd.select_by_index(cluster_indices)\n\n            if len(cluster_pcd.points) >= 10:  # Only consider substantial clusters\n                clusters.append(cluster_pcd)\n\n        return clusters\n\n    def compute_surface_normals(self, pointcloud: o3d.geometry.PointCloud, radius: float = 0.02) -> np.ndarray:\n        """\n        Compute surface normals for point cloud\n\n        Args:\n            pointcloud: Input point cloud\n            radius: Radius for normal computation\n\n        Returns:\n            Array of surface normals\n        """\n        # Estimate normals\n        pointcloud.estimate_normals(\n            search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=radius, max_nn=30)\n        )\n\n        return np.asarray(pointcloud.normals)\n\nclass ManipulationTargetDetector:\n    def __init__(self, depth_processor: DepthProcessor):\n        self.depth_processor = depth_processor\n\n    def find_graspable_objects(self, depth_image: np.ndarray, rgb_image: np.ndarray,\n                              robot_workspace: np.ndarray) -> List[Dict]:\n        """\n        Find objects that can be grasped by the robot\n\n        Args:\n            depth_image: Depth image from robot camera\n            rgb_image: RGB image for color information\n            robot_workspace: 3D workspace bounds [min_x, min_y, min_z, max_x, max_y, max_z]\n\n        Returns:\n            List of graspable objects with properties\n        """\n        # Convert to point cloud\n        pcd = self.depth_processor.depth_to_pointcloud(depth_image, rgb_image)\n\n        # Remove points outside robot workspace\n        points = np.asarray(pcd.points)\n        workspace_mask = (\n            (points[:, 0] >= robot_workspace[0]) & (points[:, 0] <= robot_workspace[3]) &\n            (points[:, 1] >= robot_workspace[1]) & (points[:, 1] <= robot_workspace[4]) &\n            (points[:, 2] >= robot_workspace[2]) & (points[:, 2] <= robot_workspace[5])\n        )\n\n        workspace_pcd = pcd.select_by_index(np.where(workspace_mask)[0])\n\n        # Segment the floor plane\n        floor_model, floor_points = self.depth_processor.segment_planes(workspace_pcd)\n\n        # Remove floor points to focus on objects\n        non_floor_pcd = workspace_pcd.select_by_index(\n            np.setdiff1d(np.arange(len(workspace_pcd.points)),\n                        np.asarray(floor_points.points), assume_unique=True)\n        )\n\n        # Find objects on surfaces\n        objects = self.depth_processor.find_objects_on_surface(non_floor_pcd, floor_model)\n\n        graspable_objects = []\n        for i, obj_pcd in enumerate(objects):\n            # Compute object properties\n            points = np.asarray(obj_pcd.points)\n\n            # Calculate bounding box\n            min_bound = points.min(axis=0)\n            max_bound = points.max(axis=0)\n            center = (min_bound + max_bound) / 2.0\n            size = max_bound - min_bound\n\n            # Calculate object height above floor\n            height_above_floor = center[2] - floor_model[3] / floor_model[2] if floor_model[2] != 0 else center[2]\n\n            # Determine if object is graspable based on size\n            object_volume = size[0] * size[1] * size[2]\n            is_graspable = self._is_graspable(size, object_volume)\n\n            if is_graspable:\n                # Calculate approach directions based on object shape\n                approach_directions = self._calculate_approach_directions(obj_pcd)\n\n                graspable_object = {\n                    \'id\': i,\n                    \'center\': center.tolist(),\n                    \'size\': size.tolist(),\n                    \'volume\': float(object_volume),\n                    \'height_above_floor\': float(height_above_floor),\n                    \'approach_directions\': approach_directions,\n                    \'is_graspable\': True,\n                    \'bbox\': [min_bound.tolist(), max_bound.tolist()]\n                }\n\n                graspable_objects.append(graspable_object)\n\n        return graspable_objects\n\n    def _is_graspable(self, size: np.ndarray, volume: float) -> bool:\n        """\n        Determine if an object is graspable based on size and volume\n        """\n        # Check size constraints (min 2cm, max 30cm in any dimension)\n        min_size = 0.02  # 2cm\n        max_size = 0.30  # 30cm\n\n        if np.any(size < min_size) or np.any(size > max_size):\n            return False\n\n        # Check volume constraints (min 1cm\xb3, max 5000cm\xb3)\n        min_volume = 1e-6  # 1cm\xb3 in m\xb3\n        max_volume = 5e-3  # 5000cm\xb3 in m\xb3\n\n        if volume < min_volume or volume > max_volume:\n            return False\n\n        # Check aspect ratio (not too flat or too thin)\n        sorted_dims = np.sort(size)\n        aspect_ratio = sorted_dims[2] / (sorted_dims[0] + 1e-6)  # longest / shortest\n\n        if aspect_ratio > 10:  # Too elongated\n            return False\n\n        return True\n\n    def _calculate_approach_directions(self, object_pcd: o3d.geometry.PointCloud) -> List[List[float]]:\n        """\n        Calculate potential approach directions for grasping\n        """\n        # Compute bounding box orientation\n        bbox = object_pcd.get_oriented_bounding_box()\n\n        # Get the rotation matrix\n        rotation = np.asarray(bbox.R)\n\n        # Calculate approach directions based on object orientation\n        # Default approach: from above (z direction)\n        approach_directions = [\n            [0, 0, -1],  # From above\n            [1, 0, 0],   # From the side (positive x)\n            [-1, 0, 0],  # From the side (negative x)\n            [0, 1, 0],   # From the side (positive y)\n            [0, -1, 0]   # From the side (negative y)\n        ]\n\n        return approach_directions\n'})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-performance-optimization",children:"Real-Time Performance Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"efficient-vision-processing-for-embedded-systems",children:"Efficient Vision Processing for Embedded Systems"}),"\n",(0,o.jsx)(n.p,{children:"For deployment on target hardware like the NVIDIA Jetson Orin Nano, optimization is crucial:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import threading\nimport queue\nfrom collections import deque\nimport time\n\nclass OptimizedVisionPipeline:\n    def __init__(self, max_queue_size: int = 3, target_fps: int = 30):\n        \"\"\"\n        Initialize optimized vision pipeline for embedded systems\n\n        Args:\n            max_queue_size: Maximum frames to queue\n            target_fps: Target processing rate\n        \"\"\"\n        self.max_queue_size = max_queue_size\n        self.target_fps = target_fps\n        self.frame_interval = 1.0 / target_fps\n\n        # Frame processing queue\n        self.frame_queue = queue.Queue(maxsize=max_queue_size)\n        self.result_queue = queue.Queue(maxsize=max_queue_size)\n\n        # Processing history for performance monitoring\n        self.processing_times = deque(maxlen=30)  # Last 30 frames\n        self.is_running = False\n\n        # Initialize components\n        self.feature_detector = RobotFeatureDetector()\n        self.object_detector = RoboticObjectDetector()\n        self.depth_processor = DepthProcessor(\n            camera_intrinsics=np.array([[616.171, 0, 319.5],\n                                       [0, 616.171, 239.5],\n                                       [0, 0, 1]])\n        )\n\n    def start_processing(self):\n        \"\"\"Start the processing thread\"\"\"\n        self.is_running = True\n        self.processing_thread = threading.Thread(target=self._processing_loop)\n        self.processing_thread.start()\n\n    def stop_processing(self):\n        \"\"\"Stop the processing thread\"\"\"\n        self.is_running = False\n        if hasattr(self, 'processing_thread'):\n            self.processing_thread.join()\n\n    def submit_frame(self, image: np.ndarray, depth: Optional[np.ndarray] = None,\n                    metadata: Dict = None) -> bool:\n        \"\"\"\n        Submit a frame for processing\n\n        Args:\n            image: Input image\n            depth: Optional depth image\n            metadata: Additional metadata\n\n        Returns:\n            True if frame was accepted, False if queue is full\n        \"\"\"\n        try:\n            frame_data = {\n                'image': image,\n                'depth': depth,\n                'metadata': metadata or {},\n                'timestamp': time.time()\n            }\n            self.frame_queue.put_nowait(frame_data)\n            return True\n        except queue.Full:\n            return False  # Queue is full, drop frame\n\n    def get_results(self, timeout: float = 0.1) -> Optional[Dict]:\n        \"\"\"\n        Get processing results\n\n        Args:\n            timeout: Maximum time to wait for results\n\n        Returns:\n            Processing results or None if no results available\n        \"\"\"\n        try:\n            return self.result_queue.get_nowait()\n        except queue.Empty:\n            return None\n\n    def _processing_loop(self):\n        \"\"\"Main processing loop running in separate thread\"\"\"\n        while self.is_running:\n            try:\n                # Get frame from queue\n                frame_data = self.frame_queue.get(timeout=0.01)\n\n                start_time = time.time()\n\n                # Process the frame\n                results = self._process_single_frame(frame_data)\n\n                processing_time = time.time() - start_time\n                self.processing_times.append(processing_time)\n\n                # Add performance metrics\n                results['performance'] = {\n                    'processing_time': processing_time,\n                    'average_processing_time': np.mean(self.processing_times),\n                    'current_fps': 1.0 / processing_time if processing_time > 0 else 0\n                }\n\n                # Put results in output queue\n                try:\n                    self.result_queue.put_nowait(results)\n                except queue.Full:\n                    # Drop results if output queue is full\n                    pass\n\n            except queue.Empty:\n                continue  # No frame to process, continue loop\n\n    def _process_single_frame(self, frame_data: Dict) -> Dict:\n        \"\"\"\n        Process a single frame with all vision components\n        \"\"\"\n        image = frame_data['image']\n        depth = frame_data.get('depth')\n        metadata = frame_data['metadata']\n\n        results = {\n            'timestamp': frame_data['timestamp'],\n            'metadata': metadata,\n            'features': {},\n            'objects': {},\n            'depth_analysis': {},\n            'robot_relevant': {}\n        }\n\n        # Feature detection\n        keypoints, descriptors = self.feature_detector.detect_and_compute(image)\n        results['features'] = {\n            'keypoints_count': len(keypoints),\n            'has_features': len(keypoints) > 10\n        }\n\n        # Object detection\n        obj_detections = self.object_detector.detect_objects(image)\n        results['objects'] = {\n            'detections': obj_detections['detections'],\n            'count': len(obj_detections['detections'])\n        }\n\n        # Depth analysis if available\n        if depth is not None:\n            # Convert depth to point cloud\n            pcd = self.depth_processor.depth_to_pointcloud(depth)\n\n            # Segment surfaces\n            floor_model, floor_points = self.depth_processor.segment_planes(pcd)\n\n            results['depth_analysis'] = {\n                'has_floor': floor_model is not None,\n                'floor_points_count': len(floor_points.points) if floor_points else 0\n            }\n\n            # Find graspable objects if robot position is known\n            if 'robot_position' in metadata and 'workspace_bounds' in metadata:\n                robot_pos = np.array(metadata['robot_position'])\n                workspace = np.array(metadata['workspace_bounds'])\n\n                manip_detector = ManipulationTargetDetector(self.depth_processor)\n                graspable = manip_detector.find_graspable_objects(\n                    depth, image, workspace\n                )\n\n                results['robot_relevant']['graspable_objects'] = graspable\n\n        return results\n\n    def get_performance_stats(self) -> Dict:\n        \"\"\"Get performance statistics\"\"\"\n        if not self.processing_times:\n            return {'average_fps': 0, 'average_processing_time': 0}\n\n        avg_processing_time = np.mean(self.processing_times)\n        avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n\n        return {\n            'average_fps': avg_fps,\n            'average_processing_time': avg_processing_time,\n            'min_processing_time': min(self.processing_times) if self.processing_times else 0,\n            'max_processing_time': max(self.processing_times) if self.processing_times else 0,\n            'queue_size': self.frame_queue.qsize()\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"constitution-alignment",children:"Constitution Alignment"}),"\n",(0,o.jsx)(n.p,{children:"This chapter addresses several constitutional requirements:"}),"\n",(0,o.jsx)(n.h3,{id:"vla-convergence-mandate-principle-i",children:"VLA Convergence Mandate (Principle I)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Vision algorithms specifically designed for the VLA pipeline"}),"\n",(0,o.jsx)(n.li,{children:"Integration with action planning systems"}),"\n",(0,o.jsx)(n.li,{children:"Real-time capable implementations for humanoid robots"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"real-time-validation-principle-iv",children:"Real-Time Validation (Principle IV)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Optimized algorithms suitable for real-time operation on Jetson Orin"}),"\n",(0,o.jsx)(n.li,{children:"Performance monitoring and optimization techniques"}),"\n",(0,o.jsx)(n.li,{children:"Efficient processing pipelines for control systems"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"target-hardware-optimization-constraint",children:"Target Hardware Optimization (Constraint)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Algorithms optimized for NVIDIA Jetson Orin Nano (8GB) platform"}),"\n",(0,o.jsx)(n.li,{children:"Efficient memory usage and computational complexity"}),"\n",(0,o.jsx)(n.li,{children:"Real-time performance on embedded systems"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sim-to-real-rigor-principle-iii",children:"Sim-to-Real Rigor (Principle III)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Vision systems designed to work with both simulated and real sensors"}),"\n",(0,o.jsx)(n.li,{children:"Robust algorithms that handle sensor noise and uncertainty"}),"\n",(0,o.jsx)(n.li,{children:"Validation techniques for perception accuracy"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,o.jsx)(n.h3,{id:"example-1-humanoid-navigation-vision-system",children:"Example 1: Humanoid Navigation Vision System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class NavigationVisionSystem:\n    def __init__(self):\n        self.pipeline = OptimizedVisionPipeline(target_fps=20)  # Lower FPS for navigation\n        self.obstacle_detector = self._setup_obstacle_detection()\n        self.path_planner = self._setup_path_planning()\n\n    def _setup_obstacle_detection(self):\n        """Set up obstacle detection specifically for navigation"""\n        # Use a lighter model for real-time navigation\n        detector = RoboticObjectDetector(\n            model_type="yolo",  # Use YOLO for speed\n            confidence_threshold=0.4  # Lower threshold for more detections\n        )\n        return detector\n\n    def _setup_path_planning(self):\n        """Set up path planning integration"""\n        # Path planning will use vision output\n        return PathPlanner()\n\n    def process_navigation_frame(self, image: np.ndarray, depth: np.ndarray) -> Dict:\n        """Process a frame for navigation purposes"""\n\n        # Detect obstacles\n        detections = self.obstacle_detector.detect_objects(image)\n\n        # Analyze traversable areas using depth\n        traversable_map = self._analyze_traversable_areas(depth)\n\n        # Combine vision and depth information\n        navigation_data = {\n            \'obstacles\': self._filter_navigation_obstacles(detections[\'detections\']),\n            \'traversable_areas\': traversable_map,\n            \'safe_paths\': self._compute_safe_paths(traversable_map)\n        }\n\n        return navigation_data\n\n    def _analyze_traversable_areas(self, depth_image: np.ndarray) -> np.ndarray:\n        """Analyze depth image to determine traversable areas"""\n        # Convert depth to point cloud\n        processor = DepthProcessor(\n            camera_intrinsics=np.array([[616.171, 0, 319.5],\n                                       [0, 616.171, 239.5],\n                                       [0, 0, 1]])\n        )\n\n        pcd = processor.depth_to_pointcloud(depth_image)\n\n        # Segment ground plane\n        ground_model, ground_points = processor.segment_planes(pcd)\n\n        # Analyze surface normals to determine walkability\n        normals = processor.compute_surface_normals(pcd)\n\n        # Create traversability map\n        height, width = depth_image.shape\n        traversability = np.ones((height, width))  # 1.0 = traversable\n\n        # Mark steep areas as non-traversable\n        for i, normal in enumerate(normals):\n            if abs(normal[2]) < 0.7:  # Surface is too steep if normal z-component < 0.7\n                # Map back to image coordinates (simplified)\n                pass  # Would map 3D point back to 2D image coordinates\n\n        return traversability\n\n    def _filter_navigation_obstacles(self, detections) -> List[Dict]:\n        """Filter detections to focus on navigation-relevant obstacles"""\n        navigation_obstacles = []\n\n        for detection in detections:\n            # Consider people, furniture, large objects as obstacles\n            if detection[\'label\'] in [\'person\', \'chair\', \'couch\', \'table\', \'bed\', \'dining table\']:\n                navigation_obstacles.append(detection)\n\n        return navigation_obstacles\n\n    def _compute_safe_paths(self, traversability_map: np.ndarray) -> List[np.ndarray]:\n        """Compute safe navigation paths"""\n        # Implementation would use path planning algorithms\n        # like A*, Dijkstra, or RRT\n        return []\n'})}),"\n",(0,o.jsx)(n.h3,{id:"example-2-manipulation-vision-for-humanoid-robot",children:"Example 2: Manipulation Vision for Humanoid Robot"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ManipulationVisionSystem:\n    def __init__(self, robot_config: Dict):\n        self.robot_config = robot_config\n        self.hand_camera = None\n        self.arm_joints = robot_config.get(\'arm_joints\', [])\n\n        # Initialize specialized manipulation perception\n        self.grasp_detector = ManipulationTargetDetector(\n            DepthProcessor(\n                camera_intrinsics=np.array([[616.171, 0, 319.5],\n                                           [0, 616.171, 239.5],\n                                           [0, 0, 1]])\n            )\n        )\n\n        # Initialize object property estimation\n        self.object_property_estimator = ObjectPropertyEstimator()\n\n    def perceive_manipulation_targets(self, rgb_image: np.ndarray,\n                                    depth_image: np.ndarray) -> List[Dict]:\n        """Perceive objects suitable for manipulation"""\n\n        # Find graspable objects\n        graspable_objects = self.grasp_detector.find_graspable_objects(\n            depth_image, rgb_image,\n            self._get_robot_workspace()\n        )\n\n        # Enhance with object properties\n        for obj in graspable_objects:\n            # Estimate object properties like weight, friction, etc.\n            properties = self.object_property_estimator.estimate_properties(\n                obj, rgb_image, depth_image\n            )\n            obj[\'properties\'] = properties\n\n        return graspable_objects\n\n    def _get_robot_workspace(self) -> np.ndarray:\n        """Get robot\'s reachable workspace"""\n        # Define workspace based on robot kinematics\n        # This would typically come from robot\'s URDF or kinematic model\n        return np.array([-0.5, -0.5, 0.2, 0.8, 0.5, 1.5])  # x, y, z bounds\n\n    def select_best_grasp_object(self, objects: List[Dict]) -> Optional[Dict]:\n        """Select the best object for grasping based on criteria"""\n        if not objects:\n            return None\n\n        # Criteria for selection:\n        # 1. Within reach\n        # 2. Appropriate size\n        # 3. Good grasp poses available\n        # 4. Task relevance (if specified)\n\n        best_object = None\n        best_score = -1\n\n        for obj in objects:\n            score = self._score_object_for_grasping(obj)\n            if score > best_score:\n                best_score = score\n                best_object = obj\n\n        return best_object\n\n    def _score_object_for_grasping(self, obj: Dict) -> float:\n        """Score an object for grasping suitability"""\n        score = 0.0\n\n        # Size appropriateness (prefer medium-sized objects)\n        size = np.array(obj[\'size\'])\n        size_score = 1.0 / (1.0 + abs(np.mean(size) - 0.1))  # Prefer ~10cm objects\n        score += size_score * 0.3\n\n        # Height appropriateness (prefer objects at good heights)\n        height_score = 1.0 / (1.0 + abs(obj[\'center\'][2] - 0.8))  # Prefer ~80cm height\n        score += height_score * 0.2\n\n        # Approach direction availability\n        approach_score = len(obj.get(\'approach_directions\', [])) * 0.1\n        score += min(approach_score, 0.3)  # Cap at 0.3\n\n        # Volume consideration (not too heavy or too light)\n        volume = obj[\'volume\']\n        volume_score = 1.0 / (1.0 + abs(volume - 0.001))  # Prefer ~1000cm\xb3 objects\n        score += volume_score * 0.2\n\n        return score\n\nclass ObjectPropertyEstimator:\n    def __init__(self):\n        # Initialize models for property estimation\n        self.material_classifier = self._load_material_classifier()\n        self.weight_estimator = self._load_weight_estimator()\n\n    def _load_material_classifier(self):\n        """Load material classification model"""\n        # Placeholder - would load actual model\n        return None\n\n    def _load_weight_estimator(self):\n        """Load weight estimation model"""\n        # Placeholder - would load actual model\n        return None\n\n    def estimate_properties(self, object_info: Dict, rgb_image: np.ndarray,\n                          depth_image: np.ndarray) -> Dict:\n        """Estimate object properties for manipulation"""\n\n        # Estimate material properties\n        material_properties = self._estimate_material_properties(\n            object_info, rgb_image\n        )\n\n        # Estimate physical properties\n        physical_properties = self._estimate_physical_properties(\n            object_info, depth_image\n        )\n\n        # Estimate graspability\n        graspability = self._estimate_graspability(object_info)\n\n        return {\n            \'material\': material_properties,\n            \'physical\': physical_properties,\n            \'graspability\': graspability\n        }\n\n    def _estimate_material_properties(self, object_info: Dict, rgb_image: np.ndarray) -> Dict:\n        """Estimate material properties from visual appearance"""\n        # Extract region of interest\n        bbox = object_info[\'bbox\']\n        x1, y1 = int(bbox[0][0]), int(bbox[0][1])\n        x2, y2 = int(bbox[1][0]), int(bbox[1][1])\n\n        roi = rgb_image[y1:y2, x1:x2]\n\n        # Analyze color, texture, and reflectance properties\n        # This is a simplified approach\n        avg_color = np.mean(roi, axis=(0, 1))\n\n        material_properties = {\n            \'color\': avg_color.tolist(),\n            \'texture_complexity\': self._calculate_texture_complexity(roi),\n            \'estimated_material\': self._classify_material(avg_color)\n        }\n\n        return material_properties\n\n    def _calculate_texture_complexity(self, image: np.ndarray) -> float:\n        """Calculate texture complexity using gradient magnitude"""\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n        gradients = np.sqrt(\n            cv2.Sobel(gray, cv2.CV_64F, 1, 0)**2 +\n            cv2.Sobel(gray, cv2.CV_64F, 0, 1)**2\n        )\n        return float(np.mean(gradients))\n\n    def _classify_material(self, avg_color: np.ndarray) -> str:\n        """Classify material based on color (simplified)"""\n        r, g, b = avg_color\n\n        if r > 0.7 and g > 0.7 and b > 0.7:\n            return \'metal\'\n        elif r > 0.8 and g > 0.6 and b < 0.3:\n            return \'plastic\'\n        elif r > 0.6 and g > 0.4 and b > 0.2:\n            return \'wood\'\n        else:\n            return \'unknown\'\n\n    def _estimate_physical_properties(self, object_info: Dict, depth_image: np.ndarray) -> Dict:\n        """Estimate physical properties like weight, friction, etc."""\n        # Use object size and typical densities to estimate weight\n        size = np.array(object_info[\'size\'])\n        volume = np.prod(size)\n\n        # Assume typical density for household objects (0.5 g/cm\xb3 = 500 kg/m\xb3)\n        estimated_density = 500  # kg/m\xb3\n        estimated_weight = volume * estimated_density\n\n        physical_properties = {\n            \'volume\': float(volume),\n            \'estimated_weight\': float(estimated_weight),\n            \'estimated_density\': estimated_density,\n            \'friction_coefficient\': self._estimate_friction_coefficient(object_info)\n        }\n\n        return physical_properties\n\n    def _estimate_friction_coefficient(self, object_info: Dict) -> float:\n        """Estimate friction coefficient based on material and surface properties"""\n        # Simplified estimation based on object properties\n        # In practice, this would use more sophisticated models\n        size = np.array(object_info[\'size\'])\n        surface_area = 2 * (size[0]*size[1] + size[1]*size[2] + size[0]*size[2])\n\n        # Smaller objects generally have higher effective friction\n        friction_factor = min(1.0, 0.5 / (surface_area + 0.01))\n\n        return float(0.3 + friction_factor * 0.4)  # Range: 0.3 to 0.7\n\n    def _estimate_graspability(self, object_info: Dict) -> Dict:\n        """Estimate how graspable the object is"""\n        size = np.array(object_info[\'size\'])\n\n        # Calculate graspability metrics\n        min_dimension = np.min(size)\n        max_dimension = np.max(size)\n        aspect_ratio = max_dimension / (min_dimension + 1e-6)\n\n        graspability = {\n            \'min_dimension\': float(min_dimension),\n            \'max_dimension\': float(max_dimension),\n            \'aspect_ratio\': float(aspect_ratio),\n            \'is_ergonomic\': min_dimension > 0.02 and aspect_ratio < 5.0,  # >2cm and <5:1 ratio\n            \'preferred_grasp_type\': self._determine_grasp_type(size)\n        }\n\n        return graspability\n\n    def _determine_grasp_type(self, size: np.ndarray) -> str:\n        """Determine preferred grasp type based on object dimensions"""\n        sorted_dims = np.sort(size)\n\n        if sorted_dims[0] < 0.02:  # Very thin - need precision grasp\n            return \'pinch\'\n        elif sorted_dims[2] / sorted_dims[0] > 4:  # Elongated - need power grasp\n            return \'power\'\n        else:  # Medium size - can use various grasps\n            return \'medium\'\n'})}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsx)(n.h3,{id:"exercise-1-vision-pipeline-optimization",children:"Exercise 1: Vision Pipeline Optimization"}),"\n",(0,o.jsx)(n.p,{children:"Implement an optimized vision pipeline that:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Processes images at 30 FPS on Jetson Orin hardware"}),"\n",(0,o.jsx)(n.li,{children:"Implements multi-threading for parallel processing"}),"\n",(0,o.jsx)(n.li,{children:"Includes performance monitoring and adaptive processing"}),"\n",(0,o.jsx)(n.li,{children:"Maintains accuracy while optimizing for speed"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"exercise-2-3d-perception-system",children:"Exercise 2: 3D Perception System"}),"\n",(0,o.jsx)(n.p,{children:"Create a complete 3D perception system that:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Converts RGB-D data to point clouds"}),"\n",(0,o.jsx)(n.li,{children:"Segments planar surfaces (floors, tables)"}),"\n",(0,o.jsx)(n.li,{children:"Detects objects positioned on surfaces"}),"\n",(0,o.jsx)(n.li,{children:"Estimates object properties for manipulation"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"exercise-3-robot-integrated-vision",children:"Exercise 3: Robot-Integrated Vision"}),"\n",(0,o.jsx)(n.p,{children:"Develop a vision system integrated with robot control that:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Provides real-time feedback to navigation system"}),"\n",(0,o.jsx)(n.li,{children:"Identifies graspable objects for manipulation"}),"\n",(0,o.jsx)(n.li,{children:"Incorporates robot kinematics into perception"}),"\n",(0,o.jsx)(n.li,{children:"Handles sensor noise and uncertainty"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Computer vision for robotics requires specialized techniques that account for the unique requirements of physical interaction systems. Unlike traditional computer vision applications, robotics vision must operate under real-time constraints, handle motion and ego-motion, and provide outputs directly relevant to physical action. For humanoid robots, vision systems must be optimized for embedded hardware while maintaining the accuracy and robustness required for safe operation in human-centered environments. The integration of vision with the broader Vision-Language-Action pipeline enables humanoid robots to perceive, understand, and interact with their environment effectively."}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'"Computer Vision in Robotics and Automation" by S. Hutchinson and G. Chirikjian'}),"\n",(0,o.jsx)(n.li,{children:'"Handbook of Robotics" edited by Siciliano and Khatib (Vision chapter)'}),"\n",(0,o.jsx)(n.li,{children:'"Multiple View Geometry in Computer Vision" by Hartley and Zisserman'}),"\n",(0,o.jsx)(n.li,{children:'"Real-Time Computer Vision" by J\xe4hne'}),"\n",(0,o.jsx)(n.li,{children:'"Robotics: Vision, Manipulation and Control" by Spong, Hutchinson, and Vidyasagar'}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);