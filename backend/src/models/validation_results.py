from pydantic import BaseModel, Field
from datetime import datetime
from typing import List, Optional, Dict, Any
from enum import Enum


class ValidationStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


class Query(BaseModel):
    """
    A text-based question or search request from a user that needs to be converted to embeddings for retrieval
    """
    id: str = Field(..., description="Unique identifier for the query")
    text: str = Field(..., description="The actual query text")
    created_at: datetime = Field(default_factory=datetime.now, description="Timestamp when query was created")
    embedding_request: bool = Field(default=True, description="Whether embeddings should be generated for this query")


class EmbeddingVector(BaseModel):
    """
    A numerical representation of text content generated by the Cohere model for semantic similarity matching
    """
    id: str = Field(..., description="Unique identifier for the embedding vector")
    query_id: str = Field(..., description="Reference to parent Query")
    vector: List[float] = Field(..., description="The actual embedding vector values")
    vector_size: int = Field(..., description="Dimension of the embedding vector")
    model_name: str = Field(default="embed-english-v3.0", description="Name of the model used to generate the embedding")
    created_at: datetime = Field(default_factory=datetime.now, description="Timestamp when embedding was generated")


class RetrievedChunk(BaseModel):
    """
    A document chunk retrieved from the vector database as a result of a query
    """
    id: str = Field(..., description="Unique identifier for the retrieved chunk")
    query_id: str = Field(..., description="Reference to the original Query")
    chunk_id: str = Field(..., description="Reference to the original document chunk in storage")
    content: str = Field(..., description="The actual text content of the chunk")
    similarity_score: float = Field(..., description="Cosine similarity score between query and chunk", ge=0.0, le=1.0)
    rank: int = Field(..., description="Position in the ranked results list", ge=1)
    source_url: str = Field(..., description="URL of the original document")
    document_id: str = Field(..., description="ID of the original document")
    created_at: datetime = Field(default_factory=datetime.now, description="Timestamp when chunk was retrieved")


class ValidationResult(BaseModel):
    """
    The result of validating a retrieval operation
    """
    id: str = Field(..., description="Unique identifier for the validation result")
    query_id: str = Field(..., description="Reference to the original Query")
    is_relevant: bool = Field(..., description="Whether the retrieved content is semantically relevant")
    relevance_score: float = Field(..., description="Numerical relevance score", ge=0.0, le=1.0)
    metadata_valid: bool = Field(..., description="Whether metadata is preserved correctly")
    semantic_similarity: float = Field(..., description="Semantic similarity between query and result", ge=0.0, le=1.0)
    retrieval_latency: float = Field(..., description="Time taken for retrieval in seconds", ge=0.0)
    deterministic: bool = Field(..., description="Whether results are consistent across runs")
    created_at: datetime = Field(default_factory=datetime.now, description="Timestamp when validation was performed")
    validation_details: str = Field(default="", description="Detailed information about the validation")


class ValidationReport(BaseModel):
    """
    A comprehensive report summarizing the validation of multiple queries
    """
    id: str = Field(..., description="Unique identifier for the validation report")
    validation_name: str = Field(..., description="Name/description of the validation run")
    total_queries: int = Field(..., description="Total number of queries tested", ge=0)
    successful_retrievals: int = Field(..., description="Number of successful retrievals", ge=0)
    precision_score: float = Field(..., description="Overall precision score", ge=0.0, le=1.0)
    average_latency: float = Field(..., description="Average retrieval latency", ge=0.0)
    metadata_accuracy: float = Field(..., description="Accuracy of metadata preservation", ge=0.0, le=1.0)
    determinism_score: float = Field(..., description="Consistency of results across runs", ge=0.0, le=1.0)
    created_at: datetime = Field(default_factory=datetime.now, description="Timestamp when report was generated")
    status: ValidationStatus = Field(default=ValidationStatus.PENDING, description="Validation run status")
    results: List[Dict[str, Any]] = Field(default_factory=list, description="Detailed results for each query")